---
title: "Bootstrapping & Sample Sizes"
---

Load required libraries:

```{r, message=FALSE}
library(rms)
library(tidyverse)
library(pROC)
library(ggplot2)
```


On the previous page, we ran through some manual bootstrapping for the purposes of showing their in-principle operation. There we used just a single predictor and a relatively small sample size.

Now let's try to develop a bigger bootstrap validation procedure with a bigger sample size and more covariates. Again, start by generating some sample data, this time with $\beta_0=-1.5$ and predictor effects $\beta_k = 0.7$ for $1 \leq k \leq 10$.

```{r}
set.seed(11235)
k <- 10
n <- 100000
beta_0 <- -1.5
beta <- rep(0.7, k)

simulate_data <- function(k, n) {
  X <- matrix(rnorm(n*k), nrow = n, ncol = k)
  colnames(X) <- paste0("x", 1:k)
  eta <- beta_0 + X %*% beta
  p <- 1 / (1 + exp(-eta))
  y <- rbinom(n, size = 1, prob = p)
  return(
    list(
      df = data.frame(X, y = y),
      eta = eta,
      p = p
    )
  )
}

dgp_data <- simulate_data(k, n)
df <- dgp_data$df
print(paste0("Hence a prevalence of ", round(100 * mean(df$y), 2), "%"))
glm(y ~ ., data = df, family = "binomial")
```

Visualise:

```{r}
hist(dgp_data$eta, breaks = 100, col = "lightblue", main = "Distribution of Linear Predictor",
     xlab = expression(eta))#, freq = FALSE)
```

```{r}
hist(dgp_data$p, breaks = 100, main = "Distribution of Probabilities",
     xlab = "Theoretical Probability")#, freq = FALSE)
```
```{r}
plot_data <- cbind(dgp_data$df, eta = dgp_data$eta, p = dgp_data$p)
ggplot(plot_data, aes(x = eta, y = p)) +
  geom_line(stat = "function", fun = plogis, linewidth = 1) +
  labs(x = "Linear Predictor (η)", y = "Probability (p)", title = "Logistic Function: p = 1 / (1 + exp(-η))") +
  theme_minimal()
```

## Create dataframe and model

```{r}
dd <- datadist(df)
options(datadist = "dd")
fit <- lrm(y ~ ., data = df, x=TRUE,y=TRUE)
print(fit)
```

Odd ratios:

```{r}
exp(fit$coefficients)
```

## Initial Model Validation

```{r}
val <- validate(fit, method = "boot", B = 200)
val
```
```{r}
auc <- (val["Dxy", "index.corrected"] + 1) / 2
print(auc)
```

## Sample Size

I would like to understand the effect of sample size on model performance. Here I focus on two methods of model validation -- Cross Validation (10-fold) and Bootstrapping -- and applying them to the model formulation given above on variable sample sizes.

Let's start by setting up the basics:

```{r}
df <- dgp_data$df

sample_sizes <- c(100, 200, 300, 500, 1000, 5000, 10000, 50000)
B <- 200
performance_metrics <- c('AUC', 'calibration_slope')
```

So we have a vector of sample sizes, a number of bootstrap repetitions, and we would like to use 2 performance metrics (AUC, Calibration slope) for understand the impact of sample size on model performance.

Hence we need to calculate 5 * 200 * 2 = 2000 data points, stored in a 4D data structure.

Next let's set up a nested-list data structure to hold the results.

```{r}
rm(results) # if needed
```

```{r}
results <- list()
for (s in sample_sizes) {
  results[[as.character(s)]] <- list(auc = numeric(B), slope = numeric(B))
}
fail_counts <- setNames(rep(0, length(sample_sizes)), sample_sizes)
```

Each of these datapoints will be populated with either an AUC value or a calibration slope. Let's start with a simple function that will allow us to calculate a single result-pair. Later we will wrap this inside a double-loop to populate the results list fully.

```{r}
sample300 <- sample_n(df, 300, replace = FALSE) #for testing

performance <- function(dev_data) {
  bootstrap_data <- sample_n(dev_data, nrow(dev_data), replace = TRUE)
  mod <- suppressWarnings(glm(y ~ ., family = "binomial", data = bootstrap_data))
  if (inherits(mod, "try-error") || !mod$converged) return(NULL)
  
  pred_dev_reponse <- predict(mod, newdata = dev_data, type = "response")
  auc <- as.numeric(roc(dev_data$y, pred_dev_reponse, quiet = TRUE)$auc)
  
  # calibration slope
  pred_dev_link <- predict(mod, newdata = dev_data, type = "link")
  calibration_model <- suppressWarnings(glm(dev_data$y ~ pred_dev_link, family = "binomial"))
  slope <- as.numeric(coef(calibration_model))[2]
  
  return(
    list(
      auc = auc,
      slope = slope
      )
  )
}

res <- performance(sample300)
res
```

Next, let's check that we can successfully populate our results list with just these two values in the 300 sample-size block.

```{r}
results[["300"]][["auc"]][[1]] <- res$auc
results[["300"]][["slope"]][[1]] <- res$slope
results
```

Then take the inner loop and draw B bootstrap samples and run the model on each, reporting AUC and calibration-slope for a single sample size; say 300. Again, wrap it in a function, and this time we will just fill up the `300` block of the result nested list.

```{r}
sample300 <- sample_n(df, 300, replace = FALSE)  # for testing purposes

for (b in 1:200) {
  res <- performance(sample300)
  results[["300"]] [["auc"]] [b] <- res$auc
  results[["300"]] [["slope"]] [b] <- res$slope
}
```


```{r}
results[["300"]]
```

That all seems correct. It is interesting to note that AUC looks fairly stable here whereas the calibration slope seems more erratic, meaning we expect it to have higher variance than bigger sample sizes. Let's find out by now generalising and embedding this loop into another for-loop to cover the varying sample sizes.

```{r}
for (s in sample_sizes) {
  test_sample <- sample_n(df, s, replace = FALSE)
  
  for (b in 1:B) {
    res <- performance(test_sample)
    if (is.null(res)) {
      fail_counts[as.character(s)] <- fail_counts[as.character(s)] + 1
      next
    }
    results[[as.character(s)]] [["auc"]] [b] <- res$auc
    results[[as.character(s)]] [["slope"]] [b] <- res$slope
  }
}
```

If successful, we should now have a full complement of AUC and Slope values across all of our sample sizes.

```{r}
results
```

```{r}
fail_counts
```

## Plot results

```{r}
# Flatten into a data frame
results_df <- map_dfr(names(results), function(s) {
  tibble(
    sample_size = as.integer(s),
    auc = results[[s]]$auc,
    slope = results[[s]]$slope
  )
})
head(results_df)
```

```{r}
results_long <- results_df %>%
  pivot_longer(cols = c(auc, slope), names_to = "metric", values_to = "value")
head(results_long, 10)
```

```{r}
fail_df <- tibble(
  sample_size = as.integer(names(fail_counts)),
  failures = as.numeric(fail_counts)
)
fail_df
```

```{r}
ggplot(results_long, 
       aes(x = factor(sample_size), y = value, fill = metric)
       ) +
  geom_boxplot(outlier.shape = NA, alpha = 0.5, 
               position = position_dodge(width = 0.75)) +
  scale_y_continuous(
    name = "Model Performance"
  ) +
  labs(x = "Sample Size", fill = "Metric", color = "Metric", title = "Bootstrap Model Performance vs Sample Size") +
  theme_minimal()
```

