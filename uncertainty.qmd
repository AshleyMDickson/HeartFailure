---
title: "Some topics in LR"
subtitle: "Estimation, Uncertainty, and Validation"
---

## 1. Introduction

Here I explore some topics in logistic regression manually, including techniques in estimation, uncertainty and validation. I'll generate a simulated dataset by way of illustration.

## 2. Simulated Data

Create simple dataset from the logic model with one predictor

$$
P(Y = 1 \mid X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X)}}
$$

with parameters

-   $\beta_0 = -0.5$
-   $\beta_1 = 2$.

```{r}
set.seed(112358)
n <- 100
beta_0 <- -0.5
beta_1 <- 2
x <- rnorm(n)
eta <- beta_0 + beta_1 * x
prob <- 1 / (1 + exp(-eta))
y <- rbinom(n, 1, prob)
sim_data <- data.frame(y, x, prob)
plot(sim_data$x, sim_data$y,
     xlab = "Simulated x", ylab = "Simulated Y",
     main = "Simulated data for Logistic Regression"
)
```

## 3. Logistic Model and Estimation

Given a logit model of the form:

$$
\log\left(\frac{\pi_i}{1 - \pi_i}\right) = \eta_i,
$$

where

$$
\eta_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_p x_{ip}.
$$

We rearrange for $\pi_i$:

$$
\pi_i = \frac{\exp(\eta_i)}{1 + \exp(\eta_i)} = \frac{1}{1 + \exp(-\eta_i)}.
$$

Parameter estimates for $\beta_j$ are found firstly by deriving the likelihood function:

$$
L(\beta_j) = \prod_{i=1}^{n} \pi_i^{y_i} (1 - \pi_i)^{1 - y_i}
$$

where ( n ) is the number of observations. In MLE to goal is to maximize the log-likelihood:

$$
\log L = \ell(\beta_j) = \sum_{i=1}^{n} y_i \log(\pi_i) + (1 - y_i) \log(1 - \pi_i),
$$

$$
= \sum_{i=1}^{n} \log(1 - \pi_i) + \sum_{i=1}^{n} y_i \left[ \log(\pi_i) - \log(1 - \pi_i) \right],
$$

$$
= \sum_{i=1}^{n} \log(1 - \pi_i) + \sum_{i=1}^{n} y_i \log\left( \frac{\pi_i}{1 - \pi_i} \right).
$$

Since the logistic function is

$$
\pi_i = \frac{1}{1 + \exp(-\eta_i)},
$$

it follows that

$$
1 - \pi_i = 1 - \frac{1}{1 + \exp(-\eta_i)} = \frac{\exp(-\eta_i)}{1 + \exp(-\eta_i)}.
$$

We can take the odds:

$$
\frac{\pi_i}{1 - \pi_i} = \frac{\frac{1}{1 + \exp(-\eta_i)}}{\frac{\exp(-\eta_i)}{1 + \exp(-\eta_i)}} = \frac{1}{\exp(-\eta_i)} = \exp(\eta_i).
$$

Substitute this back into the previous equation:

$$
\mathcal{L} = \sum_{i=1}^{n} \left( \log(1 - \pi_i) + \sum_{j=1}^{p} y_i (\beta_0 + \beta_1 x_{i1} + \dots + \beta_p x_{ip}) \right),
$$

$$
= \sum_{i=1}^{n} \left[ -\log(1 + \exp(\eta_i)) + \sum_{j=1}^{p} y_i \eta_i \right].
$$

Usually in MLE we differentiate and set to 0:

$$
\frac{\partial \ell}{\partial \beta_j} = \sum_{i=1}^{n} \frac{1}{1 + \exp(\eta_i)} \cdot \exp(\eta_i) x_{ij} + \sum_{i=1}^{n} y_i x_{ij},
$$

$$
= \sum_{i=1}^{n} \frac{\exp(\eta_i)}{1 + \exp(\eta_i)} x_{ij} + \sum_{i=1}^{n} y_i x_{ij},
$$

$$
= \sum_{i=1}^{n} (y_i - \pi_i) x_{ij} = 0.
$$

However, this has no closed-form solution, so numerical methods are used to estimate $beta\_j$, e.g. Newton-Raphson. For this, we first need to derive the Score Function and the Hessian.

## 4. Gradient

To find the maxima, we firstly need to know how the log likelihood function varies w.r.t its $\beta$ parameters This is just its first derivative, called the Score Function:

The likelihood function for the whole sample is:

$$
L(\boldsymbol{\beta}) = \prod_{i=1}^n \pi_i^{y_i} (1 - \pi_i)^{1 - y_i}
$$

Taking logs gives the log-likelihood:

$$
\ell(\boldsymbol{\beta}) = \sum_{i=1}^n \left[ y_i \log(\pi_i) + (1 - y_i) \log(1 - \pi_i) \right]
$$

### Differentiate it

To find the derivative of $\ell(\boldsymbol{\beta})$ with respect to $\beta_1$, isolate a single observation

$$
\ell_i = y_i \log(\pi_i) + (1 - y_i) \log(1 - \pi_i)
$$

and note that the desired derivative is the product of three simpler derivatives via the chain rule:

$$
\frac{d\ell_i}{d\beta_1} 
= \frac{d\ell_i}{d\pi_i} \cdot \frac{d\pi_i}{d\eta_i} \cdot \frac{d\eta_i}{d\beta_1}
$$

Theese derivatives evaluate as:

-   Derivative with respect to $\pi_i$ follows from the derivative of logs:

    $$
    \frac{d\ell_i}{d\pi_i} = \frac{y_i}{\pi_i} - \frac{1 - y_i}{1 - \pi_i}
    $$

-   This derivative of the logistic function follows when we notice that the direct derivative of $\pi_i$ is equivalent to the result of multiplying it by $(1-\pi_i)$:

    $$
    \pi_i = \frac{1}{1 + \exp(-\eta_i)} \quad \Rightarrow \quad \frac{d\pi_i}{d\eta_i} = \pi_i (1 - \pi_i)
    $$

-   The derivative of the linear predictor is fairly trivial:

    $$
    \frac{d\eta_i}{d\beta_1} = x_i
    $$

### Multiplying the three derivatives

Start by substituting:

$$
\frac{d\ell_i}{d\beta_1}
= \left( \frac{y_i}{\pi_i} - \frac{1 - y_i}{1 - \pi_i} \right) \cdot \pi_i (1 - \pi_i) \cdot x_i
$$

Expand and simplify:

$$
\left( \frac{y_i}{\pi_i} - \frac{1 - y_i}{1 - \pi_i} \right) \cdot \pi_i (1 - \pi_i)
= y_i (1 - \pi_i) - (1 - y_i) \pi_i = y_i - \pi_i
$$

Hence:

$$
\frac{d\ell_i}{d\beta_1} = (y_i - \pi_i) x_i
$$

### The Score Function

Summing over all observations gives the score function, often notated $U$:

$$
U(\beta_1) = \frac{\partial \ell}{\partial \beta_1} = \sum_{i=1}^n (y_i - \pi_i) x_i
$$

A similar expression holds for the intercept:

$$
U(\beta_0) = \sum_{i=1}^n (y_i - \pi_i)
$$

### And in general...

The score function in vector form for arbitrary $\beta_k$ coefficients is sometimes called the Jacobian and can be expressed:

$$
\mathbf{U}(\boldsymbol{\beta}) = \nabla_{\boldsymbol{\beta}} \ell(\boldsymbol{\beta}) = \sum_{i=1}^n (y_i - \pi_i) \mathbf{x}_i
$$

Or in matrix form:

$$
\mathbf{U}(\boldsymbol{\beta}) = \mathbf{X}^\top (\mathbf{y} - \boldsymbol{\pi}).
$$

We can now represent the score function in code for our simulated dataset:

```{r}
compute_score <- function(beta0, beta1, data) {
  x <- data$x
  y <- data$y
  
  eta <- beta0 + beta1 * x
  pi <- 1 / (1 + exp(-eta))
  residual <- y - pi
  
  score_0 <- sum(residual)
  score_1 <- sum(residual * x)
  c(score_0, score_1)
}

compute_score(beta0 = -0.5, beta1 = 2, data = sim_data)
```

## 5. The Hessian

To understand the curvature of the log-likelihood function near its stationary points, we need need the second derivatives.

### Deriving the Hessian

Begin with the score function:

$$
\frac{\partial \ell}{\partial \beta_j} = \sum_{i=1}^n (y_i - \pi_i) x_{ij}
$$

Our goal is to differentiate this expression with respect to another parameter ($\beta_k$) to obtain the Hessian:

$$
\frac{\partial^2 \ell}{\partial \beta_j \partial \beta_k}
$$


We are interested in the derivative, which can be expressed as

$$
\frac{d}{d\beta_k} \left[ \sum_{i=1}^n (y_i - \pi_i) x_{ij} \right]
= \sum_{i=1}^n \left( \frac{d}{d\beta_k} (-\pi_i) \cdot x_{ij} \right)
$$

since $y_i$ is observed data not dependent on $\beta_k$, its derivative is zero.

Derivative of $\pi_i$:

Compute the derivative using the chain rule:

1.  First, we have the derivative of the logistic function:

$$
\frac{d\pi_i}{d\eta_i} = \pi_i (1 - \pi_i)
$$

2.  Derivative of $\eta_i$ with respect to $\beta_k$

Since:

$$
\eta_i = \sum_{l=0}^p \beta_l x_{il} = \beta_0 + \beta_1 x_{i1} + ... + \beta_k x_{ik} + ... + \beta_p x_{ip}
$$

any term not involving $\beta_k$ differentiates to $0$ and the power of $\beta_k$ goes to $0$ also, leaving:

$$
\frac{d\eta_i}{d\beta_k} = x_{ik}
$$

Chaining them together:

$$
\frac{d\pi_i}{d\beta_k} = \pi_i (1 - \pi_i) x_{ik}
$$

Substitute back into the Hessian:

We can now substitute these results back into the second derivative of the log-likelihood:

$$
\frac{\partial^2 \ell}{\partial \beta_j \partial \beta_k}
= - \sum_{i=1}^n \pi_i (1 - \pi_i) x_{ij} x_{ik}
$$

This gives an entry of the Hessian matrix.

### Hessian in Matrix Form

If we have two parrameters, $\beta_0$ and $\beta_1$, and $x_{i0} = 1$, $x_{i1} = x_i$, then the Hessian is:

$$
\mathbf{H} =
- \sum_{i=1}^n \pi_i (1 - \pi_i)
\begin{bmatrix}
1 & x_i \\
x_i & x_i^2
\end{bmatrix}
$$

### And in general...

In arbitrary parameters, Hessian matrix is expressed in matrix form

$$
\mathbf{H}(\boldsymbol{\beta}) = \frac{\partial^2 \ell}{\partial \boldsymbol{\beta} \partial \boldsymbol{\beta}^\top}
= - \mathbf{X}^\top \mathbf{W} \mathbf{X}
$$
where $W$ is diagonal with elements $p_i(1 - p_i)$.

We can now represent the Hessian in code:

```{r}
compute_hessian <- function(beta0, beta1, data) {
  x <- data$x
  y <- data$y
  n <- length(x)
  
  eta <- beta0 + beta1 * x
  pi <- 1 / (1 + exp(-eta))
  w <- pi * (1 - pi)  # weights
  
  H <- matrix(0, nrow = 2, ncol = 2)
  
  for (i in 1:n) {
    xi_vec <- c(1, x[i])
    outer_prod <- tcrossprod(xi_vec)
    H <- H + w[i] * outer_prod
  }
  
  return(-H)
}

compute_hessian(beta0 = -0.5, beta1 = 2, sim_data)
```

The determinant $\det(H)$ here is positive, meaning that we have a maximum or a minimum. The negative sign on entry $(1,1)$ indicates a maximum.

## 6. Newton-Raphson Algorithm

We are interested in solving for the values of $\boldsymbol{\beta}$ that maximise the (log-)likelihood function. Numerically, we shall invoke an iterative procedure with iteration step

$$
\boldsymbol{\beta_{t+1}} = \boldsymbol{\beta_t} - \boldsymbol{H^{-1}} \boldsymbol{U}
$$ where U is the sciore vector (Jacobian) and H is the Hessian.

### R implementation:

```{r}
NR <- function(data, beta_init = c(0, 0), iterations = 7) {
  beta <- beta_init
  n <- nrow(data)
  x <- data$x
  y <- data$y
  
  for (iter in 1:iterations) {
    eta <- beta[1] + beta[2] * x
    pi <- 1 / (1 + exp(-eta))
    residual <- y - pi
    score <- c(sum(residual), sum(residual * x))  # U_0, U_1
    H <- compute_hessian(beta[1], beta[2], data)
    
    delta <- solve(H, score)  # H^{-1} * U
    beta_new <- beta - delta
    beta <- beta_new
    #print(beta)
  }
  return(list(beta=beta, H=H))
}
```


```{r}
out <- NR(sim_data)
beta_hat <- out$beta
H <- out$H
print(beta_hat)
print(H)
```

```{r}
# Compare to GLM
mod <- glm(y ~ x, family = binomial, data = sim_data)
mod$coefficients
```

## 7. Variance of Estimates

Let's now derive the variance of the maximum likelihood estimate (MLE) $\hat{\boldsymbol{\beta}}$ in logistic regression.

### Taylor Expansion

At the MLE, the score function (gradient of the log-likelihood) is zero:

$$
\mathbf{U}(\hat{\boldsymbol{\beta}}) = \frac{\partial \ell}{\partial \boldsymbol{\beta}} \bigg|_{\hat{\boldsymbol{\beta}}} = 0
$$

We can expand the score function using a first-order Taylor approximation around the true value $\boldsymbol{\beta}$:

$$
\mathbf{U}(\hat{\boldsymbol{\beta}}) \approx \mathbf{U}(\boldsymbol{\beta}) + \mathbf{H}(\boldsymbol{\beta}) (\hat{\boldsymbol{\beta}} - \boldsymbol{\beta})
$$

Setting the left-hand side to 0

$$
0 \approx \mathbf{U}(\boldsymbol{\beta}) + \mathbf{H}(\boldsymbol{\beta}) (\hat{\boldsymbol{\beta}} - \boldsymbol{\beta})
$$

and rearranging to

$$
\hat{\boldsymbol{\beta}} - \boldsymbol{\beta} \approx - \mathbf{H}^{-1}(\boldsymbol{\beta}) \, \mathbf{U}(\boldsymbol{\beta})
$$

lets us take the variance of both sides:

$$
\operatorname{Var}(\hat{\boldsymbol{\beta}}) \approx \mathbf{H}^{-1} \operatorname{Var}(\mathbf{U}(\boldsymbol{\beta})) \left(\mathbf{H}^{-1}\right)^\top
$$

By the information equality, the variance of the score function equals the Fisher Information:

$$
\operatorname{Var}(\mathbf{U}(\boldsymbol{\beta})) = \mathcal{I}(\boldsymbol{\beta}) = - \mathbb{E}[\mathbf{H}(\boldsymbol{\beta})]
$$

So:

$$
\operatorname{Var}(\hat{\boldsymbol{\beta}}) \approx \mathbf{H}^{-1} \mathcal{I} \left( \mathbf{H}^{-1} \right)^\top = \mathcal{I}^{-1}
$$

### Application

In logistic regression, the Fisher Information (or negative Hessian) at $\hat{\boldsymbol{\beta}}$ is:

$$
\mathcal{I}(\hat{\boldsymbol{\beta}}) = \mathbf{X}^\top \mathbf{W} \mathbf{X}
$$

Where $\mathbf{W}$ is a diagonal matrix with entries $\pi_i (1 - \pi_i)$.

So, the varianceâ€“covariance matrix of $\hat{\boldsymbol{\beta}}$ is:

$$
\operatorname{Var}(\hat{\boldsymbol{\beta}}) = \left( \mathbf{X}^\top \mathbf{W} \mathbf{X} \right)^{-1}
$$

This is the matrix used to compute the standard errors and hence the Wald confidence intervals.

### Two Parameters

When we have only two parameters, $\beta_0$ and $\beta_1$, the Fisher Information is:

$$
\mathcal{I} = \sum_{i=1}^n \pi_i (1 - \pi_i)
\begin{bmatrix}
1 & x_i \\
x_i & x_i^2
\end{bmatrix}
$$

Then:

$$
\operatorname{Var}(\hat{\boldsymbol{\beta}}) = \left[ \sum_{i=1}^n \pi_i (1 - \pi_i)
\begin{bmatrix}
1 & x_i \\
x_i & x_i^2
\end{bmatrix} \right]^{-1}
$$

The diagonal elements give $\operatorname{Var}(\hat{\beta}_0)$ and $\operatorname{Var}(\hat{\beta}_1)$, respectively. Taking square roots gives their standard errors.

```{r}
vcov_mat <- solve(-H)
se <- sqrt(diag(vcov_mat))
z <- qnorm(0.975)
ci <- cbind(
  Estimate = beta_hat,
  SE = se,
  Lower = beta_hat - z * se,
  Upper = beta_hat + z * se
)
print(round(vcov_mat, 4))
```

```{r}
rownames(ci) <- c("beta_0", "beta_1")
print(round(ci, 4))
```

## 8. Estimate Variances by Bootstrapping

```{r}
bootstrap <- function(data, B) {
  n <- nrow(data)
  beta_boot <- matrix(NA, nrow = B, ncol = 2)  # Store (beta_0, beta_1)
  
  for (b in 1:B) {
    idx <- sample(1:n, size = n, replace = TRUE)
    boot_data <- data[idx, ]
    out <- NR(boot_data)
    beta_boot[b, ] <- out$beta
  }
  
  colnames(beta_boot) <- c("beta_0", "beta_1")
  return(beta_boot)
}
```

```{r}
boot_estimates <- bootstrap(sim_data, 1000)

beta_boot <- colMeans(boot_estimates)
boot_var <- apply(boot_estimates, 2, var)
boot_se <- sqrt(boot_var)
boot_ci <- apply(boot_estimates, 2, quantile, probs = c(0.025, 0.975))
round(cbind(Estimate = beta_boot, Variance = boot_var, SE = boot_se, t(boot_ci)), 4)

```

The results are technically consistent with the above, but the CIs only just include the true values. Unclear whether this is due to a programming error, unsufficient sample size, or something else.

## 9. Bootstrap Validation

Repeated sampling with replacement to estimate standard errors.

Bootstrap in R:

```{r, message=FALSE}
library(pROC)
bootstrap_validate_auc <- function(data, B = 200) {
  n <- nrow(data)
  x <- data$x
  y <- data$y
  optimism <- numeric(B)
  
  for (b in 1:B) {
    idx <- sample(1:n, size = n, replace = TRUE)
    data_boot <- data[idx, ]
    
    fit_boot <- NR(data_boot)
    beta_boot <- fit_boot$beta
    
    x_boot <- data_boot$x
    y_boot <- data_boot$y
    eta_boot <- beta_boot[1] + beta_boot[2] * x_boot
    p_boot <- 1 / (1 + exp(-eta_boot))
    auc_boot <- suppressMessages(auc(y_boot, p_boot))
    
    # AUC on original data
    eta_test <- beta_boot[1] + beta_boot[2] * x
    p_test <- 1 / (1 + exp(-eta_test))
    auc_test <- suppressMessages(auc(y, p_test))
    
    optimism[b] <- auc_boot - auc_test
  }
  
  avg_optimism <- mean(optimism)
  
  # Fit model on full data
  final_fit <- NR(data)
  beta_final <- final_fit$beta
  eta_final <- beta_final[1] + beta_final[2] * x
  p_final <- 1 / (1 + exp(-eta_final))
  auc_app_final <- suppressMessages(auc(y, p_final))
  
  validated_auc <- auc_app_final - avg_optimism
  
  return(list(
    apparent = as.numeric(auc_app_final),
    optimism = avg_optimism,
    validated = validated_auc
    )
  )
}

```

```{r}
validation_auc <- bootstrap_validate_auc(sim_data)
print(validation_auc)
```

## 10. Cross-validation


Lastly, we can validate by partitioning our data into $K$ folds, train/test each fold and measure average performance.

### 5-fold CV in R:

```{r}
cross_validate <- function(data, K = 5, metric = "auc") {
  n <- nrow(data)
  partition <- sample(rep(1:K, length.out = n))
  perf <- numeric(K)
  
  for (k in 1:K) {
    train <- data[partition != k, ]
    test  <- data[partition == k, ]
    
    model <- glm(y ~ x, data = train, family = binomial)
    p <- predict(model, newdata = test)
    y <- test$y
    perf[k] <- suppressMessages(auc(y, p))
  }
  
  list(per_fold = perf, mean = mean(perf))
}

```

```{r}
cv_auc <- cross_validate(sim_data, K = 5, metric = "auc")
round(cv_auc$mean, 4)
```
