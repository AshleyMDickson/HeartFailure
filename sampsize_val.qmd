---
title: "Sample Size Calculations & Model Validation"
author: "Ashley Dickson"
editor: visual
---

## Introduction

In this document, I simulate an approach to clinical risk prediction modelling that begins with sample size requirements, assumes a simple model, and then proceeds to assess options for internal validity. I begin with sample size requirements.

```{r}
library(pmsampsize) # Riley et al.
library(samplesizedev) # Menelaos et al.
```

## Sample Size Requirements

Here I consider two methods of deriving sample size requirements, respectively from Riley et al. (2020), and Pavlou et al. (2024). For each method of deriving sample size requirements, we need to supply the number of predictor parameters in the anticipated model, the overall prevalence of the outcome and the target discriminative skill or calibration level.

```{r}
params <- 10
prev <- 0.15
c_stat <- 0.85
calib <- 0.9
```

We can start by getting Riley et al.'s recommendations on sample size given these parameters:

```{r}
pm_samp <- pmsampsize(type = "b",
                     cstatistic = c_stat,
                     parameters = params,
                     prevalence = prev)
```

```{r}
pm_samp$results_table
```

Riley's recommendation in this scenario is 379, but we know from Pavlou (2024) that this is likely to be inadequate at this relatively high model strength. This paper recommends an increase of 50% in minimum sample size fo achieve a C-statistic of 0.85, and double the size for C=0.9.

(Parenthetically, Riley recommended a higher sample size of 547 at the lower C-statistic of 0.8. It would be good to understand why a smaller sample would be required for better discrimination. Pavlou et al. conclude that the Riley formula performs relatively well at lower strengths.)

Let's retrieve the recommendation from Pavlou et al.

```{r}
mp_samp <- samplesizedev(outcome = "Binary",
                        S = 0.9,      # calibration slope
                        phi = prev,
                        c = c_stat,
                        p = params)
```

```{r}
mp_samp$sim
```

The simulation here recommends a minimum sample size of 613 patients. At 15% prevalence this represents 92 events, and at 10 parameters is 9.2 EPP. This represents an increase of 61.6% in EPP compared with Riley.

Pavlou et al. provide expected performance statistics for the model given this set-up.

```{r}
expected_performance(outcome = "Binary", n = 613, phi = 0.15, c = 0.85, p = 10)
```

Hence we can expected to see a calibration slope of 0.901 from our model. This will help guide our assessment of internal validation.

## Simulation Data

Let's now generate some simulation data with a target prevalence of 15%, since this forms part of the sample size criteria above. To achieve 15% prevalence, we need:

$$
Pr(Y=1) = 0.15
$$

This is the marginal probability rather than the conditional on $X$. Another way to express this is that the expectation of the dependent variable, $Y$, should be 0.15:

$$
\mathbb{E}[Y] = 0.15
$$

We know that the dependent variable Y takes on values from the conditional distribution:

$$
Y|X \sim Bernoulli (p),
$$

where $p$ is the conditional probability of success, $p=Pr(Y=1 | X)$. From the definition of the logit transformation,

$$
log(\frac{p}{1-p}) = \alpha + \beta X
$$

we find p as the inverse-logit of the linear predictors

$$
p = logit^{-1} (\alpha + \beta X) = \frac{1}{1+e^{-(\alpha + \beta X)}},
$$

which we know is equal to the conditional expectation $\mathbb{E}[Y|X]$. Since $Pr(Y=1)=\mathbb{E}[Y]$, which in turn are equal to the expectation of the conditional probability of success, $\mathbb{E} [Pr(Y=1 | X)]$, we can substitute the logit model to find:

$$
\mathbb{E} [ \frac{1}{1+e^{-(\alpha + \beta X)}} ] = 0.15,
$$

where $\beta$ is a vector of coefficients and $X$ is a vector of predictor values.

Once we have chosen our values for $\beta$, we need to solve for $\alpha$ to find the value which guarantees the required level of prevalence, which we have chosen as 15%.

### Finding $\alpha$

In a sample setting, we do not have an analytic distribution over which to take an expectation, so instead we can take a simple average, as

$$
\frac{1}{n} \sum_{i=1}^{n} (\frac{1}{1+e^{-(\alpha + \beta X)}}) = 0.15.
$$

For the sake of simplicity let's choose 10 continuous predictors drawn from the standard normal distribution, $X_j \sim \mathcal{N} (0, 1)$. We'll start with Pavlou et al.'s recommendation on sample size.

```{r}
n <- 613          # sample size (Pavlou's suggestion)
```

Next we generate the predictors.

```{r}
X <- replicate(params, rnorm(n))          # 613*10 matrix
colnames(X) <- paste0("x", 1:p)
```

Set the beta coefficients including some zeroes.

```{r}
beta <- c(1.1, 0.8, 0.6, 0.4, 0.7, 0.7, 0.7, 0, 0, 0.1)
```

Solve for alpha so that logit($\eta$)) = prev. Technically, in R we're solving the following for $\alpha$:

$$
\frac{1}{n} \sum_{i=1}^{n} (\frac{1}{1 + e^{\alpha + \beta X}}) - 0.15 = 0
$$

```{r}
inv_logit <- function(z) 1/(1+exp(-z))
find_alpha  <- function(a) mean(inv_logit(a + as.vector(X %*% beta))) - prev
alpha    <- uniroot(find_alpha, c(-10, 10))$root
print(alpha)
```

With $\alpha = -2.7$ in hand, we can now generate probabilities and outcomes:

```{r}
pi <- inv_logit(alpha + as.vector(X %*% beta)) 
y <- rbinom(n, 1, pi)
sum(y)/length(y)
```

Fit logistic model and report simple diagnostics

```{r}
dat <- as.data.frame(X)
dat$y <- y 
m <- glm(y ~ ., data = dat, family = binomial())
m
```
Apparent calibration slope (Cox calibration: y ~ lp)
```{r}
lp <- predict(m, type = "link") 
cal_slope <- coef(glm(y ~ lp, data = dat, family = binomial()))
cal_slope
```
# AUC

```{r}
library(pROC)
```
```{r}
auc_val <- as.numeric(auc(roc(y, predict(m, type="response"), quiet=TRUE)))
list(realized_prevalence = mean(y), alpha = alpha, calibration_slope_app = cal_slope , AUC_app = auc_val )
```