[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Explorations in Binary Data",
    "section": "",
    "text": "Introduction\n\nHere I focus on measures of estimate uncertainty in Logistic Regression. Previous topics have been hidden for now.\n\n\nPlease navigate to Uncertainty.\n\n\nThis is a Quarto website. To learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This site has been set up as a temporary home for this analysis such that it can be shared with my supervisors.\nHow to do math in LaTeX:\nInline: \\(\\alpha\\)\nIndented:\n\\[\n\\hat{y}_i = \\beta_0 + \\sum_{j=1}^p \\beta_j x_{ji}\n\\]\n\nx = seq(0, 2*pi, by = 0.1)\ny = sin(x) + rnorm(length(x), 0, 0.3)\nplot(x,y) +\n  title(\"Sinusoidal Simulation\")\n\n\n\n\n\n\n\n\ninteger(0)"
  },
  {
    "objectID": "index.html#environment",
    "href": "index.html#environment",
    "title": "Heart Failure Analysis",
    "section": "Environment",
    "text": "Environment\nLoad the requisite libraries.\n\nlibrary(rms)\nlibrary(dplyr)\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(gtsummary)\nlibrary(ggplot2)\nlibrary(splines)\nlibrary(broom)\nlibrary(pROC)\nlibrary(knitr)\nlibrary(mgcv)\nlibrary(patchwork)"
  },
  {
    "objectID": "index.html#data",
    "href": "index.html#data",
    "title": "Heart Failure Analysis",
    "section": "Data",
    "text": "Data\nHaving loaded the various libraries needed, let’s import our heart failure data.\n\nsetwd(\"C:/Users/rmhimdi/OneDrive - University College London/Documents/HeartFailure\")\ndf &lt;- read_excel(\"simulateddata_LR_NL.xlsx\")\n\nDiscretise age and recode sex.\n\ndf$age &lt;- round(df$age, 0)\nn = 5\ndf &lt;- df %&gt;%\n  mutate(Age_ = paste0(\n    n*floor(age/n), \"-\",n*(floor(age/n)+1)\n    )\n  )\n#df$sex &lt;- factor(df$sex, levels = c(0, 1), labels = c(\"Male\", \"Female\")) \n## Assuming Female = 1 since this seems mildly protective (cf. MAGGIC), but will need to check.\n\nRenaming varibles.\n\ndf &lt;- \n  df %&gt;% \n  rename(\n    Outcome = outcome,\n    Age = age,\n    Sex = sex,\n    Creatinine = creatineDischarge,\n    Sodium = sodiumDischarge,\n    Potassium = potassiumDischarge,\n    Urea = ureaDischarge,\n    S_bp =  sbpAdmission,\n    HR = hrAdmission,\n    Hb = hbDischarge,\n    Diabetes = diabetes,\n    COPD = copd,\n    IHD = ihd, \n    ValveDisease = valveDisease,\n    NYHA_class = nyha,\n    PeripheralOedema = peripheralOedema,\n    AF = af,\n    eGFR = egfr_full\n  )\n\nInspect the processed data.\n\nkable(head(df))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOutcome\nAge\nCreatinine\nSodium\nPotassium\nUrea\nS_bp\nHR\nHb\nSex\nDiabetes\nCOPD\nIHD\nValveDisease\nNYHA_class\nPeripheralOedema\nAF\neGFR\nAge_\n\n\n\n\n0\n62\n76.99999\n138\n4.2\n19.6100\n113\n119\n11.1\n0\n1\n0\n1\n0\n1\n0\n1\n88.70592\n60-65\n\n\n1\n90\n142.00003\n146\n4.4\n20.9000\n98\n78\n12.1\n0\n0\n0\n1\n1\n0\n0\n0\n40.60656\n90-95\n\n\n1\n84\n143.00003\n141\n4.0\n15.7056\n132\n102\n13.7\n0\n0\n1\n0\n0\n1\n1\n0\n40.90517\n80-85\n\n\n0\n75\n80.00000\n137\n4.0\n7.5000\n152\n100\n11.6\n1\n0\n0\n0\n0\n1\n1\n0\n60.71780\n75-80\n\n\n0\n84\n123.99999\n139\n4.2\n8.2000\n201\n46\n11.8\n1\n0\n0\n1\n0\n0\n0\n0\n35.78796\n80-85\n\n\n0\n78\n118.00001\n140\n4.7\n12.7000\n104\n64\n10.6\n1\n0\n0\n0\n0\n1\n1\n1\n38.37403\n75-80\n\n\n\n\n\n\nHold Out\nDuring model evaluation, we will need to conduct predictive validation using the test set having estimated the model coefficients on the training set.\n\nset.seed(1729)\nrecords &lt;- dim(df)[1]\n\ntrain_size &lt;- floor(0.8*records)\ntrain_index &lt;- sample(seq_len(records), size = train_size)\ntrain &lt;- df[train_index,]\ntest &lt;- df[-train_index,]\nprint(paste(\"Size of training set is: \", nrow(train)))\n\n[1] \"Size of training set is:  43264\"\n\nprint(paste(\"Size of testing set is: \", nrow(test)))\n\n[1] \"Size of testing set is:  10817\""
  },
  {
    "objectID": "index.html#full-model",
    "href": "index.html#full-model",
    "title": "Heart Failure Analysis",
    "section": "Full Model",
    "text": "Full Model\nRun the naive model with all variables to see outline effect sizes.\n\n#Sans age categorisation\nfull_model &lt;- glm(Outcome ~ Age + Sex  + Creatinine + Sodium + Potassium + Urea + S_bp + HR + Hb+ Diabetes + COPD + IHD + ValveDisease + NYHA_class + PeripheralOedema + AF + eGFR, family = binomial, data = df)\nprint(summary(full_model))\n\n\nCall:\nglm(formula = Outcome ~ Age + Sex + Creatinine + Sodium + Potassium + \n    Urea + S_bp + HR + Hb + Diabetes + COPD + IHD + ValveDisease + \n    NYHA_class + PeripheralOedema + AF + eGFR, family = binomial, \n    data = df)\n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)      -7.1398644  0.4689428 -15.225  &lt; 2e-16 ***\nAge               0.0473564  0.0016273  29.102  &lt; 2e-16 ***\nSex               0.1904866  0.0316661   6.015 1.79e-09 ***\nCreatinine        0.0036828  0.0002298  16.024  &lt; 2e-16 ***\nSodium            0.0077452  0.0028043   2.762 0.005746 ** \nPotassium         0.3237376  0.0238317  13.584  &lt; 2e-16 ***\nUrea              0.0375205  0.0013179  28.469  &lt; 2e-16 ***\nS_bp             -0.0203323  0.0006165 -32.982  &lt; 2e-16 ***\nHR                0.0046018  0.0006626   6.945 3.78e-12 ***\nHb               -0.0256031  0.0075885  -3.374 0.000741 ***\nDiabetes         -0.1079845  0.0326271  -3.310 0.000934 ***\nCOPD              0.2200697  0.0363999   6.046 1.49e-09 ***\nIHD               0.0765564  0.0302083   2.534 0.011268 *  \nValveDisease      0.0518244  0.0321303   1.613 0.106757    \nNYHA_class       -0.0267937  0.0367931  -0.728 0.466475    \nPeripheralOedema  0.0914774  0.0304967   3.000 0.002704 ** \nAF               -0.0345800  0.0303462  -1.140 0.254488    \neGFR              0.0016651  0.0009438   1.764 0.077691 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 38473  on 54080  degrees of freedom\nResidual deviance: 32723  on 54063  degrees of freedom\nAIC: 32759\n\nNumber of Fisher Scoring iterations: 6\n\nprint(\"Odd Ratios\")\n\n[1] \"Odd Ratios\"\n\nkable(exp(coef(full_model)))\n\n\n\n\n\nx\n\n\n\n\n(Intercept)\n0.0007929\n\n\nAge\n1.0484956\n\n\nSex\n1.2098382\n\n\nCreatinine\n1.0036896\n\n\nSodium\n1.0077752\n\n\nPotassium\n1.3822845\n\n\nUrea\n1.0382333\n\n\nS_bp\n0.9798730\n\n\nHR\n1.0046124\n\n\nHb\n0.9747219\n\n\nDiabetes\n0.8976415\n\n\nCOPD\n1.2461636\n\n\nIHD\n1.0795631\n\n\nValveDisease\n1.0531908\n\n\nNYHA_class\n0.9735621\n\n\nPeripheralOedema\n1.0957920\n\n\nAF\n0.9660111\n\n\neGFR\n1.0016665\n\n\n\n\n\nWhile these Odds Ratios give us an outline picture of the effects, it is unlikely that all relationships are linear in the ‘true’ model. So, let’s try a few ways of relaxing the linearity assumption."
  },
  {
    "objectID": "index.html#age",
    "href": "index.html#age",
    "title": "Heart Failure Analysis",
    "section": "Age",
    "text": "Age\nLet’s try to identify any nonlinear age effect. We can do this in a few different ways.\n\nCategorisation\nLet’s start by using the age quintiles. We run two models to compare:\n\nmod1 &lt;- train %&gt;% \n  glm(formula = Outcome ~ Age, family = binomial)\nsummary(mod1)\n\n\nCall:\nglm(formula = Outcome ~ Age, family = binomial, data = .)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -5.647031   0.130533  -43.26   &lt;2e-16 ***\nAge          0.044565   0.001569   28.40   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 30626  on 43263  degrees of freedom\nResidual deviance: 29667  on 43262  degrees of freedom\nAIC: 29671\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nmod1 &lt;- train %&gt;% \n  glm(formula = Outcome ~ Age_, family = binomial)\nsummary(mod1)\n\n\nCall:\nglm(formula = Outcome ~ Age_, family = binomial, data = .)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -0.7867     0.1689  -4.657 3.21e-06 ***\nAge_105-110  -1.5159     1.0623  -1.427 0.153587    \nAge_15-20   -12.7794   267.7056  -0.048 0.961926    \nAge_20-25   -12.7794   101.1833  -0.126 0.899495    \nAge_25-30    -2.5092     0.7396  -3.392 0.000693 ***\nAge_30-35    -3.2993     0.7328  -4.503 6.71e-06 ***\nAge_35-40    -3.3297     0.6061  -5.494 3.93e-08 ***\nAge_40-45    -3.0072     0.4179  -7.197 6.17e-13 ***\nAge_45-50    -2.6675     0.2878  -9.268  &lt; 2e-16 ***\nAge_50-55    -2.6546     0.2487 -10.676  &lt; 2e-16 ***\nAge_55-60    -2.4627     0.2194 -11.225  &lt; 2e-16 ***\nAge_60-65    -1.9998     0.1932 -10.353  &lt; 2e-16 ***\nAge_65-70    -1.7313     0.1833  -9.444  &lt; 2e-16 ***\nAge_70-75    -1.7104     0.1776  -9.632  &lt; 2e-16 ***\nAge_75-80    -1.3749     0.1741  -7.898 2.84e-15 ***\nAge_80-85    -1.2114     0.1722  -7.034 2.01e-12 ***\nAge_85-90    -1.0309     0.1719  -5.997 2.01e-09 ***\nAge_90-95    -0.7899     0.1728  -4.572 4.84e-06 ***\nAge_95-100   -0.4525     0.1795  -2.520 0.011723 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 30626  on 43263  degrees of freedom\nResidual deviance: 29658  on 43245  degrees of freedom\nAIC: 29696\n\nNumber of Fisher Scoring iterations: 12\n\n\n\n\nSplines\n\nmod3 &lt;- glm(Outcome ~ ns(Age, df=3) + Sex  + ns(Creatinine, df=3) + Sodium + ns(Potassium, df=3) + ns(Urea, df=3) + S_bp + HR + Hb+ Diabetes + COPD + IHD + ValveDisease + NYHA_class + PeripheralOedema + AF + eGFR, family = binomial, data = train)\nprint(summary(mod3))\n\n\nCall:\nglm(formula = Outcome ~ ns(Age, df = 3) + Sex + ns(Creatinine, \n    df = 3) + Sodium + ns(Potassium, df = 3) + ns(Urea, df = 3) + \n    S_bp + HR + Hb + Diabetes + COPD + IHD + ValveDisease + NYHA_class + \n    PeripheralOedema + AF + eGFR, family = binomial, data = train)\n\nCoefficients:\n                          Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)             -3.2297212  0.8855541  -3.647 0.000265 ***\nns(Age, df = 3)1         2.2499750  0.2130202  10.562  &lt; 2e-16 ***\nns(Age, df = 3)2         5.3885694  0.8362700   6.444 1.17e-10 ***\nns(Age, df = 3)3         3.6383468  0.2490317  14.610  &lt; 2e-16 ***\nSex                      0.3452108  0.0552986   6.243 4.30e-10 ***\nns(Creatinine, df = 3)1  4.0797090  0.5571073   7.323 2.42e-13 ***\nns(Creatinine, df = 3)2  1.7759930  0.8101710   2.192 0.028371 *  \nns(Creatinine, df = 3)3 -1.2744470  0.7870926  -1.619 0.105408    \nSodium                   0.0051292  0.0031289   1.639 0.101149    \nns(Potassium, df = 3)1  -1.4475200  0.1351737 -10.709  &lt; 2e-16 ***\nns(Potassium, df = 3)2  -4.4924440  0.6021319  -7.461 8.59e-14 ***\nns(Potassium, df = 3)3   2.0966514  0.3195196   6.562 5.31e-11 ***\nns(Urea, df = 3)1        3.5024779  0.1396471  25.081  &lt; 2e-16 ***\nns(Urea, df = 3)2        3.5433371  0.2873837  12.330  &lt; 2e-16 ***\nns(Urea, df = 3)3        1.0218810  0.2459434   4.155 3.25e-05 ***\nS_bp                    -0.0189166  0.0006991 -27.060  &lt; 2e-16 ***\nHR                       0.0051112  0.0007596   6.729 1.71e-11 ***\nHb                      -0.0137585  0.0086356  -1.593 0.111108    \nDiabetes                -0.1036697  0.0375945  -2.758 0.005823 ** \nCOPD                     0.2105146  0.0419030   5.024 5.06e-07 ***\nIHD                      0.1033113  0.0345790   2.988 0.002811 ** \nValveDisease             0.0688294  0.0367263   1.874 0.060914 .  \nNYHA_class              -0.0464105  0.0421425  -1.101 0.270777    \nPeripheralOedema         0.0174441  0.0348879   0.500 0.617072    \nAF                      -0.0416977  0.0347090  -1.201 0.229615    \neGFR                     0.0156079  0.0031332   4.981 6.31e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 30626  on 43263  degrees of freedom\nResidual deviance: 25036  on 43238  degrees of freedom\nAIC: 25088\n\nNumber of Fisher Scoring iterations: 6\n\n\nHaving tried a few non-linear model options, it seems that we need a more systematic way to specify the functional form.\nFor this, we can visualise the empirical distribution of the dependent variable by the candidate covariates and inspect the shape.\n\ngam_model &lt;- gam(Outcome ~ s(Age) \n                   #+ s(Sex) + s(Creatinine) + s(Sodium) + s(Potassium) + s(Urea) + s(S_bp) + s(HR) + s(Hb) +\n                   #s(Diabetes) + s(COPD) + s(IHD) + s(ValveDisease) + s(NYHA_class) + s(PeripheralOedema) + s(AF) + s(eGFR)\n                   #, family = binomial\n                 , data = df)\nplot(gam_model)\n\n\n\n\n\n\n\n\n\nage_summary &lt;- df %&gt;%\n  group_by(Age) %&gt;%\n  summarise(\n    n = n(),\n    deaths = sum(Outcome),\n    mortality_rate = deaths / n,\n    log_odds = log((mortality_rate) / (1 - mortality_rate))  # small offset to avoid log(0)\n  )\n\nggplot(age_summary, aes(x = Age, y = log_odds)) +\n  geom_point(alpha = 0.6) +\n  geom_abline() +\n  geom_smooth(se = FALSE, method = \"loess\", span = 0.3) +\n  labs(\n    x = \"Age\",\n    y = \"Empirical log-odds of mortality\",\n    title = \"Empirical log-odds of mortality by Age\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 19 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\n\n\n\n\n\n\n\n\nplot_all_empirical_mortality_and_logodds &lt;- function(data, outcome, min_n = 10, smooth_span = 0.3, n_bins = 10, stratify_by = \"Age\") {\n  \n  outcome_sym &lt;- ensym(outcome)\n  outcome_chr &lt;- rlang::as_string(outcome_sym)\n  strat_chr &lt;- stratify_by\n  strat_sym &lt;- sym(strat_chr)\n  \n  predictors &lt;- setdiff(names(data), c(outcome_chr, strat_chr))\n  plots &lt;- list()\n  \n  for (pred in predictors) {\n    predictor_sym &lt;- ensym(pred)\n    pred_values &lt;- data %&gt;% pull(!!predictor_sym)\n    \n    # Skip if constant or fully missing\n    if (n_distinct(pred_values, na.rm = TRUE) &lt;= 1) next\n    \n    is_binary &lt;- all(pred_values %in% c(0, 1), na.rm = TRUE)\n    \n    if (is_binary) {\n      # Binary predictors now plotted against Age\n      df_binned &lt;- data %&gt;%\n        filter(!is.na(!!predictor_sym), !is.na(!!strat_sym), !is.na(!!outcome_sym)) %&gt;%\n        mutate(age_bin = cut_number(!!strat_sym, n = n_bins)) %&gt;%\n        group_by(!!predictor_sym, age_bin) %&gt;%\n        summarise(\n          mean_age = mean(!!strat_sym, na.rm = TRUE),\n          n = n(),\n          deaths = sum(!!outcome_sym),\n          mortality_rate = deaths / n,\n          log_odds = log((mortality_rate + 1e-3) / (1 - mortality_rate + 1e-3)),\n          .groups = \"drop\"\n        ) %&gt;%\n        filter(n &gt;= min_n)\n      \n      p1 &lt;- ggplot(df_binned, aes(x = mean_age, y = mortality_rate, color = factor(!!predictor_sym))) +\n        geom_point(alpha = 0.7) +\n        geom_line() +\n        labs(x = strat_chr, y = \"Mortality rate\", color = pred,\n             title = paste(\"Mortality by\", strat_chr, \"stratified by\", pred)) +\n        theme_minimal()\n      \n      p2 &lt;- ggplot(df_binned, aes(x = mean_age, y = log_odds, color = factor(!!predictor_sym))) +\n        geom_point(alpha = 0.7) +\n        geom_line() +\n        labs(x = strat_chr, y = \"Log-odds of mortality\", color = pred,\n             title = paste(\"Log-odds by\", strat_chr, \"stratified by\", pred)) +\n        theme_minimal()\n      \n    } else {\n      # Regular continuous predictors\n      unique_vals &lt;- n_distinct(pred_values, na.rm = TRUE)\n      \n      if (unique_vals &gt; 5) {\n        data_binned &lt;- data %&gt;%\n          mutate(bin = cut_number(!!predictor_sym, n = n_bins))\n        \n        df_summary &lt;- data_binned %&gt;%\n          group_by(bin) %&gt;%\n          summarise(\n            mean_value = mean(!!predictor_sym, na.rm = TRUE),\n            n = n(),\n            deaths = sum(!!outcome_sym),\n            mortality_rate = deaths / n,\n            log_odds = log((mortality_rate + 1e-3) / (1 - mortality_rate + 1e-3)),\n            .groups = \"drop\"\n          ) %&gt;%\n          filter(n &gt;= min_n)\n        \n        p1 &lt;- ggplot(df_summary, aes(x = mean_value, y = mortality_rate)) +\n          geom_point(alpha = 0.6) +\n          geom_line() +\n          labs(x = pred, y = \"Mortality rate\", title = paste(\"Binned mortality by\", pred)) +\n          theme_minimal()\n        \n        p2 &lt;- ggplot(df_summary, aes(x = mean_value, y = log_odds)) +\n          geom_point(alpha = 0.6) +\n          geom_line() +\n          labs(x = pred, y = \"Log-odds of mortality\", title = paste(\"Binned log-odds by\", pred)) +\n          theme_minimal()\n        \n      } else {\n        df_summary &lt;- data %&gt;%\n          group_by(value = !!predictor_sym) %&gt;%\n          summarise(\n            n = n(),\n            deaths = sum(!!outcome_sym),\n            mortality_rate = deaths / n,\n            log_odds = log((mortality_rate + 1e-3) / (1 - mortality_rate + 1e-3)),\n            .groups = \"drop\"\n          ) %&gt;%\n          filter(n &gt;= min_n)\n        \n        p1 &lt;- ggplot(df_summary, aes(x = value, y = mortality_rate)) +\n          geom_point(alpha = 0.6) +\n          geom_line() +\n          labs(x = pred, y = \"Mortality rate\", title = paste(\"Mortality by\", pred)) +\n          theme_minimal()\n        \n        p2 &lt;- ggplot(df_summary, aes(x = value, y = log_odds)) +\n          geom_point(alpha = 0.6) +\n          geom_line() +\n          labs(x = pred, y = \"Log-odds of mortality\", title = paste(\"Log-odds by\", pred)) +\n          theme_minimal()\n      }\n    }\n    \n    plots[[pred]] &lt;- p1 + p2\n  }\n  \n  return(plots)\n}\n\n\ndf = subset(df, select = -c(Age_))\nplot_all_empirical_mortality_and_logodds(data = df, outcome = Outcome)\n\n$Creatinine\n\n\n\n\n\n\n\n\n\n\n$Sodium\n\n\n\n\n\n\n\n\n\n\n$Potassium\n\n\n\n\n\n\n\n\n\n\n$Urea\n\n\n\n\n\n\n\n\n\n\n$S_bp\n\n\n\n\n\n\n\n\n\n\n$HR\n\n\n\n\n\n\n\n\n\n\n$Hb\n\n\n\n\n\n\n\n\n\n\n$Sex\n\n\n\n\n\n\n\n\n\n\n$Diabetes\n\n\n\n\n\n\n\n\n\n\n$COPD\n\n\n\n\n\n\n\n\n\n\n$IHD\n\n\n\n\n\n\n\n\n\n\n$ValveDisease\n\n\n\n\n\n\n\n\n\n\n$NYHA_class\n\n\n\n\n\n\n\n\n\n\n$PeripheralOedema\n\n\n\n\n\n\n\n\n\n\n$AF\n\n\n\n\n\n\n\n\n\n\n$eGFR\n\n\n\n\n\n\n\n\n\n\n\nHierarchical\nThis is a Quarto website. To learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "Validation.html",
    "href": "Validation.html",
    "title": "Validation",
    "section": "",
    "text": "set.seed(42)\nhf_data &lt;- read_excel(\"simulateddata_LR_NL.xlsx\") %&gt;% \n  rename(\n    Outcome = outcome,\n    Age = age,\n    Sex = sex,\n    Creatinine = creatineDischarge,\n    Sodium = sodiumDischarge,\n    Potassium = potassiumDischarge,\n    Urea = ureaDischarge,\n    S_bp =  sbpAdmission,\n    HR = hrAdmission,\n    Hb = hbDischarge,\n    Diabetes = diabetes,\n    COPD = copd,\n    IHD = ihd, \n    ValveDisease = valveDisease,\n    NYHA_class = nyha,\n    PeripheralOedema = peripheralOedema,\n    AF = af,\n    eGFR = egfr_full\n  )\n\n\ntrain_indices &lt;- sample(seq_len(nrow(hf_data)), size = 0.7 * nrow(hf_data))\nhf_train &lt;- hf_data[train_indices, ]\nhf_test  &lt;- hf_data[-train_indices, ]"
  },
  {
    "objectID": "Validation.html#data-preparation-and-splitting",
    "href": "Validation.html#data-preparation-and-splitting",
    "title": "Validation",
    "section": "",
    "text": "set.seed(42)\nhf_data &lt;- read_excel(\"simulateddata_LR_NL.xlsx\") %&gt;% \n  rename(\n    Outcome = outcome,\n    Age = age,\n    Sex = sex,\n    Creatinine = creatineDischarge,\n    Sodium = sodiumDischarge,\n    Potassium = potassiumDischarge,\n    Urea = ureaDischarge,\n    S_bp =  sbpAdmission,\n    HR = hrAdmission,\n    Hb = hbDischarge,\n    Diabetes = diabetes,\n    COPD = copd,\n    IHD = ihd, \n    ValveDisease = valveDisease,\n    NYHA_class = nyha,\n    PeripheralOedema = peripheralOedema,\n    AF = af,\n    eGFR = egfr_full\n  )\n\n\ntrain_indices &lt;- sample(seq_len(nrow(hf_data)), size = 0.7 * nrow(hf_data))\nhf_train &lt;- hf_data[train_indices, ]\nhf_test  &lt;- hf_data[-train_indices, ]"
  },
  {
    "objectID": "Validation.html#model-fitting-training-set-with-non-linear-age",
    "href": "Validation.html#model-fitting-training-set-with-non-linear-age",
    "title": "Validation",
    "section": "Model Fitting (Training Set) with Non-linear Age",
    "text": "Model Fitting (Training Set) with Non-linear Age\n\ndd &lt;- datadist(hf_train)\noptions(datadist = \"dd\")\n\nmodel_train &lt;- lrm(Outcome ~ \n                     rcs(Age, 4) + \n                     Sex + \n                     rcs(Urea, 4) + \n                     rcs(Creatinine, 4) + \n                     rcs(Potassium, 4) +\n                     IHD + COPD + Diabetes + ValveDisease + NYHA_class,\n                   data = hf_train,\n                   x = TRUE, y = TRUE)"
  },
  {
    "objectID": "Validation.html#discrimination-roc-and-auc-test-set",
    "href": "Validation.html#discrimination-roc-and-auc-test-set",
    "title": "Validation",
    "section": "Discrimination (ROC and AUC) – Test Set",
    "text": "Discrimination (ROC and AUC) – Test Set\n\npred_test &lt;- predict(model_train, newdata = hf_test, type = \"fitted\")\nroc_test &lt;- roc(hf_test$Outcome, pred_test)\nauc_test &lt;- auc(roc_test)\n\nroc_df &lt;- data.frame(\n  specificity = roc_test$specificities,\n  sensitivity = roc_test$sensitivities\n)\n\nggplot(roc_df, aes(x = 1 - specificity, y = sensitivity)) +\n  geom_line(color = \"steelblue\", size = 1.2) +\n  geom_abline(linetype = \"dashed\") +\n  theme_minimal() +\n  labs(title = paste(\"Test Set ROC Curve (AUC =\", round(auc_test, 3), \")\"),\n       x = \"1 - Specificity\", y = \"Sensitivity\")"
  },
  {
    "objectID": "Validation.html#calibration-smoothed-test-set",
    "href": "Validation.html#calibration-smoothed-test-set",
    "title": "Validation",
    "section": "Calibration – Smoothed (Test Set)",
    "text": "Calibration – Smoothed (Test Set)\n\ncalib_df &lt;- hf_test %&gt;%\n  mutate(pred = pred_test)\n\nggplot(calib_df, aes(x = pred, y = Outcome)) +\n  geom_smooth(method = \"loess\", se = TRUE, color = \"firebrick\", fill = \"pink\", span = 0.75) +\n  geom_abline(linetype = \"dashed\") +\n  theme_minimal() +\n  labs(title = \"Calibration Curve (Test Set)\",\n       x = \"Predicted Probability\",\n       y = \"Observed Mortality\")"
  },
  {
    "objectID": "Validation.html#goodness-of-fit-summary-training-vs-test-set",
    "href": "Validation.html#goodness-of-fit-summary-training-vs-test-set",
    "title": "Validation",
    "section": "Goodness-of-Fit Summary: Training vs Test Set",
    "text": "Goodness-of-Fit Summary: Training vs Test Set\n\n# Training set predictions\npred_train &lt;- predict(model_train, type = \"fitted\")\n\n# Brier Score\nbrier_train &lt;- mean((hf_train$Outcome - pred_train)^2)\nbrier_test &lt;- mean((hf_test$Outcome - pred_test)^2)\n\n# Calibration slope and intercept (test set)\ncal_slope_model &lt;- glm(Outcome ~ offset(qlogis(pred_test)), data = hf_test, family = binomial)\ncal_intercept_model &lt;- glm(Outcome ~ I(qlogis(pred_test)), data = hf_test, family = binomial)\n\ncal_slope &lt;- coef(cal_slope_model)\ncal_intercept &lt;- coef(cal_intercept_model)\n\n# AUC\nroc_train &lt;- roc(hf_train$Outcome, pred_train)\nauc_train &lt;- auc(roc_train)\n\n# Summary table\nsummary_df &lt;- tibble::tibble(\n  Metric = c(\"AUROC\", \"Calibration Intercept\", \"Calibration Slope\", \"Brier Score\"),\n  `Training Set` = c(round(auc_train, 3), 0, 1, round(brier_train, 3)),\n  `Test Set` = c(round(auc_test, 3), round(cal_intercept[1], 3), round(cal_slope[1], 3), round(brier_test, 3))\n)\n\nsummary_df\n\n# A tibble: 4 × 3\n  Metric                `Training Set` `Test Set`\n  &lt;chr&gt;                          &lt;dbl&gt;      &lt;dbl&gt;\n1 AUROC                          0.768      0.769\n2 Calibration Intercept          0          0.033\n3 Calibration Slope              1          0.01 \n4 Brier Score                    0.088      0.088"
  },
  {
    "objectID": "uncertainty.html",
    "href": "uncertainty.html",
    "title": "Uncertainty",
    "section": "",
    "text": "a &lt;- runif(1000, -pi, pi)\nb &lt;- rnorm(1000, 0, 0.3) + sin(a)\nplot(a, b)"
  },
  {
    "objectID": "Nonlinearity.html",
    "href": "Nonlinearity.html",
    "title": "Nonlinearity",
    "section": "",
    "text": "In this document, I build several models of the heart failure data that are able to handle non-linearities in the predictors, generate predictions and perform model validation."
  },
  {
    "objectID": "Nonlinearity.html#environment",
    "href": "Nonlinearity.html#environment",
    "title": "Nonlinearity",
    "section": "Environment",
    "text": "Environment\nLoad the requisite libraries.\n\nlibrary(rms)\nlibrary(dplyr)\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(gtsummary)\nlibrary(ggplot2)\nlibrary(splines)\nlibrary(broom)\nlibrary(pROC)\nlibrary(knitr)\nlibrary(mgcv)\nlibrary(patchwork)"
  },
  {
    "objectID": "Nonlinearity.html#data",
    "href": "Nonlinearity.html#data",
    "title": "Nonlinearity",
    "section": "Data",
    "text": "Data\nHaving loaded the various libraries needed, let’s import our heart failure data.\n\nsetwd(\"C:/Users/rmhimdi/OneDrive - University College London/Documents/HeartFailure\")\ndf &lt;- read_excel(\"simulateddata_LR_NL.xlsx\")\n\nDiscretise age and recode sex.\n\ndf$age &lt;- round(df$age, 0)\nn = 5\ndf &lt;- df %&gt;%\n  mutate(Age_ = paste0(\n    n*floor(age/n), \"-\",n*(floor(age/n)+1)\n    )\n  )\n#df$sex &lt;- factor(df$sex, levels = c(0, 1), labels = c(\"Male\", \"Female\")) \n## Assuming Female = 1 since this seems mildly protective (cf. MAGGIC), but will need to check.\n\nRenaming varibles.\n\ndf &lt;- \n  df %&gt;% \n  rename(\n    Outcome = outcome,\n    Age = age,\n    Sex = sex,\n    Creatinine = creatineDischarge,\n    Sodium = sodiumDischarge,\n    Potassium = potassiumDischarge,\n    Urea = ureaDischarge,\n    S_bp =  sbpAdmission,\n    HR = hrAdmission,\n    Hb = hbDischarge,\n    Diabetes = diabetes,\n    COPD = copd,\n    IHD = ihd, \n    ValveDisease = valveDisease,\n    NYHA_class = nyha,\n    PeripheralOedema = peripheralOedema,\n    AF = af,\n    eGFR = egfr_full\n  )\n\nInspect the processed data.\n\nkable(head(df))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOutcome\nAge\nCreatinine\nSodium\nPotassium\nUrea\nS_bp\nHR\nHb\nSex\nDiabetes\nCOPD\nIHD\nValveDisease\nNYHA_class\nPeripheralOedema\nAF\neGFR\nAge_\n\n\n\n\n0\n62\n76.99999\n138\n4.2\n19.6100\n113\n119\n11.1\n0\n1\n0\n1\n0\n1\n0\n1\n88.70592\n60-65\n\n\n1\n90\n142.00003\n146\n4.4\n20.9000\n98\n78\n12.1\n0\n0\n0\n1\n1\n0\n0\n0\n40.60656\n90-95\n\n\n1\n84\n143.00003\n141\n4.0\n15.7056\n132\n102\n13.7\n0\n0\n1\n0\n0\n1\n1\n0\n40.90517\n80-85\n\n\n0\n75\n80.00000\n137\n4.0\n7.5000\n152\n100\n11.6\n1\n0\n0\n0\n0\n1\n1\n0\n60.71780\n75-80\n\n\n0\n84\n123.99999\n139\n4.2\n8.2000\n201\n46\n11.8\n1\n0\n0\n1\n0\n0\n0\n0\n35.78796\n80-85\n\n\n0\n78\n118.00001\n140\n4.7\n12.7000\n104\n64\n10.6\n1\n0\n0\n0\n0\n1\n1\n1\n38.37403\n75-80\n\n\n\n\n\n\nHold Out\nDuring model evaluation, we will need to conduct predictive validation using the test set having estimated the model coefficients on the training set.\n\nset.seed(1729)\nrecords &lt;- dim(df)[1]\n\ntrain_size &lt;- floor(0.8*records)\ntrain_index &lt;- sample(seq_len(records), size = train_size)\ntrain &lt;- df[train_index,]\ntest &lt;- df[-train_index,]\nprint(paste(\"Size of training set is: \", nrow(train)))\n\n[1] \"Size of training set is:  43264\"\n\nprint(paste(\"Size of testing set is: \", nrow(test)))\n\n[1] \"Size of testing set is:  10817\""
  },
  {
    "objectID": "Nonlinearity.html#full-model",
    "href": "Nonlinearity.html#full-model",
    "title": "Nonlinearity",
    "section": "Full Model",
    "text": "Full Model\nRun the naive model with all variables to see outline effect sizes.\n\n#Sans age categorisation\nfull_model &lt;- glm(Outcome ~ Age + Sex  + Creatinine + Sodium + Potassium + Urea + S_bp + HR + Hb+ Diabetes + COPD + IHD + ValveDisease + NYHA_class + PeripheralOedema + AF + eGFR, family = binomial, data = df)\nprint(summary(full_model))\n\n\nCall:\nglm(formula = Outcome ~ Age + Sex + Creatinine + Sodium + Potassium + \n    Urea + S_bp + HR + Hb + Diabetes + COPD + IHD + ValveDisease + \n    NYHA_class + PeripheralOedema + AF + eGFR, family = binomial, \n    data = df)\n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)      -7.1398644  0.4689428 -15.225  &lt; 2e-16 ***\nAge               0.0473564  0.0016273  29.102  &lt; 2e-16 ***\nSex               0.1904866  0.0316661   6.015 1.79e-09 ***\nCreatinine        0.0036828  0.0002298  16.024  &lt; 2e-16 ***\nSodium            0.0077452  0.0028043   2.762 0.005746 ** \nPotassium         0.3237376  0.0238317  13.584  &lt; 2e-16 ***\nUrea              0.0375205  0.0013179  28.469  &lt; 2e-16 ***\nS_bp             -0.0203323  0.0006165 -32.982  &lt; 2e-16 ***\nHR                0.0046018  0.0006626   6.945 3.78e-12 ***\nHb               -0.0256031  0.0075885  -3.374 0.000741 ***\nDiabetes         -0.1079845  0.0326271  -3.310 0.000934 ***\nCOPD              0.2200697  0.0363999   6.046 1.49e-09 ***\nIHD               0.0765564  0.0302083   2.534 0.011268 *  \nValveDisease      0.0518244  0.0321303   1.613 0.106757    \nNYHA_class       -0.0267937  0.0367931  -0.728 0.466475    \nPeripheralOedema  0.0914774  0.0304967   3.000 0.002704 ** \nAF               -0.0345800  0.0303462  -1.140 0.254488    \neGFR              0.0016651  0.0009438   1.764 0.077691 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 38473  on 54080  degrees of freedom\nResidual deviance: 32723  on 54063  degrees of freedom\nAIC: 32759\n\nNumber of Fisher Scoring iterations: 6\n\nprint(\"Odd Ratios\")\n\n[1] \"Odd Ratios\"\n\nkable(exp(coef(full_model)))\n\n\n\n\n\nx\n\n\n\n\n(Intercept)\n0.0007929\n\n\nAge\n1.0484956\n\n\nSex\n1.2098382\n\n\nCreatinine\n1.0036896\n\n\nSodium\n1.0077752\n\n\nPotassium\n1.3822845\n\n\nUrea\n1.0382333\n\n\nS_bp\n0.9798730\n\n\nHR\n1.0046124\n\n\nHb\n0.9747219\n\n\nDiabetes\n0.8976415\n\n\nCOPD\n1.2461636\n\n\nIHD\n1.0795631\n\n\nValveDisease\n1.0531908\n\n\nNYHA_class\n0.9735621\n\n\nPeripheralOedema\n1.0957920\n\n\nAF\n0.9660111\n\n\neGFR\n1.0016665\n\n\n\n\n\nWhile these Odds Ratios give us an outline picture of the effects, it is unlikely that all relationships are linear in the ‘true’ model. So, let’s try a few ways of relaxing the linearity assumption."
  },
  {
    "objectID": "Nonlinearity.html#age",
    "href": "Nonlinearity.html#age",
    "title": "Nonlinearity",
    "section": "Age",
    "text": "Age\nLet’s try to identify any nonlinear age effect. We can do this in a few different ways.\n\nCategorisation\nLet’s start by using the age quintiles. We run two models to compare:\n\nmod1 &lt;- train %&gt;% \n  glm(formula = Outcome ~ Age, family = binomial)\nsummary(mod1)\n\n\nCall:\nglm(formula = Outcome ~ Age, family = binomial, data = .)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -5.647031   0.130533  -43.26   &lt;2e-16 ***\nAge          0.044565   0.001569   28.40   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 30626  on 43263  degrees of freedom\nResidual deviance: 29667  on 43262  degrees of freedom\nAIC: 29671\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nmod1 &lt;- train %&gt;% \n  glm(formula = Outcome ~ Age_, family = binomial)\nsummary(mod1)\n\n\nCall:\nglm(formula = Outcome ~ Age_, family = binomial, data = .)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -0.7867     0.1689  -4.657 3.21e-06 ***\nAge_105-110  -1.5159     1.0623  -1.427 0.153587    \nAge_15-20   -12.7794   267.7056  -0.048 0.961926    \nAge_20-25   -12.7794   101.1833  -0.126 0.899495    \nAge_25-30    -2.5092     0.7396  -3.392 0.000693 ***\nAge_30-35    -3.2993     0.7328  -4.503 6.71e-06 ***\nAge_35-40    -3.3297     0.6061  -5.494 3.93e-08 ***\nAge_40-45    -3.0072     0.4179  -7.197 6.17e-13 ***\nAge_45-50    -2.6675     0.2878  -9.268  &lt; 2e-16 ***\nAge_50-55    -2.6546     0.2487 -10.676  &lt; 2e-16 ***\nAge_55-60    -2.4627     0.2194 -11.225  &lt; 2e-16 ***\nAge_60-65    -1.9998     0.1932 -10.353  &lt; 2e-16 ***\nAge_65-70    -1.7313     0.1833  -9.444  &lt; 2e-16 ***\nAge_70-75    -1.7104     0.1776  -9.632  &lt; 2e-16 ***\nAge_75-80    -1.3749     0.1741  -7.898 2.84e-15 ***\nAge_80-85    -1.2114     0.1722  -7.034 2.01e-12 ***\nAge_85-90    -1.0309     0.1719  -5.997 2.01e-09 ***\nAge_90-95    -0.7899     0.1728  -4.572 4.84e-06 ***\nAge_95-100   -0.4525     0.1795  -2.520 0.011723 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 30626  on 43263  degrees of freedom\nResidual deviance: 29658  on 43245  degrees of freedom\nAIC: 29696\n\nNumber of Fisher Scoring iterations: 12\n\n\n\n\nSplines\n\nmod3 &lt;- glm(Outcome ~ ns(Age, df=3) + Sex  + ns(Creatinine, df=3) + Sodium + ns(Potassium, df=3) + ns(Urea, df=3) + S_bp + HR + Hb+ Diabetes + COPD + IHD + ValveDisease + NYHA_class + PeripheralOedema + AF + eGFR, family = binomial, data = train)\nprint(summary(mod3))\n\n\nCall:\nglm(formula = Outcome ~ ns(Age, df = 3) + Sex + ns(Creatinine, \n    df = 3) + Sodium + ns(Potassium, df = 3) + ns(Urea, df = 3) + \n    S_bp + HR + Hb + Diabetes + COPD + IHD + ValveDisease + NYHA_class + \n    PeripheralOedema + AF + eGFR, family = binomial, data = train)\n\nCoefficients:\n                          Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)             -3.2297212  0.8855541  -3.647 0.000265 ***\nns(Age, df = 3)1         2.2499750  0.2130202  10.562  &lt; 2e-16 ***\nns(Age, df = 3)2         5.3885694  0.8362700   6.444 1.17e-10 ***\nns(Age, df = 3)3         3.6383468  0.2490317  14.610  &lt; 2e-16 ***\nSex                      0.3452108  0.0552986   6.243 4.30e-10 ***\nns(Creatinine, df = 3)1  4.0797090  0.5571073   7.323 2.42e-13 ***\nns(Creatinine, df = 3)2  1.7759930  0.8101710   2.192 0.028371 *  \nns(Creatinine, df = 3)3 -1.2744470  0.7870926  -1.619 0.105408    \nSodium                   0.0051292  0.0031289   1.639 0.101149    \nns(Potassium, df = 3)1  -1.4475200  0.1351737 -10.709  &lt; 2e-16 ***\nns(Potassium, df = 3)2  -4.4924440  0.6021319  -7.461 8.59e-14 ***\nns(Potassium, df = 3)3   2.0966514  0.3195196   6.562 5.31e-11 ***\nns(Urea, df = 3)1        3.5024779  0.1396471  25.081  &lt; 2e-16 ***\nns(Urea, df = 3)2        3.5433371  0.2873837  12.330  &lt; 2e-16 ***\nns(Urea, df = 3)3        1.0218810  0.2459434   4.155 3.25e-05 ***\nS_bp                    -0.0189166  0.0006991 -27.060  &lt; 2e-16 ***\nHR                       0.0051112  0.0007596   6.729 1.71e-11 ***\nHb                      -0.0137585  0.0086356  -1.593 0.111108    \nDiabetes                -0.1036697  0.0375945  -2.758 0.005823 ** \nCOPD                     0.2105146  0.0419030   5.024 5.06e-07 ***\nIHD                      0.1033113  0.0345790   2.988 0.002811 ** \nValveDisease             0.0688294  0.0367263   1.874 0.060914 .  \nNYHA_class              -0.0464105  0.0421425  -1.101 0.270777    \nPeripheralOedema         0.0174441  0.0348879   0.500 0.617072    \nAF                      -0.0416977  0.0347090  -1.201 0.229615    \neGFR                     0.0156079  0.0031332   4.981 6.31e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 30626  on 43263  degrees of freedom\nResidual deviance: 25036  on 43238  degrees of freedom\nAIC: 25088\n\nNumber of Fisher Scoring iterations: 6\n\n\nHaving tried a few non-linear model options, it seems that we need a more systematic way to specify the functional form.\nFor this, we can visualise the empirical distribution of the dependent variable by the candidate covariates and inspect the shape.\n\ngam_model &lt;- gam(Outcome ~ s(Age) \n                   #+ s(Sex) + s(Creatinine) + s(Sodium) + s(Potassium) + s(Urea) + s(S_bp) + s(HR) + s(Hb) +\n                   #s(Diabetes) + s(COPD) + s(IHD) + s(ValveDisease) + s(NYHA_class) + s(PeripheralOedema) + s(AF) + s(eGFR)\n                   #, family = binomial\n                 , data = df)\nplot(gam_model)\n\n\n\n\n\n\n\n\n\nage_summary &lt;- df %&gt;%\n  group_by(Age) %&gt;%\n  summarise(\n    n = n(),\n    deaths = sum(Outcome),\n    mortality_rate = deaths / n,\n    log_odds = log((mortality_rate) / (1 - mortality_rate))  # small offset to avoid log(0)\n  )\n\nggplot(age_summary, aes(x = Age, y = log_odds)) +\n  geom_point(alpha = 0.6) +\n  geom_abline() +\n  geom_smooth(se = FALSE, method = \"loess\", span = 0.3) +\n  labs(\n    x = \"Age\",\n    y = \"Empirical log-odds of mortality\",\n    title = \"Empirical log-odds of mortality by Age\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 19 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\n\n\n\n\n\n\n\n\nplot_all_empirical_mortality_and_logodds &lt;- function(data, outcome, min_n = 10, smooth_span = 0.3, n_bins = 10, stratify_by = \"Age\") {\n  \n  outcome_sym &lt;- ensym(outcome)\n  outcome_chr &lt;- rlang::as_string(outcome_sym)\n  strat_chr &lt;- stratify_by\n  strat_sym &lt;- sym(strat_chr)\n  \n  predictors &lt;- setdiff(names(data), c(outcome_chr, strat_chr))\n  plots &lt;- list()\n  \n  for (pred in predictors) {\n    predictor_sym &lt;- ensym(pred)\n    pred_values &lt;- data %&gt;% pull(!!predictor_sym)\n    \n    # Skip if constant or fully missing\n    if (n_distinct(pred_values, na.rm = TRUE) &lt;= 1) next\n    \n    is_binary &lt;- all(pred_values %in% c(0, 1), na.rm = TRUE)\n    \n    if (is_binary) {\n      # Binary predictors now plotted against Age\n      df_binned &lt;- data %&gt;%\n        filter(!is.na(!!predictor_sym), !is.na(!!strat_sym), !is.na(!!outcome_sym)) %&gt;%\n        mutate(age_bin = cut_number(!!strat_sym, n = n_bins)) %&gt;%\n        group_by(!!predictor_sym, age_bin) %&gt;%\n        summarise(\n          mean_age = mean(!!strat_sym, na.rm = TRUE),\n          n = n(),\n          deaths = sum(!!outcome_sym),\n          mortality_rate = deaths / n,\n          log_odds = log((mortality_rate + 1e-3) / (1 - mortality_rate + 1e-3)),\n          .groups = \"drop\"\n        ) %&gt;%\n        filter(n &gt;= min_n)\n      \n      p1 &lt;- ggplot(df_binned, aes(x = mean_age, y = mortality_rate, color = factor(!!predictor_sym))) +\n        geom_point(alpha = 0.7) +\n        geom_line() +\n        labs(x = strat_chr, y = \"Mortality rate\", color = pred,\n             title = paste(\"Mortality by\", strat_chr, \"stratified by\", pred)) +\n        theme_minimal()\n      \n      p2 &lt;- ggplot(df_binned, aes(x = mean_age, y = log_odds, color = factor(!!predictor_sym))) +\n        geom_point(alpha = 0.7) +\n        geom_line() +\n        labs(x = strat_chr, y = \"Log-odds of mortality\", color = pred,\n             title = paste(\"Log-odds by\", strat_chr, \"stratified by\", pred)) +\n        theme_minimal()\n      \n    } else {\n      # Regular continuous predictors\n      unique_vals &lt;- n_distinct(pred_values, na.rm = TRUE)\n      \n      if (unique_vals &gt; 5) {\n        data_binned &lt;- data %&gt;%\n          mutate(bin = cut_number(!!predictor_sym, n = n_bins))\n        \n        df_summary &lt;- data_binned %&gt;%\n          group_by(bin) %&gt;%\n          summarise(\n            mean_value = mean(!!predictor_sym, na.rm = TRUE),\n            n = n(),\n            deaths = sum(!!outcome_sym),\n            mortality_rate = deaths / n,\n            log_odds = log((mortality_rate + 1e-3) / (1 - mortality_rate + 1e-3)),\n            .groups = \"drop\"\n          ) %&gt;%\n          filter(n &gt;= min_n)\n        \n        p1 &lt;- ggplot(df_summary, aes(x = mean_value, y = mortality_rate)) +\n          geom_point(alpha = 0.6) +\n          geom_line() +\n          labs(x = pred, y = \"Mortality rate\", title = paste(\"Binned mortality by\", pred)) +\n          theme_minimal()\n        \n        p2 &lt;- ggplot(df_summary, aes(x = mean_value, y = log_odds)) +\n          geom_point(alpha = 0.6) +\n          geom_line() +\n          labs(x = pred, y = \"Log-odds of mortality\", title = paste(\"Binned log-odds by\", pred)) +\n          theme_minimal()\n        \n      } else {\n        df_summary &lt;- data %&gt;%\n          group_by(value = !!predictor_sym) %&gt;%\n          summarise(\n            n = n(),\n            deaths = sum(!!outcome_sym),\n            mortality_rate = deaths / n,\n            log_odds = log((mortality_rate + 1e-3) / (1 - mortality_rate + 1e-3)),\n            .groups = \"drop\"\n          ) %&gt;%\n          filter(n &gt;= min_n)\n        \n        p1 &lt;- ggplot(df_summary, aes(x = value, y = mortality_rate)) +\n          geom_point(alpha = 0.6) +\n          geom_line() +\n          labs(x = pred, y = \"Mortality rate\", title = paste(\"Mortality by\", pred)) +\n          theme_minimal()\n        \n        p2 &lt;- ggplot(df_summary, aes(x = value, y = log_odds)) +\n          geom_point(alpha = 0.6) +\n          geom_line() +\n          labs(x = pred, y = \"Log-odds of mortality\", title = paste(\"Log-odds by\", pred)) +\n          theme_minimal()\n      }\n    }\n    \n    plots[[pred]] &lt;- p1 + p2\n  }\n  \n  return(plots)\n}\n\n\ndf = subset(df, select = -c(Age_))\nplot_all_empirical_mortality_and_logodds(data = df, outcome = Outcome)\n\n$Creatinine\n\n\n\n\n\n\n\n\n\n\n$Sodium\n\n\n\n\n\n\n\n\n\n\n$Potassium\n\n\n\n\n\n\n\n\n\n\n$Urea\n\n\n\n\n\n\n\n\n\n\n$S_bp\n\n\n\n\n\n\n\n\n\n\n$HR\n\n\n\n\n\n\n\n\n\n\n$Hb\n\n\n\n\n\n\n\n\n\n\n$Sex\n\n\n\n\n\n\n\n\n\n\n$Diabetes\n\n\n\n\n\n\n\n\n\n\n$COPD\n\n\n\n\n\n\n\n\n\n\n$IHD\n\n\n\n\n\n\n\n\n\n\n$ValveDisease\n\n\n\n\n\n\n\n\n\n\n$NYHA_class\n\n\n\n\n\n\n\n\n\n\n$PeripheralOedema\n\n\n\n\n\n\n\n\n\n\n$AF\n\n\n\n\n\n\n\n\n\n\n$eGFR\n\n\n\n\n\n\n\n\n\n\n\nHierarchical\nThis is a Quarto website. To learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "uncertainty.html#manual-exploration-of-logistic-regression",
    "href": "uncertainty.html#manual-exploration-of-logistic-regression",
    "title": "Some Topics in Logistic Regression",
    "section": "Manual Exploration of Logistic Regression",
    "text": "Manual Exploration of Logistic Regression\n\n1. Introduction\nWe manually explore logistic regression, highlighting key concepts of estimation, uncertainty, and validation. We use a manually simulated dataset to illustrate each step clearly.\n\n\n2. Simulating Data\nWe simulate data with one predictor:\n\\[\nP(Y = 1 \\mid X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X)}}\n\\]\nwith parameters:\n\n\\(\\beta_0 = -0.5\\)\n\\(\\beta_1 = 2\\).\n\n\n\nR code:\nset.seed(123)\nn &lt;- 100\nbeta_0 &lt;- -0.5\nbeta_1 &lt;- 2\nx &lt;- rnorm(n)\neta &lt;- beta_0 + beta_1 * x\nprob &lt;- 1 / (1 + exp(-eta))\ny &lt;- rbinom(n, 1, prob)\nsim_data &lt;- data.frame(y, x)\n\n\n\n3. Logistic Model and Likelihood\nThe logistic model is:\n\\[\nP(Y = 1 \\mid X, \\beta) = \\frac{1}{1 + e^{-X\\beta}}\n\\]\nNote: include likelihood version from written notes. Log-likelihood:\n\\[\n\\ell(\\beta) = \\sum_{i=1}^{n} \\left[ y_i(X_i \\beta) - \\log(1 + e^{X_i \\beta}) \\right]\n\\]\n\n\n\n4. Gradient and Hessian\nGradient (score function):\n\\[\n\\nabla \\ell(\\beta) = X^T(y - p)\n\\]\nHessian:\n\\[\nH(\\beta) = -X^T W X\n\\]\nwhere \\(W\\) is diagonal with elements \\(p_i(1 - p_i)\\).\n\n\nR code for Gradient and Hessian:\nX &lt;- cbind(1, x)\nbeta &lt;- c(0, 0)\neta &lt;- X %*% beta\np &lt;- 1 / (1 + exp(-eta))\ngradient &lt;- t(X) %*% (y - p)\nW &lt;- diag(as.vector(p * (1 - p)))\nhessian &lt;- -t(X) %*% W %*% X\n\n\n\n5. Newton-Raphson Algorithm\nIteration step:\n\\[\n\\beta_{\\text{new}} = \\beta_{\\text{old}} - [H(\\beta_{\\text{old}})]^{-1} \\nabla \\ell(\\beta_{\\text{old}})\n\\]\n\n\nR implementation:\nbeta &lt;- c(0, 0)\nfor (i in 1:10) {\n  eta &lt;- X %*% beta\n  p &lt;- 1 / (1 + exp(-eta))\n  gradient &lt;- t(X) %*% (y - p)\n  W &lt;- diag(as.vector(p * (1 - p)))\n  hessian &lt;- -t(X) %*% W %*% X\n  beta &lt;- beta - solve(hessian) %*% gradient\n}\nprint(beta)\n\n\n\n6. Variance of Estimates\nEstimated covariance matrix:\n\\[\n\\text{Var}(\\hat{\\beta}) = (X^T W X)^{-1}\n\\]\n\n\n\n7. Bootstrap Validation\nRepeated sampling with replacement to estimate standard errors.\n\n\nBootstrap R code:\nB &lt;- 500\nbootstrap_estimates &lt;- matrix(NA, B, 2)\n\nfor (b in 1:B) {\n  idx &lt;- sample(1:n, replace = TRUE)\n  X_b &lt;- X[idx, ]\n  y_b &lt;- y[idx]\n  beta_b &lt;- glm(y_b ~ X_b[,2], family=\"binomial\")$coefficients\n  bootstrap_estimates[b, ] &lt;- beta_b\n}\n\napply(bootstrap_estimates, 2, sd)\n\n\n\n8. Cross-validation\nPartition data into $K$ folds, train/test each fold.\n\n\nExample 5-fold CV in R:\nK &lt;- 5\nfolds &lt;- sample(rep(1:K, length.out = n))\ncv_error &lt;- numeric(K)\n\nfor (k in 1:K) {\n  train_idx &lt;- which(folds != k)\n  test_idx &lt;- which(folds == k)\n  fit &lt;- glm(y ~ x, family=\"binomial\", data=sim_data[train_idx,])\n  preds &lt;- predict(fit, sim_data[test_idx,], type=\"response\")\n  cv_error[k] &lt;- mean((sim_data$y[test_idx] - preds)^2)\n}\nmean(cv_error)\n\n\n\n9. Comparison of Bootstrap and Cross-validation\nDiscuss strengths/weaknesses:\n\nBootstrap gives parameter uncertainty.\nCV provides estimate of prediction error.\n\n\n\n\n10. Extensions and Practical Considerations\nDiscuss regularization and connections to real-world datasets, and further work."
  },
  {
    "objectID": "uncertainty.html#introduction",
    "href": "uncertainty.html#introduction",
    "title": "Some Topics in Logistic Regression",
    "section": "1. Introduction",
    "text": "1. Introduction\nHere I explore some topics in logistic regression manually, including techniques in estimation, uncertainty and validation. I’ll generate a simulated dataset by way of illustration."
  },
  {
    "objectID": "uncertainty.html#simulated-data",
    "href": "uncertainty.html#simulated-data",
    "title": "Some Topics in Logistic Regression",
    "section": "2. Simulated Data",
    "text": "2. Simulated Data\nCreate simple dataset from the logic model with one predictor\n\\[\nP(Y = 1 \\mid X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X)}}\n\\]\nwith parameters\n\n\\(\\beta_0 = -0.5\\)\n\\(\\beta_1 = 2\\).\n\n\nset.seed(112358)\nn &lt;- 100\nbeta_0 &lt;- -0.5\nbeta_1 &lt;- 2\nx &lt;- rnorm(n)\neta &lt;- beta_0 + beta_1 * x\nprob &lt;- 1 / (1 + exp(-eta))\ny &lt;- rbinom(n, 1, prob)\nsim_data &lt;- data.frame(y, x, prob)\nplot(sim_data$x, sim_data$y,\n     xlab = \"Simulated x\", ylab = \"Simulated Y\",\n     main = \"Simulated data for Logistic Regression\"\n)"
  },
  {
    "objectID": "uncertainty.html#logistic-model-and-estimation",
    "href": "uncertainty.html#logistic-model-and-estimation",
    "title": "Some Topics in Logistic Regression",
    "section": "3. Logistic Model and Estimation",
    "text": "3. Logistic Model and Estimation\nGiven a logit model of the form:\n\\[\n\\log\\left(\\frac{\\pi_i}{1 - \\pi_i}\\right) = \\eta_i,\n\\]\nwhere\n\\[\n\\eta_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip}.\n\\]\nWe rearrange for \\(\\pi_i\\):\n\\[\n\\pi_i = \\frac{\\exp(\\eta_i)}{1 + \\exp(\\eta_i)} = \\frac{1}{1 + \\exp(-\\eta_i)}.\n\\]\nParameter estimates for \\(\\beta_j\\) are found firstly by deriving the likelihood function:\n\\[\nL(\\beta_j) = \\prod_{i=1}^{n} \\pi_i^{y_i} (1 - \\pi_i)^{1 - y_i}\n\\]\nwhere ( n ) is the number of observations. In MLE to goal is to maximize the log-likelihood:\n\\[\n\\log L = \\ell(\\beta_j) = \\sum_{i=1}^{n} y_i \\log(\\pi_i) + (1 - y_i) \\log(1 - \\pi_i),\n\\]\n\\[\n= \\sum_{i=1}^{n} \\log(1 - \\pi_i) + \\sum_{i=1}^{n} y_i \\left[ \\log(\\pi_i) - \\log(1 - \\pi_i) \\right],\n\\]\n\\[\n= \\sum_{i=1}^{n} \\log(1 - \\pi_i) + \\sum_{i=1}^{n} y_i \\log\\left( \\frac{\\pi_i}{1 - \\pi_i} \\right).\n\\]\nSince the logistic function is\n\\[\n\\pi_i = \\frac{1}{1 + \\exp(-\\eta_i)},\n\\]\nit follows that\n\\[\n1 - \\pi_i = 1 - \\frac{1}{1 + \\exp(-\\eta_i)} = \\frac{\\exp(-\\eta_i)}{1 + \\exp(-\\eta_i)}.\n\\]\nWe can take the odds:\n\\[\n\\frac{\\pi_i}{1 - \\pi_i} = \\frac{\\frac{1}{1 + \\exp(-\\eta_i)}}{\\frac{\\exp(-\\eta_i)}{1 + \\exp(-\\eta_i)}} = \\frac{1}{\\exp(-\\eta_i)} = \\exp(\\eta_i).\n\\]\nSubstitute this back into the previous equation:\n\\[\n\\mathcal{L} = \\sum_{i=1}^{n} \\left( \\log(1 - \\pi_i) + \\sum_{j=1}^{p} y_i (\\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip}) \\right),\n\\]\n\\[\n= \\sum_{i=1}^{n} \\left[ -\\log(1 + \\exp(\\eta_i)) + \\sum_{j=1}^{p} y_i \\eta_i \\right].\n\\]\nUsually in MLE we differentiate and set to 0:\n\\[\n\\frac{\\partial \\ell}{\\partial \\beta_j} = \\sum_{i=1}^{n} \\frac{1}{1 + \\exp(\\eta_i)} \\cdot \\exp(\\eta_i) x_{ij} + \\sum_{i=1}^{n} y_i x_{ij},\n\\]\n\\[\n= \\sum_{i=1}^{n} \\frac{\\exp(\\eta_i)}{1 + \\exp(\\eta_i)} x_{ij} + \\sum_{i=1}^{n} y_i x_{ij},\n\\]\n\\[\n= \\sum_{i=1}^{n} (y_i - \\pi_i) x_{ij} = 0.\n\\]\nHowever, this has no closed-form solution, so numerical methods are used to estimate \\(beta\\_j\\), e.g. Newton-Raphson. For this, we first need to derive the Score Function and the Hessian."
  },
  {
    "objectID": "uncertainty.html#gradient-and-hessian",
    "href": "uncertainty.html#gradient-and-hessian",
    "title": "Some Topics in Logistic Regression",
    "section": "4. Gradient and Hessian",
    "text": "4. Gradient and Hessian\nTo find the maxima, we firstly need to know how the log likelihood function varies w.r.t its \\(\\beta\\) parameters This is just its first derivative, called the Score Function:\nThe likelihood function for the whole sample is:\n\\[\nL(\\boldsymbol{\\beta}) = \\prod_{i=1}^n \\pi_i^{y_i} (1 - \\pi_i)^{1 - y_i}\n\\]\nTaking logs gives the log-likelihood:\n\\[\n\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\left[ y_i \\log(\\pi_i) + (1 - y_i) \\log(1 - \\pi_i) \\right]\n\\]\n\nDifferentiate it\nTo find the derivative of \\(\\ell(\\boldsymbol{\\beta})\\) with respect to \\(\\beta_1\\), isolate a single observation\n\\[\n\\ell_i = y_i \\log(\\pi_i) + (1 - y_i) \\log(1 - \\pi_i)\n\\]\nand note that the desired derivative is the product of three simpler derivatives via the chain rule:\n\\[\n\\frac{d\\ell_i}{d\\beta_1}\n= \\frac{d\\ell_i}{d\\pi_i} \\cdot \\frac{d\\pi_i}{d\\eta_i} \\cdot \\frac{d\\eta_i}{d\\beta_1}\n\\]\nTheese derivatives evaluate as:\n\nDerivative with respect to \\(\\pi_i\\) follows from the derivative of logs:\n\\[\n\\frac{d\\ell_i}{d\\pi_i} = \\frac{y_i}{\\pi_i} - \\frac{1 - y_i}{1 - \\pi_i}\n\\]\nThis derivative of the logistic function follows when we notice that the direct derivative of \\(\\pi_i\\) is equivalent to the result of multiplying it by \\((1-\\pi_i)\\):\n\\[\n\\pi_i = \\frac{1}{1 + \\exp(-\\eta_i)} \\quad \\Rightarrow \\quad \\frac{d\\pi_i}{d\\eta_i} = \\pi_i (1 - \\pi_i)\n\\]\nThe derivative of the linear predictor is fairly trivial:\n\\[\n\\frac{d\\eta_i}{d\\beta_1} = x_i\n\\]\n\n\n\nMultiplying the three derivatives\nStart by substituting:\n\\[\n\\frac{d\\ell_i}{d\\beta_1}\n= \\left( \\frac{y_i}{\\pi_i} - \\frac{1 - y_i}{1 - \\pi_i} \\right) \\cdot \\pi_i (1 - \\pi_i) \\cdot x_i\n\\]\nExpand and simplify:\n\\[\n\\left( \\frac{y_i}{\\pi_i} - \\frac{1 - y_i}{1 - \\pi_i} \\right) \\cdot \\pi_i (1 - \\pi_i)\n= y_i (1 - \\pi_i) - (1 - y_i) \\pi_i = y_i - \\pi_i\n\\]\nHence:\n\\[\n\\frac{d\\ell_i}{d\\beta_1} = (y_i - \\pi_i) x_i\n\\]\n\n\nThe Score Function\nSumming over all observations gives the score function, often notated \\(U\\):\n\\[\nU(\\beta_1) = \\frac{\\partial \\ell}{\\partial \\beta_1} = \\sum_{i=1}^n (y_i - \\pi_i) x_i\n\\]\nA similar expression holds for the intercept \\(\\beta_0\\), where \\(x_i = 1\\):\n\\[\nU(\\beta_0) = \\sum_{i=1}^n (y_i - \\pi_i)\n\\]"
  },
  {
    "objectID": "uncertainty.html#the-general-case",
    "href": "uncertainty.html#the-general-case",
    "title": "Some Topics in Logistic Regression",
    "section": "The General Case",
    "text": "The General Case\n\n\\(\\eta_i = \\mathbf{x}_i^\\top \\boldsymbol{\\beta}\\)\n\\(\\pi_i = \\frac{1}{1 + \\exp(-\\eta_i)}\\)\n\nThe score function in vector form for arbitrary \\(\\beta_k\\) coefficients is:\n\\[\n\\mathbf{U}(\\boldsymbol{\\beta}) = \\nabla_{\\boldsymbol{\\beta}} \\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^n (y_i - \\pi_i) \\mathbf{x}_i\n\\]\nOr in matrix form:\n\\[\n\\mathbf{U}(\\boldsymbol{\\beta}) = \\mathbf{X}^\\top (\\mathbf{y} - \\boldsymbol{\\pi})\n\\]\nwhere \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times p}\\), and \\(\\boldsymbol{\\pi} \\in \\mathbb{R}^n\\) is the vector of fitted probabilities.\nWe can now represent the score function in code for our simulated dataset:\n\ncompute_score &lt;- function(beta0, beta1, data) {\n  x &lt;- data$x\n  y &lt;- data$y\n  \n  eta &lt;- beta0 + beta1 * x\n  pi &lt;- 1 / (1 + exp(-eta))  # predicted probabilities\n  \n  residual &lt;- y - pi  # (y_i - pi_i)\n  \n  score_0 &lt;- sum(residual)\n  score_1 &lt;- sum(residual * x)\n  \n  c(score_0, score_1)\n}\n\n# Example: compute the score at the true parameters\ncompute_score(beta0 = -0.5, beta1 = 2, data = sim_data)\n\n[1] -1.494431  3.035431\n\n\nHessian:\n\\[\nH(\\beta) = -X^T W X\n\\]\nwhere \\(W\\) is diagonal with elements \\(p_i(1 - p_i)\\).\n\nR code for Gradient and Hessian:\nX &lt;- cbind(1, x)\nbeta &lt;- c(0, 0)\neta &lt;- X %*% beta\np &lt;- 1 / (1 + exp(-eta))\ngradient &lt;- t(X) %*% (y - p)\nW &lt;- diag(as.vector(p * (1 - p)))\nhessian &lt;- -t(X) %*% W %*% X\n\n\n\n5. Newton-Raphson Algorithm\nIteration step:\n\\[\n\\beta_{\\text{new}} = \\beta_{\\text{old}} - [H(\\beta_{\\text{old}})]^{-1} \\nabla \\ell(\\beta_{\\text{old}})\n\\]\n\n\nR implementation:\nbeta &lt;- c(0, 0)\nfor (i in 1:10) {\n  eta &lt;- X %*% beta\n  p &lt;- 1 / (1 + exp(-eta))\n  gradient &lt;- t(X) %*% (y - p)\n  W &lt;- diag(as.vector(p * (1 - p)))\n  hessian &lt;- -t(X) %*% W %*% X\n  beta &lt;- beta - solve(hessian) %*% gradient\n}\nprint(beta)\n\n\n\n6. Variance of Estimates\nEstimated covariance matrix:\n\\[\n\\text{Var}(\\hat{\\beta}) = (X^T W X)^{-1}\n\\]\n\n\n\n7. Bootstrap Validation\nRepeated sampling with replacement to estimate standard errors.\n\n\nBootstrap R code:\nB &lt;- 500\nbootstrap_estimates &lt;- matrix(NA, B, 2)\n\nfor (b in 1:B) {\n  idx &lt;- sample(1:n, replace = TRUE)\n  X_b &lt;- X[idx, ]\n  y_b &lt;- y[idx]\n  beta_b &lt;- glm(y_b ~ X_b[,2], family=\"binomial\")$coefficients\n  bootstrap_estimates[b, ] &lt;- beta_b\n}\n\napply(bootstrap_estimates, 2, sd)\n\n\n\n8. Cross-validation\nPartition data into $K$ folds, train/test each fold.\n\n\nExample 5-fold CV in R:\nK &lt;- 5\nfolds &lt;- sample(rep(1:K, length.out = n))\ncv_error &lt;- numeric(K)\n\nfor (k in 1:K) {\n  train_idx &lt;- which(folds != k)\n  test_idx &lt;- which(folds == k)\n  fit &lt;- glm(y ~ x, family=\"binomial\", data=sim_data[train_idx,])\n  preds &lt;- predict(fit, sim_data[test_idx,], type=\"response\")\n  cv_error[k] &lt;- mean((sim_data$y[test_idx] - preds)^2)\n}\nmean(cv_error)\n\n\n\n9. Comparison of Bootstrap and Cross-validation\nDiscuss strengths/weaknesses:\n\nBootstrap gives parameter uncertainty.\nCV provides estimate of prediction error.\n\n\n\n\n10. Extensions and Practical Considerations\nDiscuss regularization and connections to real-world datasets, and further work."
  }
]