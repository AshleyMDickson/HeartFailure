[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Internal Validation & Sample Size in Clinical Risk Prediction Models",
    "section": "",
    "text": "In this document, I consider the effect of internal validation requirements on sample size requirements. Recent papers by Riley et al. (YYYY) and Pavlou et al. (YYYY) have made recommendations on the sample size required to achieve a particular level of model performance, usually expressed in terms of the C-statistic and Calibration slope. Inputs to these calculations also depend on the marginal prevalence of the target outcome and the number of prediction parameters to be estimated. Supplying these inputs to Pavlou et al.’s sampsizedev package in R, for instance, will yield the recommended sample size, from which further model performance metrics are then derived.\nHowever, the sample size recommendations in the literature so far relate to model development. Less attention has been paid to the sample size requirements given the need to internally validate models before they can be used in clinical practice.\nWhile external validation is the gold standard, and should be completed prior to deployment into clinical care, it is often not possible or practical for well-established reasons (citation needed). Further, internal validation is needed in model specification and variable selection. The simplest case - sample splitting - illustrates the need for the present study. If we have recruited exactly the right number of patients per the recommendation, then in sample-splitting we will invariably have too few patients for model development and training. The hold-out patients needed to validate will be deleterious on the possible model performance that can be achieved.\nThe solution in the simple case of sample splitting is easy: we simply choose our split ratio (say 70/30) and uplift the recommendation accordingly (i.e. multiply by 1.43, or recruit 43% more patients.) But in other methods of internal validation, the problem is not so simple. In these more interesting cases, we do not know (a) what effect, if any, imposing internal validation requirements on the recommended sample size will have, and (b) where there is an effect, what its size would be. We might hypothesise that k-fold cross validation may have a similar effect to sample splitting, because each trained model is on too small a sample - and no amount of repetition will help. But in bootstrap resampling, all the data are used in both development and testing, so there may be no effect at all.\nHere I run some simulations to try to answer the question of whether internal validation has a deleterious effect on model performance at the recommended sample size, and if so, by how much the sample size needs to be uplifted to account for it. I keep this first run simple and use standard (non-shrunk) logit models, keeping the number of predictor parameters fixed at 10, and vary the prevalence and hence events per predictor parameter (EPP)."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Internal Validation & Sample Size in Clinical Risk Prediction Models",
    "section": "",
    "text": "In this document, I consider the effect of internal validation requirements on sample size requirements. Recent papers by Riley et al. (YYYY) and Pavlou et al. (YYYY) have made recommendations on the sample size required to achieve a particular level of model performance, usually expressed in terms of the C-statistic and Calibration slope. Inputs to these calculations also depend on the marginal prevalence of the target outcome and the number of prediction parameters to be estimated. Supplying these inputs to Pavlou et al.’s sampsizedev package in R, for instance, will yield the recommended sample size, from which further model performance metrics are then derived.\nHowever, the sample size recommendations in the literature so far relate to model development. Less attention has been paid to the sample size requirements given the need to internally validate models before they can be used in clinical practice.\nWhile external validation is the gold standard, and should be completed prior to deployment into clinical care, it is often not possible or practical for well-established reasons (citation needed). Further, internal validation is needed in model specification and variable selection. The simplest case - sample splitting - illustrates the need for the present study. If we have recruited exactly the right number of patients per the recommendation, then in sample-splitting we will invariably have too few patients for model development and training. The hold-out patients needed to validate will be deleterious on the possible model performance that can be achieved.\nThe solution in the simple case of sample splitting is easy: we simply choose our split ratio (say 70/30) and uplift the recommendation accordingly (i.e. multiply by 1.43, or recruit 43% more patients.) But in other methods of internal validation, the problem is not so simple. In these more interesting cases, we do not know (a) what effect, if any, imposing internal validation requirements on the recommended sample size will have, and (b) where there is an effect, what its size would be. We might hypothesise that k-fold cross validation may have a similar effect to sample splitting, because each trained model is on too small a sample - and no amount of repetition will help. But in bootstrap resampling, all the data are used in both development and testing, so there may be no effect at all.\nHere I run some simulations to try to answer the question of whether internal validation has a deleterious effect on model performance at the recommended sample size, and if so, by how much the sample size needs to be uplifted to account for it. I keep this first run simple and use standard (non-shrunk) logit models, keeping the number of predictor parameters fixed at 10, and vary the prevalence and hence events per predictor parameter (EPP)."
  },
  {
    "objectID": "index.html#preliminaries",
    "href": "index.html#preliminaries",
    "title": "Internal Validation & Sample Size in Clinical Risk Prediction Models",
    "section": "Preliminaries",
    "text": "Preliminaries\nWe begin by setting up some essential maths and importing the relevant libraries.\n\nlibrary(samplesizedev)\nlibrary(rsample)\nlibrary(rms)\nlibrary(pROC)\nlibrary(dplyr)\nlibrary(data.table)\n\n# Parallel setup\nlibrary(future)\nlibrary(future.apply)\nn_workers &lt;- max(1L, parallel::detectCores() - 1L)\nplan(multisession, workers = n_workers)\nRNGkind(\"L'Ecuyer-CMRG\")\n \nlogit &lt;- function(p) log(p/(1 - p))\nexpit &lt;- function(x) 1/(1 + exp(-x))\n\nThe expit function is the inverse of the logit function, representing the core logistic model which we use:\n\\[\nP(Y = 1 \\mid X) = \\frac{1}{1 + e^{-(\\alpha + \\beta X)}}\n\\]\nWe need a method of creating different data-generating processes (DGPs) for varying prevalence values but with constant beta-coefficients. Let’s start with an example and generate some simulation data with a target prevalence of 1.5%. To achieve this, we need \\(Pr(Y=1) = 0.015\\).\nThis is the marginal probability rather than the conditional on \\(X\\). Another way to express this is that the expectation of the dependent variable, \\(Y\\), should be 0.15:\\(\\mathbb{E}[Y] = 0.015\\).\nWe know that the dependent variable Y takes on values from the conditional distribution:\n\\[\nY|X \\sim Bernoulli (p),\n\\]\nwhere \\(p\\) is the conditional probability of success, \\(p_i=Pr(Y=1 | X_i)\\). From the definition of the logit transformation (and dropping subscript),\n\\[\nlog(\\frac{p}{1-p}) = \\alpha + \\beta X = \\eta\n\\]\nwe find p as the inverse-logit of the linear predictors\n\\[\np = logit^{-1} (\\eta) = \\frac{1}{1+e^{-(\\eta)}},\n\\]\nwhich we know is equal to the conditional expectation \\(\\mathbb{E}[Y|X]\\). Since \\(Pr(Y=1)=\\mathbb{E}[Y]\\), which in turn are equal to the expectation of the conditional probability of success, \\(\\mathbb{E} [Pr(Y=1 | X)]\\), we can substitute the logit model to find:\n\\[\n\\mathbb{E} [ \\frac{1}{1+e^{(-\\eta)}} ] = 0.015,\n\\]\nwhere \\(\\beta\\) is a vector of coefficients and \\(X\\) is a vector of predictor values.\nOnce we have chosen our values for \\(\\beta\\), we need to solve for \\(\\alpha\\) to find the value which guarantees the required level of prevalence.\n\nFinding \\(\\alpha\\)\nIn a sample setting, we do not have an analytic distribution over which to take an expectation, so instead we can take a simple average, as\n\\[\n\\frac{1}{n} \\sum_{i=1}^{n} (\\frac{1}{1+e^{(-\\eta)}}) = 0.015.\n\\]\nFor the sake of simplicity we choose 10 continuous predictors drawn from the standard normal distribution, \\(X_j \\sim \\mathcal{N} (0, 1)\\). We shall come to sample size calculations shortly, but for now let’s suppose we have 3,000 records.\n\nn &lt;- 3000\nprev = 0.015\n\nNext we generate the predictors and choose the beta coefficients including some zeroes.\n\nX &lt;- replicate(10, rnorm(n))        #n*10 matrix\ncolnames(X) &lt;- paste0(\"x\", 1:10)\nbeta &lt;- c(1.1, 0.8, 0.6, 0.4, 0.7, 0.7, 0.7, 0, 0, 0.1)\n\nNow solve; technically, we’re solving the following for \\(\\alpha\\):\n\\[\n\\frac{1}{n} \\sum_{i=1}^{n} (\\frac{1}{1 + e^{\\alpha + \\beta X}}) - 0.015 = 0\n\\]\nIn R, we can use the uniroot function to solve this and and we wrap it into a function for use later on.\n\nfind_alpha &lt;- function(a) mean(expit(a + as.vector(X %*% beta))) - prev\nalpha &lt;- uniroot(find_alpha, c(-10, 10))$root\nprint(alpha)\n\n[1] -5.991107\n\n\nWith \\(\\alpha \\approx -5.8\\) in hand, we can now generate probabilities and outcomes, checking that the prevalence is approximately as expected.\n\npi &lt;- expit(alpha + as.vector(X %*% beta))\ny &lt;- rbinom(n, 1, pi)\nsum(y)/length(y)\n\n[1] 0.017"
  },
  {
    "objectID": "index.html#data-generating-process",
    "href": "index.html#data-generating-process",
    "title": "Internal Validation & Sample Size in Clinical Risk Prediction Models",
    "section": "Data Generating Process",
    "text": "Data Generating Process\nWe’ll simulate binary outcomes under a simple logistic model with p = 10 standard normal predictors. Coefficients \\(\\beta\\) are drawn once per dataset from N(0,1). We choose the intercept so that the marginal prevalence equals a target prevalence using the method from above. This lets us vary \\(\\phi\\) (and thus EPP) directly.\n\nsimulate_logistic_data &lt;- function(n, p = 10, phi = 0.20) {\n  X &lt;- matrix(rnorm(n * p), n, p)\n  colnames(X) &lt;- paste0(\"x\", seq_len(p))\n  beta &lt;- rnorm(p, 0, 1) # standard normal coefficients\n  xb &lt;- X %*% beta\n \n  f &lt;- function(b0) mean(expit(b0 + xb)) - phi\n  b0 &lt;- uniroot(f, c(-12, 12))$root\n \n  eta &lt;- b0 + xb\n  p_true &lt;- expit(eta)\n  y &lt;- rbinom(n, 1, p_true)\n \n  list(\n    df  = cbind(as.data.frame(X), y = y),\n    beta = beta,\n    b0   = b0,\n    phi  = phi\n  )\n}\n\nThis DGP is capable of generating a dataset for model development and testing of any sample size with a fixed number of Predictor Parameters and a variable prevalence, \\(\\phi\\). Next we’ll need some supporting functions that we can use to easily fit the relevant logistic model to each sample and to generate predictions in new data (which will be needed for validation). I’ve also given a simple function to bring together a collection of performance metrics to be used across validation methods (and which can be updated in due course.) It take an outcome \\(y\\), a predicted probability, and a vector of linear predictors as inputs.\n\nfit_logistic &lt;- function(df, formula) glm(formula, data = df, family = binomial())\npredict_prob &lt;- function(mod, newdata) predict(mod, newdata, type = \"response\")\n \n# Perf. metrics\ncalc_metrics &lt;- function(y, p, lp = qlogis(pmin(pmax(p, 1e-6), 1 - 1e-6))) {\n  data.frame(\n    AUC      = as.numeric(pROC::roc(y, p, quiet = TRUE)$auc),\n    CalSlope = coef(glm(y ~ lp, family = binomial()))[2],\n    Brier    = mean((p - y)^2),\n    MAPE     = mean(abs(p - y))\n  )\n}"
  },
  {
    "objectID": "index.html#validation-procedures",
    "href": "index.html#validation-procedures",
    "title": "Internal Validation & Sample Size in Clinical Risk Prediction Models",
    "section": "Validation Procedures",
    "text": "Validation Procedures\nTo understand the optimism of a given model, we first need to understand its apparent performance, which is unadjusted and assesses predictive performance in the training data. To assess this, we need to supply a function with the original (training) dataset and the model formula used in estimation.\n\napparent_perf &lt;- function(df, formula) {\n  mod &lt;- fit_logistic(df, formula)\n  p   &lt;- predict_prob(mod, df)\n  lp  &lt;- predict(mod, type = \"link\")\n  list(metrics = calc_metrics(df$y, p, lp), model = mod)\n}\n\nHaving calculated apparent performance, we can then go on the calulate k-fold Cross Validation performance in which we take \\(k\\) resamples, train on \\(k-1\\) of them and test on the remaining resample, and do this k times (once for each resample).\n\nkfold_cv_mean &lt;- function(df, formula, K = 10) {\n  folds &lt;- rsample::vfold_cv(df, v = K, strata = \"y\")\n  mets &lt;- lapply(folds$splits, function(sp) {\n    train &lt;- rsample::analysis(sp)\n    test  &lt;- rsample::assessment(sp)\n    mod   &lt;- fit_logistic(train, formula)\n    p     &lt;- predict_prob(mod, test)\n    lp    &lt;- predict(mod, newdata = test, type = \"link\")\n    calc_metrics(test$y, p, lp)\n  })\n  out &lt;- Reduce(`+`, mets) / length(mets)\n  as.data.frame(out)\n}\n\nHere I have used a simple mean of the performance metric across the folds. This is not optimal and in the next version will include pooled predictions in Cross Validation if it is deemed a productive line of enquiry.\nNext I set up the function to run the Bootstrap validation, which resamples the data with replacement 200 times (by convention), fits the model on the resample and generates in-bag and out-of-bag predictions to calculate the optimism (as mean(in - out) and corrects it.\n\nbootstrap_optimism &lt;- function(df, formula, B = 200) {\n  app_full &lt;- apparent_perf(df, formula)$metrics\n  n &lt;- nrow(df)\n  opt_mat &lt;- replicate(B, {\n    idx &lt;- sample.int(n, n, replace = TRUE)\n    dfb &lt;- df[idx, ]\n    mod &lt;- fit_logistic(dfb, formula)\n    # in-bag\n    p_in  &lt;- predict_prob(mod, dfb)\n    lp_in &lt;- predict(mod, newdata = dfb, type = \"link\")\n    m_in  &lt;- calc_metrics(dfb$y, p_in, lp_in)\n    # out-of-bag (original dev data)\n    p_out  &lt;- predict_prob(mod, df)\n    lp_out &lt;- predict(mod, newdata = df, type = \"link\")\n    m_out  &lt;- calc_metrics(df$y, p_out, lp_out)\n    unlist(m_in - m_out)\n  })\n  optimism &lt;- rowMeans(opt_mat)\n  corrected &lt;- as.numeric(app_full) - optimism\n  names(corrected) &lt;- names(app_full)\n  as.data.frame(as.list(corrected))\n}\n\nFinally in this section, I provide a simple method of external validation such that we can compare internally-validated performance with ‘true’, gold-standard external. I generate some new logistic data using the same Data Generating Process from the simulate_logistic_data() method.\n\nexternal_perf &lt;- function(fitted_model, p, phi, N_ext = 200000) {\n  sim_ext &lt;- simulate_logistic_data(N_ext, p = p, phi = phi)\n  df_ext  &lt;- sim_ext$df\n  p_hat   &lt;- predict_prob(fitted_model, df_ext)\n  lp_hat  &lt;- predict(fitted_model, newdata = df_ext, type = \"link\")\n  calc_metrics(df_ext$y, p_hat, lp_hat)\n}\n\nBy drawing 200,000 samples as the external dataset we are able to approach the asymptotic domain and draw inferences accordingly when making the comparison with internally-validated model performance."
  },
  {
    "objectID": "index.html#initial-sample-size-recommendation",
    "href": "index.html#initial-sample-size-recommendation",
    "title": "Internal Validation & Sample Size in Clinical Risk Prediction Models",
    "section": "Initial Sample Size Recommendation",
    "text": "Initial Sample Size Recommendation\nIn the simulation scenarios below, I start with the recommended sample size from Pavlou et al.’s samplesizedev package. I assume that the target calibration slope is \\(CS_{target} = 0.9\\) for simplicity; this can be varied in future.\n\npavlou_dev_n &lt;- function(p, phi, c_stat, target_slope = 0.90) {\n  ss &lt;- samplesizedev(outcome = \"Binary\", S = target_slope, phi = phi, c = c_stat, p = p)\n  list(N_sim = ss$sim, N_rvs = ss$rvs)  # use N_sim for primary decisions\n}\n\nAt this sample size, we can invoke Pavlou’s expected_performance method such that we can compare with the external performance results and then understand the damage, if any, to model performance estimates.\n\npavlou_expected &lt;- function(n, p, phi, c_stat) {\n  expected_performance(outcome = \"Binary\", n = n, phi = phi, c = c_stat, p = p)\n}\n\nWe’ll centre our scenarios around Pavlou’s N (targeting S=0.90, by default), then test a few multipliers below/above to see what internal validation needs to be trustworthy."
  },
  {
    "objectID": "index.html#a-single-simulation-run",
    "href": "index.html#a-single-simulation-run",
    "title": "Internal Validation & Sample Size in Clinical Risk Prediction Models",
    "section": "A Single Simulation Run",
    "text": "A Single Simulation Run\nWith this setup in hand, we can generate a single simulation run using the simulated data, validation methods, and performance metric calculations.\n\nrun_rep &lt;- function(n, p = 10, phi = 0.20,\n                    B_boot = 200, K = 10, c_stat_anticipated = 0.80) {\n \n  sim &lt;- simulate_logistic_data(n, p, phi)\n  df  &lt;- sim$df\n  form &lt;- as.formula(paste(\"y ~\", paste(colnames(df)[colnames(df) != \"y\"], collapse = \" + \")))\n \n  # Run validation methods\n  app  &lt;- apparent_perf(df, form)\n  boot &lt;- bootstrap_optimism(df, form, B = B_boot)\n  cv   &lt;- kfold_cv_mean(df, form, K = K)\n  ext  &lt;- external_perf(app$model, p, phi)\n \n  # Extract metrics\n  AUC_app    &lt;- app$metrics$AUC\n  AUC_boot   &lt;- boot$AUC\n  AUC_cv     &lt;- cv$AUC\n  AUC_ext    &lt;- ext$AUC\n \n  Slope_app  &lt;- app$metrics$CalSlope\n  Slope_boot &lt;- boot$CalSlope\n  Slope_cv   &lt;- cv$CalSlope\n  Slope_ext  &lt;- ext$CalSlope\n \n  Brier_app  &lt;- app$metrics$Brier\n  Brier_boot &lt;- boot$Brier\n  Brier_cv   &lt;- cv$Brier\n  Brier_ext  &lt;- ext$Brier\n \n  MAPE_app   &lt;- app$metrics$MAPE\n  MAPE_boot  &lt;- boot$MAPE\n  MAPE_cv    &lt;- cv$MAPE\n  MAPE_ext   &lt;- ext$MAPE\n \n  pav_N      &lt;- pavlou_dev_n(p, phi, c_stat_anticipated, target_slope = 0.90)\n  exp_at_n   &lt;- pavlou_expected(n, p, phi, c_stat_anticipated)\n \n  # Combine rows\n  data.frame(\n    n = n, p = p, phi = phi, EPP = n * phi / p,\n \n    AUC_app = AUC_app,     AUC_boot = AUC_boot,     AUC_cv = AUC_cv,     AUC_ext = AUC_ext,\n    Slope_app = Slope_app, Slope_boot = Slope_boot, Slope_cv = Slope_cv, Slope_ext = Slope_ext,\n    Brier_app = Brier_app, Brier_boot = Brier_boot, Brier_cv = Brier_cv, Brier_ext = Brier_ext,\n    MAPE_app  = MAPE_app,  MAPE_boot  = MAPE_boot,  MAPE_cv  = MAPE_cv,  MAPE_ext  = MAPE_ext,\n \n    dAUC_app   = AUC_app   - AUC_ext,\n    dAUC_boot  = AUC_boot  - AUC_ext,\n    dAUC_cv    = AUC_cv    - AUC_ext,\n \n    dSlope_app = Slope_app - Slope_ext,\n    dSlope_boot= Slope_boot- Slope_ext,\n    dSlope_cv  = Slope_cv  - Slope_ext,\n \n    dBrier_app = Brier_app - Brier_ext,\n    dBrier_boot= Brier_boot- Brier_ext,\n    dBrier_cv  = Brier_cv  - Brier_ext,\n \n    dMAPE_app  = MAPE_app  - MAPE_ext,\n    dMAPE_boot = MAPE_boot - MAPE_ext,\n    dMAPE_cv   = MAPE_cv   - MAPE_ext,\n \n    N_Pavlou_sim = pav_N$N_sim,\n    #N_Pavlou_RvS = pav_N$rvs, \n    ExpSlope_at_n = exp_at_n[\"Mean_calibration_slope\"],\n    ExpMAPE_at_n  = exp_at_n[\"Mean_MAPE\"]\n  )\n}\n\nEach replication (i) computes internal validation performance, (ii) compares it to a large-sample external benchmark (bias), and (iii) records Pavlou’s recommended N and expected calibration/MAPE at the candidate n."
  },
  {
    "objectID": "index.html#scenario-grid",
    "href": "index.html#scenario-grid",
    "title": "Internal Validation & Sample Size in Clinical Risk Prediction Models",
    "section": "Scenario Grid",
    "text": "Scenario Grid\nWe now want to run the scenario over multipliers of the sample size and populate a grid with the prevalence values, recommended sample sizes, and sample sizes that are scaled up according to the multipliers. The resulting dataframe gives all combinations of these.\nWith the simulation_grid function I then run the above single simulation over the set of scenarios and collects all results into a single table. This is run 200 times by default, but this can be scaled up.\n\nbuild_grid_from_pavlou &lt;- function(phi_vals, p = 10, c_stat = 0.80, target_slope = 0.90,\n                                   multipliers = c(1.0, 1.3, 1.6, 2.0)) {\n  rows &lt;- lapply(phi_vals, function(phi) {\n    Ns &lt;- pavlou_dev_n(p = p, phi = phi, c_stat = c_stat, target_slope = target_slope)$N_sim\n    data.frame(n = round(Ns * multipliers), phi = phi)\n  })\n  do.call(rbind, rows)\n}\n \n# simulation over the grid\nsimulate_grid_parallel &lt;- function(grid, n_reps = 200, B_boot = 200, K = 10, \n                                   c_stat_anticipated = 0.80) {\n\n  # make a task table: one row per (scenario, replication)\n  task_df &lt;- do.call(\n    rbind,\n    lapply(seq_len(nrow(grid)), function(g) {\n      data.frame(\n        n   = grid$n[g],\n        phi = grid$phi[g],\n        rep = seq_len(n_reps)\n      )\n    })\n  )\n  # run one replication per task in parallel\n  res_list &lt;- future_lapply(\n    seq_len(nrow(task_df)),\n    function(i) {\n      run_rep(\n        n = task_df$n[i],\n        p = 10,\n        phi = task_df$phi[i],\n        B_boot = B_boot,\n        K = K,\n        c_stat_anticipated = c_stat_anticipated\n      )\n    },\n    future.seed = TRUE  # reproducible across workers\n  )\n  data.table::rbindlist(res_list)\n}\n\nFor each prevalence value, we get the samplesizedev recommendation and evaluate n at (0.7, 1.0, 1.3, 1.6) × N. This whether internal validation good enough at Pavlou’s N and if not, how much larger does n need to be.\n\nsummarise_results &lt;- function(dt, slope_tol = 0.05) {\n  \n  summarize_metric &lt;- function(dt, metric) {\n    methods &lt;- c(\"app\" = \"Apparent\", \"boot\" = \"Bootstrap\", \"cv\" = \"kfoldMean\")\n    \n    data.table::rbindlist(lapply(names(methods), function(m) {\n      col_name &lt;- paste0(\"d\", metric, \"_\", m)\n      dt[, .(bias = mean(get(col_name)),\n             sd   = sd(get(col_name)),\n             rmse = sqrt(mean(get(col_name)^2)),\n             method = methods[m]),\n         by = .(n, phi, EPP)]\n    }))[, metric := metric][]\n  }\n \n  mets &lt;- c(\"AUC\", \"Slope\", \"Brier\", \"MAPE\")\n  perf &lt;- data.table::rbindlist(lapply(mets, function(m) summarize_metric(dt, m)))\n \n  # Sufficiency here means is the median bias within tolerance?\n  suff &lt;- dt[, .(\n    med_Slope_boot = median(Slope_boot, na.rm = TRUE),\n    med_Slope_ext  = median(Slope_ext,  na.rm = TRUE),\n    ok_calib       = abs(median(dSlope_boot, na.rm = TRUE)) &lt;= slope_tol\n  ), by = .(n, phi, EPP)]\n \n  list(perf = perf, sufficiency = suff)\n}\n\nThe sufficiency flag reports whether bootstrap-corrected internal validation is close enough to external (within a tolerance of 0.05 on slope). Now we build a grid of results drawing on Pavlou’s recommended sample sizes and the set of multipliers to ascertain sufficiency.\nset.seed(112358)\n\nphi_vals &lt;- c(0.05, 0.10, 0.20, 0.40)\ngrid &lt;- build_grid_from_pavlou(phi_vals, p = 10, c_stat = 0.80, target_slope = 0.90)\n\n# Parallel run\nsim_dt &lt;- simulate_grid_parallel(\n  grid,\n  n_reps = 30,\n  B_boot = 100,\n  K = 10,\n  c_stat_anticipated = 0.80\n)\n\nsum_out &lt;- summarise_results(sim_dt, slope_tol = 0.05)\nperf_summary &lt;- sum_out$perf\ncalib_ok     &lt;- sum_out$sufficiency\nNeed to increase n_reps and B_boot values when running correctly.\n# Which (phi, n) combos meet the calibration tolerance?\ncalib_ok %&gt;%\n  arrange(phi, n) %&gt;%\n  print(n = 50)\n \n# Compare Pavlou's expected slope at n vs empirical bootstrap & external\nsim_dt %&gt;%\n  select(n, phi, EPP, Slope_boot, Slope_ext, ExpSlope_at_n) %&gt;%\n  group_by(n, phi, EPP) %&gt;%\n  summarise(med_boot = median(Slope_boot),\n            med_ext  = median(Slope_ext),\n            exp_slope = first(ExpSlope_at_n),\n            .groups = \"drop\") %&gt;%\n  arrange(phi, n) %&gt;%\n  print(n = 50)\n \n# Bias summaries (AUC and slope) by method\nperf_summary %&gt;%\n  filter(metric %in% c(\"AUC\", \"Slope\")) %&gt;%\n  arrange(metric, phi, n, method) %&gt;%\n  print(n = 80)"
  },
  {
    "objectID": "index.html#todo",
    "href": "index.html#todo",
    "title": "Internal Validation & Sample Size in Clinical Risk Prediction Models",
    "section": "TODO",
    "text": "TODO\nPlot (i) heatmaps of sufficiency over {prevalence × multiplier}, and (ii) bias vs. EPP line charts by method.\nNotes\nCore question: At Pavlou’s recommended N (for target S=0.90), are internal validation estimates (especially bootstrap) sufficiently close to external?\nIf not, how much larger N is needed? The multipliers point toward answer.\nPrevalence makes a difference by fixing p = 10 and varying ##, you naturally vary EPP, which is often considered in the literature.\nHere I’ve used mean-of-folds in CV; a development will involve pooled-predictions as a robustness check."
  },
  {
    "objectID": "index.html#plots",
    "href": "index.html#plots",
    "title": "Internal Validation & Sample Size in Clinical Risk Prediction Models",
    "section": "Plots",
    "text": "Plots"
  }
]