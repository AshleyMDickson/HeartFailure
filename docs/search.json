[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Explorations in Binary Data",
    "section": "",
    "text": "Introduction\n\nHere I focus on model validation to answer the question of whether internal validation can be a decent proxy for external validation.\n\n\nlibrary(rms)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(pROC)\nlibrary(tidyr)\n\nIn this version, I define a set of basic functions that generate data, get the calibration slope from a model, run a single simulation and then a wider set of simulations, and then create the internal-external boxplot.\n\nset.seed(112358)\n\nInitial function to generate simulation data; to be used twice, for internal and external validation.\n\ngen_data &lt;- function(n, n_preds = 10, intercept = -1.5, coef_value = 0.7) {\n  X &lt;- matrix(rnorm(n * n_preds), nrow = n, ncol = n_preds)\n  colnames(X) &lt;- paste0(\"x\", 1:n_preds)\n  eta &lt;- intercept + X %*% rep(coef_value, n_preds)\n  prob &lt;- 1/(1 + exp(-eta))\n  y &lt;- rbinom(n, 1, prob)\n  return(data.frame(y = y, prob = prob, X))\n}\n\nFunction to return key validation metric: calibration slope\n\nget_cal_slope &lt;- function(model, newdata) {\n  lp &lt;- predict(model, newdata = newdata) #lp = linear predictor from rms\n  cal_mod &lt;- glm(y ~ lp, data = newdata, family = binomial)\n  return(cal_mod$coef[2])\n}\n\nFunction to create core simulation for a single simulation (couldn’t get parallel processing working)\n\nsingle_simulation &lt;- function(sim_id, n_train, n_ext, n_boot) {\n  train_data &lt;- gen_data(n_train, n_preds = 10, intercept = -1.5, coef_value = 0.7)\n  ext_data &lt;- gen_data(n_ext, n_preds = 10, intercept = -1.5, coef_value = 0.7)\n  \n  dd &lt;- datadist(train_data)\n  options(datadist = dd)\n  on.exit(options(datadist = NULL))\n  \n  model_fn &lt;- paste(\"y ~\", paste(paste0(\"x\", 1:10), collapse = \" + \"))\n  model &lt;- suppressMessages(suppressWarnings(\n    lrm(as.formula(model_fn), data = train_data, x = TRUE, y = TRUE)\n  ))\n  \n  ext_cal_slope &lt;- get_cal_slope(model, ext_data)\n  boot_val &lt;- validate(model, method = \"boot\", B = n_boot)\n  boot_cal_slope &lt;- boot_val[\"Slope\", \"index.corrected\"]\n  \n  return(data.frame(sim = sim_id, n_train = n_train, \n             ext_cal_slope = ext_cal_slope,\n             boot_cal_slope = boot_cal_slope))\n}\n\nMain function to call simulations setting errors to NA then filtering out.\n\nrun_simulation &lt;- function(n_train, n_ext, n_sims = 50, n_boot = 200) {\n  results &lt;- vector(\"list\", n_sims)\n  start_time &lt;- Sys.time()\n  \n  for (i in 1:n_sims) {\n    results[[i]] &lt;- tryCatch({\n      single_simulation(i, n_train, n_ext, n_boot)\n    }, error = function(e) {\n      data.frame(sim = i, n_train = n_train, ext_cal_slope = NA, boot_cal_slope = NA)\n    })\n  }\n  \n  all_results &lt;- do.call(rbind, results)\n  all_results &lt;- all_results[!is.na(all_results$ext_cal_slope), ]\n\n  cat(\"Sample size n =\", n_train, \"completed in\", \n      round(as.numeric(Sys.time() - start_time, units = \"secs\"), 1), \"seconds\\n\\n\")\n  \n  return(all_results)\n}\n\nNow let’s run the simulation:\n\ntotal_start_time &lt;- Sys.time()\nsample_sizes &lt;- c(50, 100, 200, 300, 500, 750, 1000, 2000)\n\nall_results &lt;- lapply(sample_sizes, function(n) run_simulation(n, n))\n\noptimization did not converge for lrm.fit\nsingular Hessian matrix\nconvergence code 2\noptimization did not converge for lrm.fit\nsingular Hessian matrix\nconvergence code 2\noptimization did not converge for lrm.fit\nsingular Hessian matrix\nconvergence code 2\noptimization did not converge for lrm.fit\nsingular Hessian matrix\nconvergence code 2\noptimization did not converge for lrm.fit\nsingular Hessian matrix\nconvergence code 2\noptimization did not converge for lrm.fit\nsingular Hessian matrix\nconvergence code 2\noptimization did not converge for lrm.fit\nsingular Hessian matrix\nconvergence code 2\noptimization did not converge for lrm.fit\nsingular Hessian matrix\nconvergence code 2\noptimization did not converge for lrm.fit\nsingular Hessian matrix\nconvergence code 2\n\n\nSample size n = 50 completed in 18.5 seconds\n\n\noptimization did not converge for lrm.fit\nsingular Hessian matrix\nconvergence code 2\noptimization did not converge for lrm.fit\nsingular Hessian matrix\nconvergence code 2\noptimization did not converge for lrm.fit\nsingular Hessian matrix\nconvergence code 2\n\n\nSample size n = 100 completed in 16.7 seconds\n\nSample size n = 200 completed in 18.8 seconds\n\nSample size n = 300 completed in 21.3 seconds\n\nSample size n = 500 completed in 27.1 seconds\n\nSample size n = 750 completed in 33.5 seconds\n\nSample size n = 1000 completed in 40.2 seconds\n\nSample size n = 2000 completed in 66.8 seconds\n\nnames(all_results) &lt;- paste0(\"n_\", sample_sizes)\ncombined_results &lt;- do.call(rbind, all_results)\n\nA function to create the boxplot: sample sizes on x-axis and CS on y-axis; 2 series.\n\ngen_boxplot &lt;- function(all_results) {\n  plot_data &lt;- all_results %&gt;%\n    select(sim, n_train, ext_cal_slope, boot_cal_slope) %&gt;%\n    pivot_longer(cols = c(ext_cal_slope, boot_cal_slope),\n                 names_to = \"validation_type\", values_to = \"cal_slope\") %&gt;%\n    mutate(validation_type = case_when(\n      validation_type == \"ext_cal_slope\" ~ \"External Validation\",\n      validation_type == \"boot_cal_slope\" ~ \"Bootstrap Internal Validation\"),\n      n_train = factor(n_train)) %&gt;%\n    filter(!is.na(cal_slope))\n  \n  ggplot(plot_data, aes(x = n_train, y = cal_slope, fill = validation_type)) +\n    geom_boxplot(alpha = 0.7, position = position_dodge(0.8)) +\n    scale_fill_manual(values = c(\"External Validation\" = \"lightblue\", \n                                \"Bootstrap Internal Validation\" = \"lightcoral\")) +\n    labs(title = \"Calibration Slope by Sample Size\",\n         x = \"Sample Size\", y = \"Calibration Slope\", fill = \"Validation Type\") +\n    scale_y_continuous(limits = c(0, 1.5)) +\n    theme_minimal() +\n    theme(plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n          plot.subtitle = element_text(hjust = 0.5, size = 11),\n          legend.position = \"bottom\")\n}\n\n\nmain_plot &lt;- gen_boxplot(combined_results)\nprint(main_plot)\n\nWarning: Removed 4 rows containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\n\n\n\ntotal_runtime &lt;- round(as.numeric(Sys.time() - total_start_time, units = \"mins\"), 1)\ncat(\"Total runtime:\", total_runtime, \"minutes\")\n\nTotal runtime: 4.1 minutes"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This site has been set up as a temporary home for this analysis such that it can be shared with my supervisors.\nHow to do math in LaTeX:\nInline: \\(\\alpha\\)\nIndented:\n\\[\n\\hat{y}_i = \\beta_0 + \\sum_{j=1}^p \\beta_j x_{ji}\n\\]\n\nx = seq(0, 2*pi, by = 0.1)\ny = sin(x) + rnorm(length(x), 0, 0.3)\nplot(x,y) +\n  title(\"Sinusoidal Simulation\")\n\n\n\n\n\n\n\n\ninteger(0)"
  },
  {
    "objectID": "index.html#environment",
    "href": "index.html#environment",
    "title": "Heart Failure Analysis",
    "section": "Environment",
    "text": "Environment\nLoad the requisite libraries.\n\nlibrary(rms)\nlibrary(dplyr)\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(gtsummary)\nlibrary(ggplot2)\nlibrary(splines)\nlibrary(broom)\nlibrary(pROC)\nlibrary(knitr)\nlibrary(mgcv)\nlibrary(patchwork)"
  },
  {
    "objectID": "index.html#data",
    "href": "index.html#data",
    "title": "Heart Failure Analysis",
    "section": "Data",
    "text": "Data\nHaving loaded the various libraries needed, let’s import our heart failure data.\n\nsetwd(\"C:/Users/rmhimdi/OneDrive - University College London/Documents/HeartFailure\")\ndf &lt;- read_excel(\"simulateddata_LR_NL.xlsx\")\n\nDiscretise age and recode sex.\n\ndf$age &lt;- round(df$age, 0)\nn = 5\ndf &lt;- df %&gt;%\n  mutate(Age_ = paste0(\n    n*floor(age/n), \"-\",n*(floor(age/n)+1)\n    )\n  )\n#df$sex &lt;- factor(df$sex, levels = c(0, 1), labels = c(\"Male\", \"Female\")) \n## Assuming Female = 1 since this seems mildly protective (cf. MAGGIC), but will need to check.\n\nRenaming varibles.\n\ndf &lt;- \n  df %&gt;% \n  rename(\n    Outcome = outcome,\n    Age = age,\n    Sex = sex,\n    Creatinine = creatineDischarge,\n    Sodium = sodiumDischarge,\n    Potassium = potassiumDischarge,\n    Urea = ureaDischarge,\n    S_bp =  sbpAdmission,\n    HR = hrAdmission,\n    Hb = hbDischarge,\n    Diabetes = diabetes,\n    COPD = copd,\n    IHD = ihd, \n    ValveDisease = valveDisease,\n    NYHA_class = nyha,\n    PeripheralOedema = peripheralOedema,\n    AF = af,\n    eGFR = egfr_full\n  )\n\nInspect the processed data.\n\nkable(head(df))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOutcome\nAge\nCreatinine\nSodium\nPotassium\nUrea\nS_bp\nHR\nHb\nSex\nDiabetes\nCOPD\nIHD\nValveDisease\nNYHA_class\nPeripheralOedema\nAF\neGFR\nAge_\n\n\n\n\n0\n62\n76.99999\n138\n4.2\n19.6100\n113\n119\n11.1\n0\n1\n0\n1\n0\n1\n0\n1\n88.70592\n60-65\n\n\n1\n90\n142.00003\n146\n4.4\n20.9000\n98\n78\n12.1\n0\n0\n0\n1\n1\n0\n0\n0\n40.60656\n90-95\n\n\n1\n84\n143.00003\n141\n4.0\n15.7056\n132\n102\n13.7\n0\n0\n1\n0\n0\n1\n1\n0\n40.90517\n80-85\n\n\n0\n75\n80.00000\n137\n4.0\n7.5000\n152\n100\n11.6\n1\n0\n0\n0\n0\n1\n1\n0\n60.71780\n75-80\n\n\n0\n84\n123.99999\n139\n4.2\n8.2000\n201\n46\n11.8\n1\n0\n0\n1\n0\n0\n0\n0\n35.78796\n80-85\n\n\n0\n78\n118.00001\n140\n4.7\n12.7000\n104\n64\n10.6\n1\n0\n0\n0\n0\n1\n1\n1\n38.37403\n75-80\n\n\n\n\n\n\nHold Out\nDuring model evaluation, we will need to conduct predictive validation using the test set having estimated the model coefficients on the training set.\n\nset.seed(1729)\nrecords &lt;- dim(df)[1]\n\ntrain_size &lt;- floor(0.8*records)\ntrain_index &lt;- sample(seq_len(records), size = train_size)\ntrain &lt;- df[train_index,]\ntest &lt;- df[-train_index,]\nprint(paste(\"Size of training set is: \", nrow(train)))\n\n[1] \"Size of training set is:  43264\"\n\nprint(paste(\"Size of testing set is: \", nrow(test)))\n\n[1] \"Size of testing set is:  10817\""
  },
  {
    "objectID": "index.html#full-model",
    "href": "index.html#full-model",
    "title": "Heart Failure Analysis",
    "section": "Full Model",
    "text": "Full Model\nRun the naive model with all variables to see outline effect sizes.\n\n#Sans age categorisation\nfull_model &lt;- glm(Outcome ~ Age + Sex  + Creatinine + Sodium + Potassium + Urea + S_bp + HR + Hb+ Diabetes + COPD + IHD + ValveDisease + NYHA_class + PeripheralOedema + AF + eGFR, family = binomial, data = df)\nprint(summary(full_model))\n\n\nCall:\nglm(formula = Outcome ~ Age + Sex + Creatinine + Sodium + Potassium + \n    Urea + S_bp + HR + Hb + Diabetes + COPD + IHD + ValveDisease + \n    NYHA_class + PeripheralOedema + AF + eGFR, family = binomial, \n    data = df)\n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)      -7.1398644  0.4689428 -15.225  &lt; 2e-16 ***\nAge               0.0473564  0.0016273  29.102  &lt; 2e-16 ***\nSex               0.1904866  0.0316661   6.015 1.79e-09 ***\nCreatinine        0.0036828  0.0002298  16.024  &lt; 2e-16 ***\nSodium            0.0077452  0.0028043   2.762 0.005746 ** \nPotassium         0.3237376  0.0238317  13.584  &lt; 2e-16 ***\nUrea              0.0375205  0.0013179  28.469  &lt; 2e-16 ***\nS_bp             -0.0203323  0.0006165 -32.982  &lt; 2e-16 ***\nHR                0.0046018  0.0006626   6.945 3.78e-12 ***\nHb               -0.0256031  0.0075885  -3.374 0.000741 ***\nDiabetes         -0.1079845  0.0326271  -3.310 0.000934 ***\nCOPD              0.2200697  0.0363999   6.046 1.49e-09 ***\nIHD               0.0765564  0.0302083   2.534 0.011268 *  \nValveDisease      0.0518244  0.0321303   1.613 0.106757    \nNYHA_class       -0.0267937  0.0367931  -0.728 0.466475    \nPeripheralOedema  0.0914774  0.0304967   3.000 0.002704 ** \nAF               -0.0345800  0.0303462  -1.140 0.254488    \neGFR              0.0016651  0.0009438   1.764 0.077691 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 38473  on 54080  degrees of freedom\nResidual deviance: 32723  on 54063  degrees of freedom\nAIC: 32759\n\nNumber of Fisher Scoring iterations: 6\n\nprint(\"Odd Ratios\")\n\n[1] \"Odd Ratios\"\n\nkable(exp(coef(full_model)))\n\n\n\n\n\nx\n\n\n\n\n(Intercept)\n0.0007929\n\n\nAge\n1.0484956\n\n\nSex\n1.2098382\n\n\nCreatinine\n1.0036896\n\n\nSodium\n1.0077752\n\n\nPotassium\n1.3822845\n\n\nUrea\n1.0382333\n\n\nS_bp\n0.9798730\n\n\nHR\n1.0046124\n\n\nHb\n0.9747219\n\n\nDiabetes\n0.8976415\n\n\nCOPD\n1.2461636\n\n\nIHD\n1.0795631\n\n\nValveDisease\n1.0531908\n\n\nNYHA_class\n0.9735621\n\n\nPeripheralOedema\n1.0957920\n\n\nAF\n0.9660111\n\n\neGFR\n1.0016665\n\n\n\n\n\nWhile these Odds Ratios give us an outline picture of the effects, it is unlikely that all relationships are linear in the ‘true’ model. So, let’s try a few ways of relaxing the linearity assumption."
  },
  {
    "objectID": "index.html#age",
    "href": "index.html#age",
    "title": "Heart Failure Analysis",
    "section": "Age",
    "text": "Age\nLet’s try to identify any nonlinear age effect. We can do this in a few different ways.\n\nCategorisation\nLet’s start by using the age quintiles. We run two models to compare:\n\nmod1 &lt;- train %&gt;% \n  glm(formula = Outcome ~ Age, family = binomial)\nsummary(mod1)\n\n\nCall:\nglm(formula = Outcome ~ Age, family = binomial, data = .)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -5.647031   0.130533  -43.26   &lt;2e-16 ***\nAge          0.044565   0.001569   28.40   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 30626  on 43263  degrees of freedom\nResidual deviance: 29667  on 43262  degrees of freedom\nAIC: 29671\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nmod1 &lt;- train %&gt;% \n  glm(formula = Outcome ~ Age_, family = binomial)\nsummary(mod1)\n\n\nCall:\nglm(formula = Outcome ~ Age_, family = binomial, data = .)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -0.7867     0.1689  -4.657 3.21e-06 ***\nAge_105-110  -1.5159     1.0623  -1.427 0.153587    \nAge_15-20   -12.7794   267.7056  -0.048 0.961926    \nAge_20-25   -12.7794   101.1833  -0.126 0.899495    \nAge_25-30    -2.5092     0.7396  -3.392 0.000693 ***\nAge_30-35    -3.2993     0.7328  -4.503 6.71e-06 ***\nAge_35-40    -3.3297     0.6061  -5.494 3.93e-08 ***\nAge_40-45    -3.0072     0.4179  -7.197 6.17e-13 ***\nAge_45-50    -2.6675     0.2878  -9.268  &lt; 2e-16 ***\nAge_50-55    -2.6546     0.2487 -10.676  &lt; 2e-16 ***\nAge_55-60    -2.4627     0.2194 -11.225  &lt; 2e-16 ***\nAge_60-65    -1.9998     0.1932 -10.353  &lt; 2e-16 ***\nAge_65-70    -1.7313     0.1833  -9.444  &lt; 2e-16 ***\nAge_70-75    -1.7104     0.1776  -9.632  &lt; 2e-16 ***\nAge_75-80    -1.3749     0.1741  -7.898 2.84e-15 ***\nAge_80-85    -1.2114     0.1722  -7.034 2.01e-12 ***\nAge_85-90    -1.0309     0.1719  -5.997 2.01e-09 ***\nAge_90-95    -0.7899     0.1728  -4.572 4.84e-06 ***\nAge_95-100   -0.4525     0.1795  -2.520 0.011723 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 30626  on 43263  degrees of freedom\nResidual deviance: 29658  on 43245  degrees of freedom\nAIC: 29696\n\nNumber of Fisher Scoring iterations: 12\n\n\n\n\nSplines\n\nmod3 &lt;- glm(Outcome ~ ns(Age, df=3) + Sex  + ns(Creatinine, df=3) + Sodium + ns(Potassium, df=3) + ns(Urea, df=3) + S_bp + HR + Hb+ Diabetes + COPD + IHD + ValveDisease + NYHA_class + PeripheralOedema + AF + eGFR, family = binomial, data = train)\nprint(summary(mod3))\n\n\nCall:\nglm(formula = Outcome ~ ns(Age, df = 3) + Sex + ns(Creatinine, \n    df = 3) + Sodium + ns(Potassium, df = 3) + ns(Urea, df = 3) + \n    S_bp + HR + Hb + Diabetes + COPD + IHD + ValveDisease + NYHA_class + \n    PeripheralOedema + AF + eGFR, family = binomial, data = train)\n\nCoefficients:\n                          Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)             -3.2297212  0.8855541  -3.647 0.000265 ***\nns(Age, df = 3)1         2.2499750  0.2130202  10.562  &lt; 2e-16 ***\nns(Age, df = 3)2         5.3885694  0.8362700   6.444 1.17e-10 ***\nns(Age, df = 3)3         3.6383468  0.2490317  14.610  &lt; 2e-16 ***\nSex                      0.3452108  0.0552986   6.243 4.30e-10 ***\nns(Creatinine, df = 3)1  4.0797090  0.5571073   7.323 2.42e-13 ***\nns(Creatinine, df = 3)2  1.7759930  0.8101710   2.192 0.028371 *  \nns(Creatinine, df = 3)3 -1.2744470  0.7870926  -1.619 0.105408    \nSodium                   0.0051292  0.0031289   1.639 0.101149    \nns(Potassium, df = 3)1  -1.4475200  0.1351737 -10.709  &lt; 2e-16 ***\nns(Potassium, df = 3)2  -4.4924440  0.6021319  -7.461 8.59e-14 ***\nns(Potassium, df = 3)3   2.0966514  0.3195196   6.562 5.31e-11 ***\nns(Urea, df = 3)1        3.5024779  0.1396471  25.081  &lt; 2e-16 ***\nns(Urea, df = 3)2        3.5433371  0.2873837  12.330  &lt; 2e-16 ***\nns(Urea, df = 3)3        1.0218810  0.2459434   4.155 3.25e-05 ***\nS_bp                    -0.0189166  0.0006991 -27.060  &lt; 2e-16 ***\nHR                       0.0051112  0.0007596   6.729 1.71e-11 ***\nHb                      -0.0137585  0.0086356  -1.593 0.111108    \nDiabetes                -0.1036697  0.0375945  -2.758 0.005823 ** \nCOPD                     0.2105146  0.0419030   5.024 5.06e-07 ***\nIHD                      0.1033113  0.0345790   2.988 0.002811 ** \nValveDisease             0.0688294  0.0367263   1.874 0.060914 .  \nNYHA_class              -0.0464105  0.0421425  -1.101 0.270777    \nPeripheralOedema         0.0174441  0.0348879   0.500 0.617072    \nAF                      -0.0416977  0.0347090  -1.201 0.229615    \neGFR                     0.0156079  0.0031332   4.981 6.31e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 30626  on 43263  degrees of freedom\nResidual deviance: 25036  on 43238  degrees of freedom\nAIC: 25088\n\nNumber of Fisher Scoring iterations: 6\n\n\nHaving tried a few non-linear model options, it seems that we need a more systematic way to specify the functional form.\nFor this, we can visualise the empirical distribution of the dependent variable by the candidate covariates and inspect the shape.\n\ngam_model &lt;- gam(Outcome ~ s(Age) \n                   #+ s(Sex) + s(Creatinine) + s(Sodium) + s(Potassium) + s(Urea) + s(S_bp) + s(HR) + s(Hb) +\n                   #s(Diabetes) + s(COPD) + s(IHD) + s(ValveDisease) + s(NYHA_class) + s(PeripheralOedema) + s(AF) + s(eGFR)\n                   #, family = binomial\n                 , data = df)\nplot(gam_model)\n\n\n\n\n\n\n\n\n\nage_summary &lt;- df %&gt;%\n  group_by(Age) %&gt;%\n  summarise(\n    n = n(),\n    deaths = sum(Outcome),\n    mortality_rate = deaths / n,\n    log_odds = log((mortality_rate) / (1 - mortality_rate))  # small offset to avoid log(0)\n  )\n\nggplot(age_summary, aes(x = Age, y = log_odds)) +\n  geom_point(alpha = 0.6) +\n  geom_abline() +\n  geom_smooth(se = FALSE, method = \"loess\", span = 0.3) +\n  labs(\n    x = \"Age\",\n    y = \"Empirical log-odds of mortality\",\n    title = \"Empirical log-odds of mortality by Age\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 19 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\n\n\n\n\n\n\n\n\nplot_all_empirical_mortality_and_logodds &lt;- function(data, outcome, min_n = 10, smooth_span = 0.3, n_bins = 10, stratify_by = \"Age\") {\n  \n  outcome_sym &lt;- ensym(outcome)\n  outcome_chr &lt;- rlang::as_string(outcome_sym)\n  strat_chr &lt;- stratify_by\n  strat_sym &lt;- sym(strat_chr)\n  \n  predictors &lt;- setdiff(names(data), c(outcome_chr, strat_chr))\n  plots &lt;- list()\n  \n  for (pred in predictors) {\n    predictor_sym &lt;- ensym(pred)\n    pred_values &lt;- data %&gt;% pull(!!predictor_sym)\n    \n    # Skip if constant or fully missing\n    if (n_distinct(pred_values, na.rm = TRUE) &lt;= 1) next\n    \n    is_binary &lt;- all(pred_values %in% c(0, 1), na.rm = TRUE)\n    \n    if (is_binary) {\n      # Binary predictors now plotted against Age\n      df_binned &lt;- data %&gt;%\n        filter(!is.na(!!predictor_sym), !is.na(!!strat_sym), !is.na(!!outcome_sym)) %&gt;%\n        mutate(age_bin = cut_number(!!strat_sym, n = n_bins)) %&gt;%\n        group_by(!!predictor_sym, age_bin) %&gt;%\n        summarise(\n          mean_age = mean(!!strat_sym, na.rm = TRUE),\n          n = n(),\n          deaths = sum(!!outcome_sym),\n          mortality_rate = deaths / n,\n          log_odds = log((mortality_rate + 1e-3) / (1 - mortality_rate + 1e-3)),\n          .groups = \"drop\"\n        ) %&gt;%\n        filter(n &gt;= min_n)\n      \n      p1 &lt;- ggplot(df_binned, aes(x = mean_age, y = mortality_rate, color = factor(!!predictor_sym))) +\n        geom_point(alpha = 0.7) +\n        geom_line() +\n        labs(x = strat_chr, y = \"Mortality rate\", color = pred,\n             title = paste(\"Mortality by\", strat_chr, \"stratified by\", pred)) +\n        theme_minimal()\n      \n      p2 &lt;- ggplot(df_binned, aes(x = mean_age, y = log_odds, color = factor(!!predictor_sym))) +\n        geom_point(alpha = 0.7) +\n        geom_line() +\n        labs(x = strat_chr, y = \"Log-odds of mortality\", color = pred,\n             title = paste(\"Log-odds by\", strat_chr, \"stratified by\", pred)) +\n        theme_minimal()\n      \n    } else {\n      # Regular continuous predictors\n      unique_vals &lt;- n_distinct(pred_values, na.rm = TRUE)\n      \n      if (unique_vals &gt; 5) {\n        data_binned &lt;- data %&gt;%\n          mutate(bin = cut_number(!!predictor_sym, n = n_bins))\n        \n        df_summary &lt;- data_binned %&gt;%\n          group_by(bin) %&gt;%\n          summarise(\n            mean_value = mean(!!predictor_sym, na.rm = TRUE),\n            n = n(),\n            deaths = sum(!!outcome_sym),\n            mortality_rate = deaths / n,\n            log_odds = log((mortality_rate + 1e-3) / (1 - mortality_rate + 1e-3)),\n            .groups = \"drop\"\n          ) %&gt;%\n          filter(n &gt;= min_n)\n        \n        p1 &lt;- ggplot(df_summary, aes(x = mean_value, y = mortality_rate)) +\n          geom_point(alpha = 0.6) +\n          geom_line() +\n          labs(x = pred, y = \"Mortality rate\", title = paste(\"Binned mortality by\", pred)) +\n          theme_minimal()\n        \n        p2 &lt;- ggplot(df_summary, aes(x = mean_value, y = log_odds)) +\n          geom_point(alpha = 0.6) +\n          geom_line() +\n          labs(x = pred, y = \"Log-odds of mortality\", title = paste(\"Binned log-odds by\", pred)) +\n          theme_minimal()\n        \n      } else {\n        df_summary &lt;- data %&gt;%\n          group_by(value = !!predictor_sym) %&gt;%\n          summarise(\n            n = n(),\n            deaths = sum(!!outcome_sym),\n            mortality_rate = deaths / n,\n            log_odds = log((mortality_rate + 1e-3) / (1 - mortality_rate + 1e-3)),\n            .groups = \"drop\"\n          ) %&gt;%\n          filter(n &gt;= min_n)\n        \n        p1 &lt;- ggplot(df_summary, aes(x = value, y = mortality_rate)) +\n          geom_point(alpha = 0.6) +\n          geom_line() +\n          labs(x = pred, y = \"Mortality rate\", title = paste(\"Mortality by\", pred)) +\n          theme_minimal()\n        \n        p2 &lt;- ggplot(df_summary, aes(x = value, y = log_odds)) +\n          geom_point(alpha = 0.6) +\n          geom_line() +\n          labs(x = pred, y = \"Log-odds of mortality\", title = paste(\"Log-odds by\", pred)) +\n          theme_minimal()\n      }\n    }\n    \n    plots[[pred]] &lt;- p1 + p2\n  }\n  \n  return(plots)\n}\n\n\ndf = subset(df, select = -c(Age_))\nplot_all_empirical_mortality_and_logodds(data = df, outcome = Outcome)\n\n$Creatinine\n\n\n\n\n\n\n\n\n\n\n$Sodium\n\n\n\n\n\n\n\n\n\n\n$Potassium\n\n\n\n\n\n\n\n\n\n\n$Urea\n\n\n\n\n\n\n\n\n\n\n$S_bp\n\n\n\n\n\n\n\n\n\n\n$HR\n\n\n\n\n\n\n\n\n\n\n$Hb\n\n\n\n\n\n\n\n\n\n\n$Sex\n\n\n\n\n\n\n\n\n\n\n$Diabetes\n\n\n\n\n\n\n\n\n\n\n$COPD\n\n\n\n\n\n\n\n\n\n\n$IHD\n\n\n\n\n\n\n\n\n\n\n$ValveDisease\n\n\n\n\n\n\n\n\n\n\n$NYHA_class\n\n\n\n\n\n\n\n\n\n\n$PeripheralOedema\n\n\n\n\n\n\n\n\n\n\n$AF\n\n\n\n\n\n\n\n\n\n\n$eGFR\n\n\n\n\n\n\n\n\n\n\n\nHierarchical\nThis is a Quarto website. To learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "Validation.html",
    "href": "Validation.html",
    "title": "Validation",
    "section": "",
    "text": "set.seed(42)\nhf_data &lt;- read_excel(\"simulateddata_LR_NL.xlsx\") %&gt;% \n  rename(\n    Outcome = outcome,\n    Age = age,\n    Sex = sex,\n    Creatinine = creatineDischarge,\n    Sodium = sodiumDischarge,\n    Potassium = potassiumDischarge,\n    Urea = ureaDischarge,\n    S_bp =  sbpAdmission,\n    HR = hrAdmission,\n    Hb = hbDischarge,\n    Diabetes = diabetes,\n    COPD = copd,\n    IHD = ihd, \n    ValveDisease = valveDisease,\n    NYHA_class = nyha,\n    PeripheralOedema = peripheralOedema,\n    AF = af,\n    eGFR = egfr_full\n  )\n\n\ntrain_indices &lt;- sample(seq_len(nrow(hf_data)), size = 0.7 * nrow(hf_data))\nhf_train &lt;- hf_data[train_indices, ]\nhf_test  &lt;- hf_data[-train_indices, ]"
  },
  {
    "objectID": "Validation.html#data-preparation-and-splitting",
    "href": "Validation.html#data-preparation-and-splitting",
    "title": "Validation",
    "section": "",
    "text": "set.seed(42)\nhf_data &lt;- read_excel(\"simulateddata_LR_NL.xlsx\") %&gt;% \n  rename(\n    Outcome = outcome,\n    Age = age,\n    Sex = sex,\n    Creatinine = creatineDischarge,\n    Sodium = sodiumDischarge,\n    Potassium = potassiumDischarge,\n    Urea = ureaDischarge,\n    S_bp =  sbpAdmission,\n    HR = hrAdmission,\n    Hb = hbDischarge,\n    Diabetes = diabetes,\n    COPD = copd,\n    IHD = ihd, \n    ValveDisease = valveDisease,\n    NYHA_class = nyha,\n    PeripheralOedema = peripheralOedema,\n    AF = af,\n    eGFR = egfr_full\n  )\n\n\ntrain_indices &lt;- sample(seq_len(nrow(hf_data)), size = 0.7 * nrow(hf_data))\nhf_train &lt;- hf_data[train_indices, ]\nhf_test  &lt;- hf_data[-train_indices, ]"
  },
  {
    "objectID": "Validation.html#model-fitting-training-set-with-non-linear-age",
    "href": "Validation.html#model-fitting-training-set-with-non-linear-age",
    "title": "Validation",
    "section": "Model Fitting (Training Set) with Non-linear Age",
    "text": "Model Fitting (Training Set) with Non-linear Age\n\ndd &lt;- datadist(hf_train)\noptions(datadist = \"dd\")\n\nmodel_train &lt;- lrm(Outcome ~ \n                     rcs(Age, 4) + \n                     Sex + \n                     rcs(Urea, 4) + \n                     rcs(Creatinine, 4) + \n                     rcs(Potassium, 4) +\n                     IHD + COPD + Diabetes + ValveDisease + NYHA_class,\n                   data = hf_train,\n                   x = TRUE, y = TRUE)"
  },
  {
    "objectID": "Validation.html#discrimination-roc-and-auc-test-set",
    "href": "Validation.html#discrimination-roc-and-auc-test-set",
    "title": "Validation",
    "section": "Discrimination (ROC and AUC) – Test Set",
    "text": "Discrimination (ROC and AUC) – Test Set\n\npred_test &lt;- predict(model_train, newdata = hf_test, type = \"fitted\")\nroc_test &lt;- roc(hf_test$Outcome, pred_test)\nauc_test &lt;- auc(roc_test)\n\nroc_df &lt;- data.frame(\n  specificity = roc_test$specificities,\n  sensitivity = roc_test$sensitivities\n)\n\nggplot(roc_df, aes(x = 1 - specificity, y = sensitivity)) +\n  geom_line(color = \"steelblue\", size = 1.2) +\n  geom_abline(linetype = \"dashed\") +\n  theme_minimal() +\n  labs(title = paste(\"Test Set ROC Curve (AUC =\", round(auc_test, 3), \")\"),\n       x = \"1 - Specificity\", y = \"Sensitivity\")"
  },
  {
    "objectID": "Validation.html#calibration-smoothed-test-set",
    "href": "Validation.html#calibration-smoothed-test-set",
    "title": "Validation",
    "section": "Calibration – Smoothed (Test Set)",
    "text": "Calibration – Smoothed (Test Set)\n\ncalib_df &lt;- hf_test %&gt;%\n  mutate(pred = pred_test)\n\nggplot(calib_df, aes(x = pred, y = Outcome)) +\n  geom_smooth(method = \"loess\", se = TRUE, color = \"firebrick\", fill = \"pink\", span = 0.75) +\n  geom_abline(linetype = \"dashed\") +\n  theme_minimal() +\n  labs(title = \"Calibration Curve (Test Set)\",\n       x = \"Predicted Probability\",\n       y = \"Observed Mortality\")"
  },
  {
    "objectID": "Validation.html#goodness-of-fit-summary-training-vs-test-set",
    "href": "Validation.html#goodness-of-fit-summary-training-vs-test-set",
    "title": "Validation",
    "section": "Goodness-of-Fit Summary: Training vs Test Set",
    "text": "Goodness-of-Fit Summary: Training vs Test Set\n\n# Training set predictions\npred_train &lt;- predict(model_train, type = \"fitted\")\n\n# Brier Score\nbrier_train &lt;- mean((hf_train$Outcome - pred_train)^2)\nbrier_test &lt;- mean((hf_test$Outcome - pred_test)^2)\n\n# Calibration slope and intercept (test set)\ncal_slope_model &lt;- glm(Outcome ~ offset(qlogis(pred_test)), data = hf_test, family = binomial)\ncal_intercept_model &lt;- glm(Outcome ~ I(qlogis(pred_test)), data = hf_test, family = binomial)\n\ncal_slope &lt;- coef(cal_slope_model)\ncal_intercept &lt;- coef(cal_intercept_model)\n\n# AUC\nroc_train &lt;- roc(hf_train$Outcome, pred_train)\nauc_train &lt;- auc(roc_train)\n\n# Summary table\nsummary_df &lt;- tibble::tibble(\n  Metric = c(\"AUROC\", \"Calibration Intercept\", \"Calibration Slope\", \"Brier Score\"),\n  `Training Set` = c(round(auc_train, 3), 0, 1, round(brier_train, 3)),\n  `Test Set` = c(round(auc_test, 3), round(cal_intercept[1], 3), round(cal_slope[1], 3), round(brier_test, 3))\n)\n\nsummary_df\n\n# A tibble: 4 × 3\n  Metric                `Training Set` `Test Set`\n  &lt;chr&gt;                          &lt;dbl&gt;      &lt;dbl&gt;\n1 AUROC                          0.768      0.769\n2 Calibration Intercept          0          0.033\n3 Calibration Slope              1          0.01 \n4 Brier Score                    0.088      0.088"
  },
  {
    "objectID": "uncertainty.html",
    "href": "uncertainty.html",
    "title": "Some topics in LR",
    "section": "",
    "text": "Here I explore some topics in logistic regression manually, including techniques in estimation, uncertainty and validation. I’ll generate a simulated dataset by way of illustration."
  },
  {
    "objectID": "Nonlinearity.html",
    "href": "Nonlinearity.html",
    "title": "Nonlinearity",
    "section": "",
    "text": "In this document, I build several models of the heart failure data that are able to handle non-linearities in the predictors, generate predictions and perform model validation."
  },
  {
    "objectID": "Nonlinearity.html#environment",
    "href": "Nonlinearity.html#environment",
    "title": "Nonlinearity",
    "section": "Environment",
    "text": "Environment\nLoad the requisite libraries.\n\nlibrary(rms)\nlibrary(dplyr)\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(gtsummary)\nlibrary(ggplot2)\nlibrary(splines)\nlibrary(broom)\nlibrary(pROC)\nlibrary(knitr)\nlibrary(mgcv)\nlibrary(patchwork)"
  },
  {
    "objectID": "Nonlinearity.html#data",
    "href": "Nonlinearity.html#data",
    "title": "Nonlinearity",
    "section": "Data",
    "text": "Data\nHaving loaded the various libraries needed, let’s import our heart failure data.\n\nsetwd(\"C:/Users/rmhimdi/OneDrive - University College London/Documents/HeartFailure\")\ndf &lt;- read_excel(\"simulateddata_LR_NL.xlsx\")\n\nDiscretise age and recode sex.\n\ndf$age &lt;- round(df$age, 0)\nn = 5\ndf &lt;- df %&gt;%\n  mutate(Age_ = paste0(\n    n*floor(age/n), \"-\",n*(floor(age/n)+1)\n    )\n  )\n#df$sex &lt;- factor(df$sex, levels = c(0, 1), labels = c(\"Male\", \"Female\")) \n## Assuming Female = 1 since this seems mildly protective (cf. MAGGIC), but will need to check.\n\nRenaming varibles.\n\ndf &lt;- \n  df %&gt;% \n  rename(\n    Outcome = outcome,\n    Age = age,\n    Sex = sex,\n    Creatinine = creatineDischarge,\n    Sodium = sodiumDischarge,\n    Potassium = potassiumDischarge,\n    Urea = ureaDischarge,\n    S_bp =  sbpAdmission,\n    HR = hrAdmission,\n    Hb = hbDischarge,\n    Diabetes = diabetes,\n    COPD = copd,\n    IHD = ihd, \n    ValveDisease = valveDisease,\n    NYHA_class = nyha,\n    PeripheralOedema = peripheralOedema,\n    AF = af,\n    eGFR = egfr_full\n  )\n\nInspect the processed data.\n\nkable(head(df))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOutcome\nAge\nCreatinine\nSodium\nPotassium\nUrea\nS_bp\nHR\nHb\nSex\nDiabetes\nCOPD\nIHD\nValveDisease\nNYHA_class\nPeripheralOedema\nAF\neGFR\nAge_\n\n\n\n\n0\n62\n76.99999\n138\n4.2\n19.6100\n113\n119\n11.1\n0\n1\n0\n1\n0\n1\n0\n1\n88.70592\n60-65\n\n\n1\n90\n142.00003\n146\n4.4\n20.9000\n98\n78\n12.1\n0\n0\n0\n1\n1\n0\n0\n0\n40.60656\n90-95\n\n\n1\n84\n143.00003\n141\n4.0\n15.7056\n132\n102\n13.7\n0\n0\n1\n0\n0\n1\n1\n0\n40.90517\n80-85\n\n\n0\n75\n80.00000\n137\n4.0\n7.5000\n152\n100\n11.6\n1\n0\n0\n0\n0\n1\n1\n0\n60.71780\n75-80\n\n\n0\n84\n123.99999\n139\n4.2\n8.2000\n201\n46\n11.8\n1\n0\n0\n1\n0\n0\n0\n0\n35.78796\n80-85\n\n\n0\n78\n118.00001\n140\n4.7\n12.7000\n104\n64\n10.6\n1\n0\n0\n0\n0\n1\n1\n1\n38.37403\n75-80\n\n\n\n\n\n\nHold Out\nDuring model evaluation, we will need to conduct predictive validation using the test set having estimated the model coefficients on the training set.\n\nset.seed(1729)\nrecords &lt;- dim(df)[1]\n\ntrain_size &lt;- floor(0.8*records)\ntrain_index &lt;- sample(seq_len(records), size = train_size)\ntrain &lt;- df[train_index,]\ntest &lt;- df[-train_index,]\nprint(paste(\"Size of training set is: \", nrow(train)))\n\n[1] \"Size of training set is:  43264\"\n\nprint(paste(\"Size of testing set is: \", nrow(test)))\n\n[1] \"Size of testing set is:  10817\""
  },
  {
    "objectID": "Nonlinearity.html#full-model",
    "href": "Nonlinearity.html#full-model",
    "title": "Nonlinearity",
    "section": "Full Model",
    "text": "Full Model\nRun the naive model with all variables to see outline effect sizes.\n\n#Sans age categorisation\nfull_model &lt;- glm(Outcome ~ Age + Sex  + Creatinine + Sodium + Potassium + Urea + S_bp + HR + Hb+ Diabetes + COPD + IHD + ValveDisease + NYHA_class + PeripheralOedema + AF + eGFR, family = binomial, data = df)\nprint(summary(full_model))\n\n\nCall:\nglm(formula = Outcome ~ Age + Sex + Creatinine + Sodium + Potassium + \n    Urea + S_bp + HR + Hb + Diabetes + COPD + IHD + ValveDisease + \n    NYHA_class + PeripheralOedema + AF + eGFR, family = binomial, \n    data = df)\n\nCoefficients:\n                   Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)      -7.1398644  0.4689428 -15.225  &lt; 2e-16 ***\nAge               0.0473564  0.0016273  29.102  &lt; 2e-16 ***\nSex               0.1904866  0.0316661   6.015 1.79e-09 ***\nCreatinine        0.0036828  0.0002298  16.024  &lt; 2e-16 ***\nSodium            0.0077452  0.0028043   2.762 0.005746 ** \nPotassium         0.3237376  0.0238317  13.584  &lt; 2e-16 ***\nUrea              0.0375205  0.0013179  28.469  &lt; 2e-16 ***\nS_bp             -0.0203323  0.0006165 -32.982  &lt; 2e-16 ***\nHR                0.0046018  0.0006626   6.945 3.78e-12 ***\nHb               -0.0256031  0.0075885  -3.374 0.000741 ***\nDiabetes         -0.1079845  0.0326271  -3.310 0.000934 ***\nCOPD              0.2200697  0.0363999   6.046 1.49e-09 ***\nIHD               0.0765564  0.0302083   2.534 0.011268 *  \nValveDisease      0.0518244  0.0321303   1.613 0.106757    \nNYHA_class       -0.0267937  0.0367931  -0.728 0.466475    \nPeripheralOedema  0.0914774  0.0304967   3.000 0.002704 ** \nAF               -0.0345800  0.0303462  -1.140 0.254488    \neGFR              0.0016651  0.0009438   1.764 0.077691 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 38473  on 54080  degrees of freedom\nResidual deviance: 32723  on 54063  degrees of freedom\nAIC: 32759\n\nNumber of Fisher Scoring iterations: 6\n\nprint(\"Odd Ratios\")\n\n[1] \"Odd Ratios\"\n\nkable(exp(coef(full_model)))\n\n\n\n\n\nx\n\n\n\n\n(Intercept)\n0.0007929\n\n\nAge\n1.0484956\n\n\nSex\n1.2098382\n\n\nCreatinine\n1.0036896\n\n\nSodium\n1.0077752\n\n\nPotassium\n1.3822845\n\n\nUrea\n1.0382333\n\n\nS_bp\n0.9798730\n\n\nHR\n1.0046124\n\n\nHb\n0.9747219\n\n\nDiabetes\n0.8976415\n\n\nCOPD\n1.2461636\n\n\nIHD\n1.0795631\n\n\nValveDisease\n1.0531908\n\n\nNYHA_class\n0.9735621\n\n\nPeripheralOedema\n1.0957920\n\n\nAF\n0.9660111\n\n\neGFR\n1.0016665\n\n\n\n\n\nWhile these Odds Ratios give us an outline picture of the effects, it is unlikely that all relationships are linear in the ‘true’ model. So, let’s try a few ways of relaxing the linearity assumption."
  },
  {
    "objectID": "Nonlinearity.html#age",
    "href": "Nonlinearity.html#age",
    "title": "Nonlinearity",
    "section": "Age",
    "text": "Age\nLet’s try to identify any nonlinear age effect. We can do this in a few different ways.\n\nCategorisation\nLet’s start by using the age quintiles. We run two models to compare:\n\nmod1 &lt;- train %&gt;% \n  glm(formula = Outcome ~ Age, family = binomial)\nsummary(mod1)\n\n\nCall:\nglm(formula = Outcome ~ Age, family = binomial, data = .)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -5.647031   0.130533  -43.26   &lt;2e-16 ***\nAge          0.044565   0.001569   28.40   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 30626  on 43263  degrees of freedom\nResidual deviance: 29667  on 43262  degrees of freedom\nAIC: 29671\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nmod1 &lt;- train %&gt;% \n  glm(formula = Outcome ~ Age_, family = binomial)\nsummary(mod1)\n\n\nCall:\nglm(formula = Outcome ~ Age_, family = binomial, data = .)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -0.7867     0.1689  -4.657 3.21e-06 ***\nAge_105-110  -1.5159     1.0623  -1.427 0.153587    \nAge_15-20   -12.7794   267.7056  -0.048 0.961926    \nAge_20-25   -12.7794   101.1833  -0.126 0.899495    \nAge_25-30    -2.5092     0.7396  -3.392 0.000693 ***\nAge_30-35    -3.2993     0.7328  -4.503 6.71e-06 ***\nAge_35-40    -3.3297     0.6061  -5.494 3.93e-08 ***\nAge_40-45    -3.0072     0.4179  -7.197 6.17e-13 ***\nAge_45-50    -2.6675     0.2878  -9.268  &lt; 2e-16 ***\nAge_50-55    -2.6546     0.2487 -10.676  &lt; 2e-16 ***\nAge_55-60    -2.4627     0.2194 -11.225  &lt; 2e-16 ***\nAge_60-65    -1.9998     0.1932 -10.353  &lt; 2e-16 ***\nAge_65-70    -1.7313     0.1833  -9.444  &lt; 2e-16 ***\nAge_70-75    -1.7104     0.1776  -9.632  &lt; 2e-16 ***\nAge_75-80    -1.3749     0.1741  -7.898 2.84e-15 ***\nAge_80-85    -1.2114     0.1722  -7.034 2.01e-12 ***\nAge_85-90    -1.0309     0.1719  -5.997 2.01e-09 ***\nAge_90-95    -0.7899     0.1728  -4.572 4.84e-06 ***\nAge_95-100   -0.4525     0.1795  -2.520 0.011723 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 30626  on 43263  degrees of freedom\nResidual deviance: 29658  on 43245  degrees of freedom\nAIC: 29696\n\nNumber of Fisher Scoring iterations: 12\n\n\n\n\nSplines\n\nmod3 &lt;- glm(Outcome ~ ns(Age, df=3) + Sex  + ns(Creatinine, df=3) + Sodium + ns(Potassium, df=3) + ns(Urea, df=3) + S_bp + HR + Hb+ Diabetes + COPD + IHD + ValveDisease + NYHA_class + PeripheralOedema + AF + eGFR, family = binomial, data = train)\nprint(summary(mod3))\n\n\nCall:\nglm(formula = Outcome ~ ns(Age, df = 3) + Sex + ns(Creatinine, \n    df = 3) + Sodium + ns(Potassium, df = 3) + ns(Urea, df = 3) + \n    S_bp + HR + Hb + Diabetes + COPD + IHD + ValveDisease + NYHA_class + \n    PeripheralOedema + AF + eGFR, family = binomial, data = train)\n\nCoefficients:\n                          Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)             -3.2297212  0.8855541  -3.647 0.000265 ***\nns(Age, df = 3)1         2.2499750  0.2130202  10.562  &lt; 2e-16 ***\nns(Age, df = 3)2         5.3885694  0.8362700   6.444 1.17e-10 ***\nns(Age, df = 3)3         3.6383468  0.2490317  14.610  &lt; 2e-16 ***\nSex                      0.3452108  0.0552986   6.243 4.30e-10 ***\nns(Creatinine, df = 3)1  4.0797090  0.5571073   7.323 2.42e-13 ***\nns(Creatinine, df = 3)2  1.7759930  0.8101710   2.192 0.028371 *  \nns(Creatinine, df = 3)3 -1.2744470  0.7870926  -1.619 0.105408    \nSodium                   0.0051292  0.0031289   1.639 0.101149    \nns(Potassium, df = 3)1  -1.4475200  0.1351737 -10.709  &lt; 2e-16 ***\nns(Potassium, df = 3)2  -4.4924440  0.6021319  -7.461 8.59e-14 ***\nns(Potassium, df = 3)3   2.0966514  0.3195196   6.562 5.31e-11 ***\nns(Urea, df = 3)1        3.5024779  0.1396471  25.081  &lt; 2e-16 ***\nns(Urea, df = 3)2        3.5433371  0.2873837  12.330  &lt; 2e-16 ***\nns(Urea, df = 3)3        1.0218810  0.2459434   4.155 3.25e-05 ***\nS_bp                    -0.0189166  0.0006991 -27.060  &lt; 2e-16 ***\nHR                       0.0051112  0.0007596   6.729 1.71e-11 ***\nHb                      -0.0137585  0.0086356  -1.593 0.111108    \nDiabetes                -0.1036697  0.0375945  -2.758 0.005823 ** \nCOPD                     0.2105146  0.0419030   5.024 5.06e-07 ***\nIHD                      0.1033113  0.0345790   2.988 0.002811 ** \nValveDisease             0.0688294  0.0367263   1.874 0.060914 .  \nNYHA_class              -0.0464105  0.0421425  -1.101 0.270777    \nPeripheralOedema         0.0174441  0.0348879   0.500 0.617072    \nAF                      -0.0416977  0.0347090  -1.201 0.229615    \neGFR                     0.0156079  0.0031332   4.981 6.31e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 30626  on 43263  degrees of freedom\nResidual deviance: 25036  on 43238  degrees of freedom\nAIC: 25088\n\nNumber of Fisher Scoring iterations: 6\n\n\nHaving tried a few non-linear model options, it seems that we need a more systematic way to specify the functional form.\nFor this, we can visualise the empirical distribution of the dependent variable by the candidate covariates and inspect the shape.\n\ngam_model &lt;- gam(Outcome ~ s(Age) \n                   #+ s(Sex) + s(Creatinine) + s(Sodium) + s(Potassium) + s(Urea) + s(S_bp) + s(HR) + s(Hb) +\n                   #s(Diabetes) + s(COPD) + s(IHD) + s(ValveDisease) + s(NYHA_class) + s(PeripheralOedema) + s(AF) + s(eGFR)\n                   #, family = binomial\n                 , data = df)\nplot(gam_model)\n\n\n\n\n\n\n\n\n\nage_summary &lt;- df %&gt;%\n  group_by(Age) %&gt;%\n  summarise(\n    n = n(),\n    deaths = sum(Outcome),\n    mortality_rate = deaths / n,\n    log_odds = log((mortality_rate) / (1 - mortality_rate))  # small offset to avoid log(0)\n  )\n\nggplot(age_summary, aes(x = Age, y = log_odds)) +\n  geom_point(alpha = 0.6) +\n  geom_abline() +\n  geom_smooth(se = FALSE, method = \"loess\", span = 0.3) +\n  labs(\n    x = \"Age\",\n    y = \"Empirical log-odds of mortality\",\n    title = \"Empirical log-odds of mortality by Age\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 19 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\n\n\n\n\n\n\n\n\nplot_all_empirical_mortality_and_logodds &lt;- function(data, outcome, min_n = 10, smooth_span = 0.3, n_bins = 10, stratify_by = \"Age\") {\n  \n  outcome_sym &lt;- ensym(outcome)\n  outcome_chr &lt;- rlang::as_string(outcome_sym)\n  strat_chr &lt;- stratify_by\n  strat_sym &lt;- sym(strat_chr)\n  \n  predictors &lt;- setdiff(names(data), c(outcome_chr, strat_chr))\n  plots &lt;- list()\n  \n  for (pred in predictors) {\n    predictor_sym &lt;- ensym(pred)\n    pred_values &lt;- data %&gt;% pull(!!predictor_sym)\n    \n    # Skip if constant or fully missing\n    if (n_distinct(pred_values, na.rm = TRUE) &lt;= 1) next\n    \n    is_binary &lt;- all(pred_values %in% c(0, 1), na.rm = TRUE)\n    \n    if (is_binary) {\n      # Binary predictors now plotted against Age\n      df_binned &lt;- data %&gt;%\n        filter(!is.na(!!predictor_sym), !is.na(!!strat_sym), !is.na(!!outcome_sym)) %&gt;%\n        mutate(age_bin = cut_number(!!strat_sym, n = n_bins)) %&gt;%\n        group_by(!!predictor_sym, age_bin) %&gt;%\n        summarise(\n          mean_age = mean(!!strat_sym, na.rm = TRUE),\n          n = n(),\n          deaths = sum(!!outcome_sym),\n          mortality_rate = deaths / n,\n          log_odds = log((mortality_rate + 1e-3) / (1 - mortality_rate + 1e-3)),\n          .groups = \"drop\"\n        ) %&gt;%\n        filter(n &gt;= min_n)\n      \n      p1 &lt;- ggplot(df_binned, aes(x = mean_age, y = mortality_rate, color = factor(!!predictor_sym))) +\n        geom_point(alpha = 0.7) +\n        geom_line() +\n        labs(x = strat_chr, y = \"Mortality rate\", color = pred,\n             title = paste(\"Mortality by\", strat_chr, \"stratified by\", pred)) +\n        theme_minimal()\n      \n      p2 &lt;- ggplot(df_binned, aes(x = mean_age, y = log_odds, color = factor(!!predictor_sym))) +\n        geom_point(alpha = 0.7) +\n        geom_line() +\n        labs(x = strat_chr, y = \"Log-odds of mortality\", color = pred,\n             title = paste(\"Log-odds by\", strat_chr, \"stratified by\", pred)) +\n        theme_minimal()\n      \n    } else {\n      # Regular continuous predictors\n      unique_vals &lt;- n_distinct(pred_values, na.rm = TRUE)\n      \n      if (unique_vals &gt; 5) {\n        data_binned &lt;- data %&gt;%\n          mutate(bin = cut_number(!!predictor_sym, n = n_bins))\n        \n        df_summary &lt;- data_binned %&gt;%\n          group_by(bin) %&gt;%\n          summarise(\n            mean_value = mean(!!predictor_sym, na.rm = TRUE),\n            n = n(),\n            deaths = sum(!!outcome_sym),\n            mortality_rate = deaths / n,\n            log_odds = log((mortality_rate + 1e-3) / (1 - mortality_rate + 1e-3)),\n            .groups = \"drop\"\n          ) %&gt;%\n          filter(n &gt;= min_n)\n        \n        p1 &lt;- ggplot(df_summary, aes(x = mean_value, y = mortality_rate)) +\n          geom_point(alpha = 0.6) +\n          geom_line() +\n          labs(x = pred, y = \"Mortality rate\", title = paste(\"Binned mortality by\", pred)) +\n          theme_minimal()\n        \n        p2 &lt;- ggplot(df_summary, aes(x = mean_value, y = log_odds)) +\n          geom_point(alpha = 0.6) +\n          geom_line() +\n          labs(x = pred, y = \"Log-odds of mortality\", title = paste(\"Binned log-odds by\", pred)) +\n          theme_minimal()\n        \n      } else {\n        df_summary &lt;- data %&gt;%\n          group_by(value = !!predictor_sym) %&gt;%\n          summarise(\n            n = n(),\n            deaths = sum(!!outcome_sym),\n            mortality_rate = deaths / n,\n            log_odds = log((mortality_rate + 1e-3) / (1 - mortality_rate + 1e-3)),\n            .groups = \"drop\"\n          ) %&gt;%\n          filter(n &gt;= min_n)\n        \n        p1 &lt;- ggplot(df_summary, aes(x = value, y = mortality_rate)) +\n          geom_point(alpha = 0.6) +\n          geom_line() +\n          labs(x = pred, y = \"Mortality rate\", title = paste(\"Mortality by\", pred)) +\n          theme_minimal()\n        \n        p2 &lt;- ggplot(df_summary, aes(x = value, y = log_odds)) +\n          geom_point(alpha = 0.6) +\n          geom_line() +\n          labs(x = pred, y = \"Log-odds of mortality\", title = paste(\"Log-odds by\", pred)) +\n          theme_minimal()\n      }\n    }\n    \n    plots[[pred]] &lt;- p1 + p2\n  }\n  \n  return(plots)\n}\n\n\ndf = subset(df, select = -c(Age_))\nplot_all_empirical_mortality_and_logodds(data = df, outcome = Outcome)\n\n$Creatinine\n\n\n\n\n\n\n\n\n\n\n$Sodium\n\n\n\n\n\n\n\n\n\n\n$Potassium\n\n\n\n\n\n\n\n\n\n\n$Urea\n\n\n\n\n\n\n\n\n\n\n$S_bp\n\n\n\n\n\n\n\n\n\n\n$HR\n\n\n\n\n\n\n\n\n\n\n$Hb\n\n\n\n\n\n\n\n\n\n\n$Sex\n\n\n\n\n\n\n\n\n\n\n$Diabetes\n\n\n\n\n\n\n\n\n\n\n$COPD\n\n\n\n\n\n\n\n\n\n\n$IHD\n\n\n\n\n\n\n\n\n\n\n$ValveDisease\n\n\n\n\n\n\n\n\n\n\n$NYHA_class\n\n\n\n\n\n\n\n\n\n\n$PeripheralOedema\n\n\n\n\n\n\n\n\n\n\n$AF\n\n\n\n\n\n\n\n\n\n\n$eGFR\n\n\n\n\n\n\n\n\n\n\n\nHierarchical\nThis is a Quarto website. To learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "uncertainty.html#manual-exploration-of-logistic-regression",
    "href": "uncertainty.html#manual-exploration-of-logistic-regression",
    "title": "Some Topics in Logistic Regression",
    "section": "Manual Exploration of Logistic Regression",
    "text": "Manual Exploration of Logistic Regression\n\n1. Introduction\nWe manually explore logistic regression, highlighting key concepts of estimation, uncertainty, and validation. We use a manually simulated dataset to illustrate each step clearly.\n\n\n2. Simulating Data\nWe simulate data with one predictor:\n\\[\nP(Y = 1 \\mid X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X)}}\n\\]\nwith parameters:\n\n\\(\\beta_0 = -0.5\\)\n\\(\\beta_1 = 2\\).\n\n\n\nR code:\nset.seed(123)\nn &lt;- 100\nbeta_0 &lt;- -0.5\nbeta_1 &lt;- 2\nx &lt;- rnorm(n)\neta &lt;- beta_0 + beta_1 * x\nprob &lt;- 1 / (1 + exp(-eta))\ny &lt;- rbinom(n, 1, prob)\nsim_data &lt;- data.frame(y, x)\n\n\n\n3. Logistic Model and Likelihood\nThe logistic model is:\n\\[\nP(Y = 1 \\mid X, \\beta) = \\frac{1}{1 + e^{-X\\beta}}\n\\]\nNote: include likelihood version from written notes. Log-likelihood:\n\\[\n\\ell(\\beta) = \\sum_{i=1}^{n} \\left[ y_i(X_i \\beta) - \\log(1 + e^{X_i \\beta}) \\right]\n\\]\n\n\n\n4. Gradient and Hessian\nGradient (score function):\n\\[\n\\nabla \\ell(\\beta) = X^T(y - p)\n\\]\nHessian:\n\\[\nH(\\beta) = -X^T W X\n\\]\nwhere \\(W\\) is diagonal with elements \\(p_i(1 - p_i)\\).\n\n\nR code for Gradient and Hessian:\nX &lt;- cbind(1, x)\nbeta &lt;- c(0, 0)\neta &lt;- X %*% beta\np &lt;- 1 / (1 + exp(-eta))\ngradient &lt;- t(X) %*% (y - p)\nW &lt;- diag(as.vector(p * (1 - p)))\nhessian &lt;- -t(X) %*% W %*% X\n\n\n\n5. Newton-Raphson Algorithm\nIteration step:\n\\[\n\\beta_{\\text{new}} = \\beta_{\\text{old}} - [H(\\beta_{\\text{old}})]^{-1} \\nabla \\ell(\\beta_{\\text{old}})\n\\]\n\n\nR implementation:\nbeta &lt;- c(0, 0)\nfor (i in 1:10) {\n  eta &lt;- X %*% beta\n  p &lt;- 1 / (1 + exp(-eta))\n  gradient &lt;- t(X) %*% (y - p)\n  W &lt;- diag(as.vector(p * (1 - p)))\n  hessian &lt;- -t(X) %*% W %*% X\n  beta &lt;- beta - solve(hessian) %*% gradient\n}\nprint(beta)\n\n\n\n6. Variance of Estimates\nEstimated covariance matrix:\n\\[\n\\text{Var}(\\hat{\\beta}) = (X^T W X)^{-1}\n\\]\n\n\n\n7. Bootstrap Validation\nRepeated sampling with replacement to estimate standard errors.\n\n\nBootstrap R code:\nB &lt;- 500\nbootstrap_estimates &lt;- matrix(NA, B, 2)\n\nfor (b in 1:B) {\n  idx &lt;- sample(1:n, replace = TRUE)\n  X_b &lt;- X[idx, ]\n  y_b &lt;- y[idx]\n  beta_b &lt;- glm(y_b ~ X_b[,2], family=\"binomial\")$coefficients\n  bootstrap_estimates[b, ] &lt;- beta_b\n}\n\napply(bootstrap_estimates, 2, sd)\n\n\n\n8. Cross-validation\nPartition data into $K$ folds, train/test each fold.\n\n\nExample 5-fold CV in R:\nK &lt;- 5\nfolds &lt;- sample(rep(1:K, length.out = n))\ncv_error &lt;- numeric(K)\n\nfor (k in 1:K) {\n  train_idx &lt;- which(folds != k)\n  test_idx &lt;- which(folds == k)\n  fit &lt;- glm(y ~ x, family=\"binomial\", data=sim_data[train_idx,])\n  preds &lt;- predict(fit, sim_data[test_idx,], type=\"response\")\n  cv_error[k] &lt;- mean((sim_data$y[test_idx] - preds)^2)\n}\nmean(cv_error)\n\n\n\n9. Comparison of Bootstrap and Cross-validation\nDiscuss strengths/weaknesses:\n\nBootstrap gives parameter uncertainty.\nCV provides estimate of prediction error.\n\n\n\n\n10. Extensions and Practical Considerations\nDiscuss regularization and connections to real-world datasets, and further work."
  },
  {
    "objectID": "uncertainty.html#introduction",
    "href": "uncertainty.html#introduction",
    "title": "Some topics in LR",
    "section": "",
    "text": "Here I explore some topics in logistic regression manually, including techniques in estimation, uncertainty and validation. I’ll generate a simulated dataset by way of illustration."
  },
  {
    "objectID": "uncertainty.html#simulated-data",
    "href": "uncertainty.html#simulated-data",
    "title": "Some topics in LR",
    "section": "2. Simulated Data",
    "text": "2. Simulated Data\nCreate simple dataset from the logic model with one predictor\n\\[\nP(Y = 1 \\mid X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X)}}\n\\]\nwith parameters\n\n\\(\\beta_0 = -0.5\\)\n\\(\\beta_1 = 2\\).\n\n\nset.seed(112358)\nn &lt;- 1000\nbeta_0 &lt;- -0.5\nbeta_1 &lt;- 2\nx &lt;- rnorm(n)\neta &lt;- beta_0 + beta_1 * x\nprob &lt;- 1 / (1 + exp(-eta))\ny &lt;- rbinom(n, 1, prob)\nsim_data &lt;- data.frame(y, x, prob)\ndata_sort &lt;- sim_data[order(sim_data$x), ]\n\nplot(sim_data$x, sim_data$y,\n     xlab = \"Simulated x\", ylab = \"Simulated Y\",\n     main = \"Simulated data for Logistic Regression\"\n)\nlines(data_sort$x, data_sort$prob, col = \"blue\", lty = 2, lwd = 2)\nlegend(\"topleft\", legend = \"True model\", col = \"blue\", lty = 2, lwd = 2)"
  },
  {
    "objectID": "uncertainty.html#logistic-model-and-estimation",
    "href": "uncertainty.html#logistic-model-and-estimation",
    "title": "Some topics in LR",
    "section": "3. Logistic Model and Estimation",
    "text": "3. Logistic Model and Estimation\nGiven a logit model of the form:\n\\[\n\\log\\left(\\frac{\\pi_i}{1 - \\pi_i}\\right) = \\eta_i,\n\\]\nwhere\n\\[\n\\eta_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip}.\n\\]\nWe rearrange for \\(\\pi_i\\):\n\\[\n\\pi_i = \\frac{\\exp(\\eta_i)}{1 + \\exp(\\eta_i)} = \\frac{1}{1 + \\exp(-\\eta_i)}.\n\\]\nParameter estimates for \\(\\beta_j\\) are found firstly by deriving the likelihood function:\n\\[\nL(\\beta_j) = \\prod_{i=1}^{n} \\pi_i^{y_i} (1 - \\pi_i)^{1 - y_i}\n\\]\nwhere ( n ) is the number of observations. In MLE to goal is to maximize the log-likelihood:\n\\[\n\\log L = \\ell(\\beta_j) = \\sum_{i=1}^{n} y_i \\log(\\pi_i) + (1 - y_i) \\log(1 - \\pi_i),\n\\]\n\\[\n= \\sum_{i=1}^{n} \\log(1 - \\pi_i) + \\sum_{i=1}^{n} y_i \\left[ \\log(\\pi_i) - \\log(1 - \\pi_i) \\right],\n\\]\n\\[\n= \\sum_{i=1}^{n} \\log(1 - \\pi_i) + \\sum_{i=1}^{n} y_i \\log\\left( \\frac{\\pi_i}{1 - \\pi_i} \\right).\n\\]\nSince the logistic function is\n\\[\n\\pi_i = \\frac{1}{1 + \\exp(-\\eta_i)},\n\\]\nit follows that\n\\[\n1 - \\pi_i = 1 - \\frac{1}{1 + \\exp(-\\eta_i)} = \\frac{\\exp(-\\eta_i)}{1 + \\exp(-\\eta_i)}.\n\\]\nWe can take the odds:\n\\[\n\\frac{\\pi_i}{1 - \\pi_i} = \\frac{\\frac{1}{1 + \\exp(-\\eta_i)}}{\\frac{\\exp(-\\eta_i)}{1 + \\exp(-\\eta_i)}} = \\frac{1}{\\exp(-\\eta_i)} = \\exp(\\eta_i).\n\\]\nSubstitute this back into the previous equation:\n\\[\n\\mathcal{L} = \\sum_{i=1}^{n} \\left( \\log(1 - \\pi_i) + \\sum_{j=1}^{p} y_i (\\beta_0 + \\beta_1 x_{i1} + \\dots + \\beta_p x_{ip}) \\right),\n\\]\n\\[\n= \\sum_{i=1}^{n} \\left[ -\\log(1 + \\exp(\\eta_i)) + \\sum_{j=1}^{p} y_i \\eta_i \\right].\n\\]\nUsually in MLE we differentiate and set to 0:\n\\[\n\\frac{\\partial \\ell}{\\partial \\beta_j} = \\sum_{i=1}^{n} \\frac{1}{1 + \\exp(\\eta_i)} \\cdot \\exp(\\eta_i) x_{ij} + \\sum_{i=1}^{n} y_i x_{ij},\n\\]\n\\[\n= \\sum_{i=1}^{n} \\frac{\\exp(\\eta_i)}{1 + \\exp(\\eta_i)} x_{ij} + \\sum_{i=1}^{n} y_i x_{ij},\n\\]\n\\[\n= \\sum_{i=1}^{n} (y_i - \\pi_i) x_{ij} = 0.\n\\]\nHowever, this has no closed-form solution, so numerical methods are used to estimate \\(\\beta_j\\), e.g. Newton-Raphson. For this, we first need to derive the Score Function and the Hessian."
  },
  {
    "objectID": "uncertainty.html#gradient-and-hessian",
    "href": "uncertainty.html#gradient-and-hessian",
    "title": "Some Topics in Logistic Regression",
    "section": "4. Gradient and Hessian",
    "text": "4. Gradient and Hessian\nTo find the maxima, we firstly need to know how the log likelihood function varies w.r.t its \\(\\beta\\) parameters This is just its first derivative, called the Score Function:\nThe likelihood function for the whole sample is:\n\\[\nL(\\boldsymbol{\\beta}) = \\prod_{i=1}^n \\pi_i^{y_i} (1 - \\pi_i)^{1 - y_i}\n\\]\nTaking logs gives the log-likelihood:\n\\[\n\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\left[ y_i \\log(\\pi_i) + (1 - y_i) \\log(1 - \\pi_i) \\right]\n\\]\n\nDifferentiate it\nTo find the derivative of \\(\\ell(\\boldsymbol{\\beta})\\) with respect to \\(\\beta_1\\), isolate a single observation\n\\[\n\\ell_i = y_i \\log(\\pi_i) + (1 - y_i) \\log(1 - \\pi_i)\n\\]\nand note that the desired derivative is the product of three simpler derivatives via the chain rule:\n\\[\n\\frac{d\\ell_i}{d\\beta_1}\n= \\frac{d\\ell_i}{d\\pi_i} \\cdot \\frac{d\\pi_i}{d\\eta_i} \\cdot \\frac{d\\eta_i}{d\\beta_1}\n\\]\nTheese derivatives evaluate as:\n\nDerivative with respect to \\(\\pi_i\\) follows from the derivative of logs:\n\\[\n\\frac{d\\ell_i}{d\\pi_i} = \\frac{y_i}{\\pi_i} - \\frac{1 - y_i}{1 - \\pi_i}\n\\]\nThis derivative of the logistic function follows when we notice that the direct derivative of \\(\\pi_i\\) is equivalent to the result of multiplying it by \\((1-\\pi_i)\\):\n\\[\n\\pi_i = \\frac{1}{1 + \\exp(-\\eta_i)} \\quad \\Rightarrow \\quad \\frac{d\\pi_i}{d\\eta_i} = \\pi_i (1 - \\pi_i)\n\\]\nThe derivative of the linear predictor is fairly trivial:\n\\[\n\\frac{d\\eta_i}{d\\beta_1} = x_i\n\\]\n\n\n\nMultiplying the three derivatives\nStart by substituting:\n\\[\n\\frac{d\\ell_i}{d\\beta_1}\n= \\left( \\frac{y_i}{\\pi_i} - \\frac{1 - y_i}{1 - \\pi_i} \\right) \\cdot \\pi_i (1 - \\pi_i) \\cdot x_i\n\\]\nExpand and simplify:\n\\[\n\\left( \\frac{y_i}{\\pi_i} - \\frac{1 - y_i}{1 - \\pi_i} \\right) \\cdot \\pi_i (1 - \\pi_i)\n= y_i (1 - \\pi_i) - (1 - y_i) \\pi_i = y_i - \\pi_i\n\\]\nHence:\n\\[\n\\frac{d\\ell_i}{d\\beta_1} = (y_i - \\pi_i) x_i\n\\]\n\n\nThe Score Function\nSumming over all observations gives the score function, often notated \\(U\\):\n\\[\nU(\\beta_1) = \\frac{\\partial \\ell}{\\partial \\beta_1} = \\sum_{i=1}^n (y_i - \\pi_i) x_i\n\\]\nA similar expression holds for the intercept \\(\\beta_0\\), where \\(x_i = 1\\):\n\\[\nU(\\beta_0) = \\sum_{i=1}^n (y_i - \\pi_i)\n\\]"
  },
  {
    "objectID": "uncertainty.html#the-general-case",
    "href": "uncertainty.html#the-general-case",
    "title": "Some Topics in Logistic Regression",
    "section": "The General Case",
    "text": "The General Case\n\n\\(\\eta_i = \\mathbf{x}_i^\\top \\boldsymbol{\\beta}\\)\n\\(\\pi_i = \\frac{1}{1 + \\exp(-\\eta_i)}\\)\n\nThe score function in vector form for arbitrary \\(\\beta_k\\) coefficients is:\n\\[\n\\mathbf{U}(\\boldsymbol{\\beta}) = \\nabla_{\\boldsymbol{\\beta}} \\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^n (y_i - \\pi_i) \\mathbf{x}_i\n\\]\nOr in matrix form:\n\\[\n\\mathbf{U}(\\boldsymbol{\\beta}) = \\mathbf{X}^\\top (\\mathbf{y} - \\boldsymbol{\\pi})\n\\]\nwhere \\(\\mathbf{X} \\in \\mathbb{R}^{n \\times p}\\), and \\(\\boldsymbol{\\pi} \\in \\mathbb{R}^n\\) is the vector of fitted probabilities.\nWe can now represent the score function in code for our simulated dataset:\n\ncompute_score &lt;- function(beta0, beta1, data) {\n  x &lt;- data$x\n  y &lt;- data$y\n  \n  eta &lt;- beta0 + beta1 * x\n  pi &lt;- 1 / (1 + exp(-eta))  # predicted probabilities\n  \n  residual &lt;- y - pi  # (y_i - pi_i)\n  \n  score_0 &lt;- sum(residual)\n  score_1 &lt;- sum(residual * x)\n  \n  c(score_0, score_1)\n}\n\n# Example: compute the score at the true parameters\ncompute_score(beta0 = -0.5, beta1 = 2, data = sim_data)\n\n[1] -1.494431  3.035431\n\n\nHessian:\n\\[\nH(\\beta) = -X^T W X\n\\]\nwhere \\(W\\) is diagonal with elements \\(p_i(1 - p_i)\\).\n\nR code for Gradient and Hessian:\nX &lt;- cbind(1, x)\nbeta &lt;- c(0, 0)\neta &lt;- X %*% beta\np &lt;- 1 / (1 + exp(-eta))\ngradient &lt;- t(X) %*% (y - p)\nW &lt;- diag(as.vector(p * (1 - p)))\nhessian &lt;- -t(X) %*% W %*% X\n\n\n\n5. Newton-Raphson Algorithm\nIteration step:\n\\[\n\\beta_{\\text{new}} = \\beta_{\\text{old}} - [H(\\beta_{\\text{old}})]^{-1} \\nabla \\ell(\\beta_{\\text{old}})\n\\]\n\n\nR implementation:\nbeta &lt;- c(0, 0)\nfor (i in 1:10) {\n  eta &lt;- X %*% beta\n  p &lt;- 1 / (1 + exp(-eta))\n  gradient &lt;- t(X) %*% (y - p)\n  W &lt;- diag(as.vector(p * (1 - p)))\n  hessian &lt;- -t(X) %*% W %*% X\n  beta &lt;- beta - solve(hessian) %*% gradient\n}\nprint(beta)\n\n\n\n6. Variance of Estimates\nEstimated covariance matrix:\n\\[\n\\text{Var}(\\hat{\\beta}) = (X^T W X)^{-1}\n\\]\n\n\n\n7. Bootstrap Validation\nRepeated sampling with replacement to estimate standard errors.\n\n\nBootstrap R code:\nB &lt;- 500\nbootstrap_estimates &lt;- matrix(NA, B, 2)\n\nfor (b in 1:B) {\n  idx &lt;- sample(1:n, replace = TRUE)\n  X_b &lt;- X[idx, ]\n  y_b &lt;- y[idx]\n  beta_b &lt;- glm(y_b ~ X_b[,2], family=\"binomial\")$coefficients\n  bootstrap_estimates[b, ] &lt;- beta_b\n}\n\napply(bootstrap_estimates, 2, sd)\n\n\n\n8. Cross-validation\nPartition data into $K$ folds, train/test each fold.\n\n\nExample 5-fold CV in R:\nK &lt;- 5\nfolds &lt;- sample(rep(1:K, length.out = n))\ncv_error &lt;- numeric(K)\n\nfor (k in 1:K) {\n  train_idx &lt;- which(folds != k)\n  test_idx &lt;- which(folds == k)\n  fit &lt;- glm(y ~ x, family=\"binomial\", data=sim_data[train_idx,])\n  preds &lt;- predict(fit, sim_data[test_idx,], type=\"response\")\n  cv_error[k] &lt;- mean((sim_data$y[test_idx] - preds)^2)\n}\nmean(cv_error)\n\n\n\n9. Comparison of Bootstrap and Cross-validation\nDiscuss strengths/weaknesses:\n\nBootstrap gives parameter uncertainty.\nCV provides estimate of prediction error.\n\n\n\n\n10. Extensions and Practical Considerations\nDiscuss regularization and connections to real-world datasets, and further work."
  },
  {
    "objectID": "uncertainty.html#gradient",
    "href": "uncertainty.html#gradient",
    "title": "Some topics in LR",
    "section": "4. Gradient",
    "text": "4. Gradient\nTo find the maxima, we firstly need to know how the log likelihood function varies w.r.t its \\(\\beta\\) parameters This is just its first derivative, called the Score Function:\nThe likelihood function for the whole sample is:\n\\[\nL(\\boldsymbol{\\beta}) = \\prod_{i=1}^n \\pi_i^{y_i} (1 - \\pi_i)^{1 - y_i}\n\\]\nTaking logs gives the log-likelihood:\n\\[\n\\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^n \\left[ y_i \\log(\\pi_i) + (1 - y_i) \\log(1 - \\pi_i) \\right]\n\\]\n\nDifferentiate it\nTo find the derivative of \\(\\ell(\\boldsymbol{\\beta})\\) with respect to \\(\\beta_1\\), isolate a single observation\n\\[\n\\ell_i = y_i \\log(\\pi_i) + (1 - y_i) \\log(1 - \\pi_i)\n\\]\nand note that the desired derivative is the product of three simpler derivatives via the chain rule:\n\\[\n\\frac{d\\ell_i}{d\\beta_1}\n= \\frac{d\\ell_i}{d\\pi_i} \\cdot \\frac{d\\pi_i}{d\\eta_i} \\cdot \\frac{d\\eta_i}{d\\beta_1}\n\\]\nTheese derivatives evaluate as:\n\nDerivative with respect to \\(\\pi_i\\) follows from the derivative of logs:\n\\[\n\\frac{d\\ell_i}{d\\pi_i} = \\frac{y_i}{\\pi_i} - \\frac{1 - y_i}{1 - \\pi_i}\n\\]\nThis derivative of the logistic function follows when we notice that the direct derivative of \\(\\pi_i\\) is equivalent to the result of multiplying it by \\((1-\\pi_i)\\):\n\\[\n\\pi_i = \\frac{1}{1 + \\exp(-\\eta_i)} \\quad \\Rightarrow \\quad \\frac{d\\pi_i}{d\\eta_i} = \\pi_i (1 - \\pi_i)\n\\]\nThe derivative of the linear predictor is fairly trivial:\n\\[\n\\frac{d\\eta_i}{d\\beta_1} = x_i\n\\]\n\n\n\nMultiplying the three derivatives\nStart by substituting:\n\\[\n\\frac{d\\ell_i}{d\\beta_1}\n= \\left( \\frac{y_i}{\\pi_i} - \\frac{1 - y_i}{1 - \\pi_i} \\right) \\cdot \\pi_i (1 - \\pi_i) \\cdot x_i\n\\]\nExpand and simplify:\n\\[\n\\left( \\frac{y_i}{\\pi_i} - \\frac{1 - y_i}{1 - \\pi_i} \\right) \\cdot \\pi_i (1 - \\pi_i)\n= y_i (1 - \\pi_i) - (1 - y_i) \\pi_i = y_i - \\pi_i\n\\]\nHence:\n\\[\n\\frac{d\\ell_i}{d\\beta_1} = (y_i - \\pi_i) x_i\n\\]\n\n\nThe Score Function\nSumming over all observations gives the score function, often notated \\(U\\):\n\\[\nU(\\beta_1) = \\frac{\\partial \\ell}{\\partial \\beta_1} = \\sum_{i=1}^n (y_i - \\pi_i) x_i\n\\]\nA similar expression holds for the intercept:\n\\[\nU(\\beta_0) = \\sum_{i=1}^n (y_i - \\pi_i)\n\\]\n\n\nAnd in general…\nThe score function in vector form for arbitrary \\(\\beta_k\\) coefficients is sometimes called the Jacobian and can be expressed:\n\\[\n\\mathbf{U}(\\boldsymbol{\\beta}) = \\nabla_{\\boldsymbol{\\beta}} \\ell(\\boldsymbol{\\beta}) = \\sum_{i=1}^n (y_i - \\pi_i) \\mathbf{x}_i\n\\]\nOr in matrix form:\n\\[\n\\mathbf{U}(\\boldsymbol{\\beta}) = \\mathbf{X}^\\top (\\mathbf{y} - \\boldsymbol{\\pi}).\n\\]\nWe can now represent the score function in code for our simulated dataset:\n\ncompute_score &lt;- function(beta0, beta1, data) {\n  x &lt;- data$x\n  y &lt;- data$y\n  \n  eta &lt;- beta0 + beta1 * x\n  pi &lt;- 1 / (1 + exp(-eta))\n  residual &lt;- y - pi\n  \n  score_0 &lt;- sum(residual)\n  score_1 &lt;- sum(residual * x)\n  c(score_0, score_1)\n}\n\ncompute_score(beta0 = -0.5, beta1 = 2, data = sim_data)\n\n[1] -12.47272  14.12208\n\n\n\n# Compare to GLM Hessian\nlibrary(numDeriv)\n\nmod &lt;- glm(y ~ x, family = \"binomial\", data =  sim_data)\n\nLL &lt;- function(beta, X, y) {\n  eta &lt;- X %*% beta\n  p &lt;- 1 / (1 + exp(-eta))\n  sum(y * log(p) + (1 - y) * log(1 - p))\n}\n\nX &lt;- model.matrix(mod)\ny &lt;- mod$y\nest &lt;- coef(mod)\n\nscore_vec &lt;- grad(LL, est, X = X, y = y)\nhessian_mat &lt;- hessian(LL, est, X = X, y = y)\nprint(score_vec)\n\n[1] -7.830443e-08  2.275174e-07"
  },
  {
    "objectID": "uncertainty.html#the-hessian",
    "href": "uncertainty.html#the-hessian",
    "title": "Some topics in LR",
    "section": "5. The Hessian",
    "text": "5. The Hessian\nTo understand the curvature of the log-likelihood function near its stationary points, we need need the second derivatives.\n\nDeriving the Hessian\nBegin with the score function:\n\\[\n\\frac{\\partial \\ell}{\\partial \\beta_j} = \\sum_{i=1}^n (y_i - \\pi_i) x_{ij}\n\\]\nOur goal is to differentiate this expression with respect to another parameter (\\(\\beta_k\\)) to obtain the Hessian:\n\\[\n\\frac{\\partial^2 \\ell}{\\partial \\beta_j \\partial \\beta_k}\n\\]\nWe are interested in the derivative, which can be expressed as\n\\[\n\\frac{d}{d\\beta_k} \\left[ \\sum_{i=1}^n (y_i - \\pi_i) x_{ij} \\right]\n= \\sum_{i=1}^n \\left( \\frac{d}{d\\beta_k} (-\\pi_i) \\cdot x_{ij} \\right)\n\\]\nsince \\(y_i\\) is observed data not dependent on \\(\\beta_k\\), its derivative is zero.\nDerivative of \\(\\pi_i\\):\nCompute the derivative using the chain rule:\n\nFirst, we have the derivative of the logistic function:\n\n\\[\n\\frac{d\\pi_i}{d\\eta_i} = \\pi_i (1 - \\pi_i)\n\\]\n\nDerivative of \\(\\eta_i\\) with respect to \\(\\beta_k\\)\n\nSince:\n\\[\n\\eta_i = \\sum_{l=0}^p \\beta_l x_{il} = \\beta_0 + \\beta_1 x_{i1} + ... + \\beta_k x_{ik} + ... + \\beta_p x_{ip}\n\\]\nany term not involving \\(\\beta_k\\) differentiates to \\(0\\) and the power of \\(\\beta_k\\) goes to \\(0\\) also, leaving:\n\\[\n\\frac{d\\eta_i}{d\\beta_k} = x_{ik}\n\\]\nChaining them together:\n\\[\n\\frac{d\\pi_i}{d\\beta_k} = \\pi_i (1 - \\pi_i) x_{ik}\n\\]\nSubstitute back into the Hessian:\nWe can now substitute this results back into the second derivative of the log-likelihood:\n\\[\n\\frac{\\partial^2 \\ell}{\\partial \\beta_j \\partial \\beta_k}\n= - \\sum_{i=1}^n \\pi_i (1 - \\pi_i) x_{ij} x_{ik}\n\\]\nThis gives an entry of the Hessian matrix.\n\n\nHessian in Matrix Form\nIf we have two parrameters, \\(\\beta_0\\) and \\(\\beta_1\\), and \\(x_{i0} = 1\\), \\(x_{i1} = x_i\\), then the Hessian is:\n\\[\n\\mathbf{H} =\n- \\sum_{i=1}^n \\pi_i (1 - \\pi_i)\n\\begin{bmatrix}\n1 & x_i \\\\\nx_i & x_i^2\n\\end{bmatrix}\n\\]\n\n\nAnd in general…\nIn arbitrary parameters, Hessian matrix is expressed in matrix form\n\\[\n\\mathbf{H}(\\boldsymbol{\\beta}) = \\frac{\\partial^2 \\ell}{\\partial \\boldsymbol{\\beta} \\partial \\boldsymbol{\\beta}^\\top}\n= - \\mathbf{X}^\\top \\mathbf{W} \\mathbf{X}\n\\] where \\(W\\) is diagonal with elements \\(p_i(1 - p_i)\\).\nWe can now represent the Hessian in code:\n\ncompute_hessian &lt;- function(beta0, beta1, data) {\n  x &lt;- data$x\n  y &lt;- data$y\n  n &lt;- length(x)\n  \n  eta &lt;- beta0 + beta1 * x\n  pi &lt;- 1 / (1 + exp(-eta))\n  w &lt;- pi * (1 - pi)  # weights\n  \n  H &lt;- matrix(0, nrow = 2, ncol = 2)\n  \n  for (i in 1:n) {\n    xi_vec &lt;- c(1, x[i])\n    outer_prod &lt;- tcrossprod(xi_vec)\n    H &lt;- H + w[i] * outer_prod\n  }\n  \n  return(-H)\n}\n\ncompute_hessian(beta0 = -0.5, beta1 = 2, sim_data)\n\n           [,1]      [,2]\n[1,] -148.51726 -21.47695\n[2,]  -21.47695 -63.38486\n\n\n\nprint(hessian_mat)\n\n           [,1]      [,2]\n[1,] -134.77535 -24.15604\n[2,]  -24.15604 -51.21245\n\n\nThe determinant \\(\\det(H)\\) here is positive, meaning that we have a maximum or a minimum. The negative sign on entry \\((1,1)\\) indicates a maximum.\nComparison with the GLM() output reveals similarity with the manually calculated values, but not exact equality. Would we expect an exact match?"
  },
  {
    "objectID": "uncertainty.html#newton-raphson-algorithm",
    "href": "uncertainty.html#newton-raphson-algorithm",
    "title": "Some topics in LR",
    "section": "6. Newton-Raphson Algorithm",
    "text": "6. Newton-Raphson Algorithm\nWe are interested in solving for the values of \\(\\boldsymbol{\\beta}\\) that maximise the (log-)likelihood function. Numerically, we shall invoke an iterative procedure with iteration step\n\\[\n\\boldsymbol{\\beta_{t+1}} = \\boldsymbol{\\beta_t} - \\boldsymbol{H^{-1}} \\boldsymbol{U}\n\\] where U is the score vector (Jacobian) and H is the Hessian.\n\nR implementation:\n\n# LL = log-likelihood\nLL &lt;- function(beta, x, y) {\n  eta &lt;- beta[1] + beta[2] * x\n  pi &lt;- 1 / (1 + exp(-eta))\n  sum(y * log(pi) + (1 - y) * log(1 - pi))\n}\n\n\nNR &lt;- function(data, beta_init = c(0, 0), max_iter = 100, tol = 1e-6) {\n  beta &lt;- beta_init\n  x &lt;- data$x\n  y &lt;- data$y\n  \n  LL_vals &lt;- numeric()\n  \n  for (iter in 1:max_iter) {\n    eta &lt;- beta[1] + beta[2] * x\n    pi &lt;- 1 / (1 + exp(-eta))\n    residual &lt;- y - pi\n    score &lt;- c(sum(residual), sum(residual * x))\n    H &lt;- compute_hessian(beta[1], beta[2], data)\n    \n    delta &lt;- solve(H, score)\n    beta_new &lt;- beta - delta\n    LL_vals[iter] &lt;- LL(beta_new, x, y)\n    \n    # Euclidean loss function\n    if (sqrt(sum((beta_new - beta)^2)) &lt; tol) {\n      #message(sprintf(\"Converged in %d iterations.\", iter))\n      break\n    }\n    \n    beta &lt;- beta_new\n  }\n  \n  return(list(beta = beta, H = H, LL = LL_vals))\n}\n\n\nout &lt;- NR(sim_data)\nplot(out$LL, type = \"b\" , main = \"Log-Likelihood over NR iterations\", xlab = \"Iteration\", ylab = \"Log Likelihood (LL)\")\n\n\n\n\n\n\n\n\n\nout &lt;- NR(sim_data)\nbeta_hat &lt;- out$beta\nH &lt;- out$H\nprint(beta_hat)\n\n[1] -0.6372742  2.3030549\n\nprint(H)\n\n           [,1]      [,2]\n[1,] -134.77537 -24.15604\n[2,]  -24.15604 -51.21247\n\n\n\n# Compare to GLM\nmod &lt;- glm(y ~ x, family = binomial, data = sim_data)\nmod$coefficients\n\n(Intercept)           x \n -0.6372743   2.3030552"
  },
  {
    "objectID": "uncertainty.html#variance-of-estimates",
    "href": "uncertainty.html#variance-of-estimates",
    "title": "Some topics in LR",
    "section": "7. Variance of Estimates",
    "text": "7. Variance of Estimates\nLet’s now derive the variance of the maximum likelihood estimate (MLE) \\(\\hat{\\boldsymbol{\\beta}}\\) in logistic regression.\n\nTaylor Expansion\nAt the MLE, the score function (gradient of the log-likelihood) is zero:\n\\[\n\\mathbf{U}(\\hat{\\boldsymbol{\\beta}}) = \\frac{\\partial \\ell}{\\partial \\boldsymbol{\\beta}} \\bigg|_{\\hat{\\boldsymbol{\\beta}}} = 0\n\\]\nWe can expand the score function using a first-order Taylor approximation around the true value \\(\\boldsymbol{\\beta}\\):\n\\[\n\\mathbf{U}(\\hat{\\boldsymbol{\\beta}}) \\approx \\mathbf{U}(\\boldsymbol{\\beta}) + \\mathbf{H}(\\boldsymbol{\\beta}) (\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})\n\\]\nSetting the left-hand side to 0\n\\[\n0 \\approx \\mathbf{U}(\\boldsymbol{\\beta}) + \\mathbf{H}(\\boldsymbol{\\beta}) (\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta})\n\\]\nand rearranging to\n\\[\n\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta} \\approx - \\mathbf{H}^{-1}(\\boldsymbol{\\beta}) \\, \\mathbf{U}(\\boldsymbol{\\beta})\n\\]\nlets us take the variance of both sides:\n\\[\n\\operatorname{Var}(\\hat{\\boldsymbol{\\beta}}) \\approx \\mathbf{H}^{-1} \\operatorname{Var}(\\mathbf{U}(\\boldsymbol{\\beta})) \\left(\\mathbf{H}^{-1}\\right)^\\top\n\\]\nBy the information equality, the variance of the score function equals the Fisher Information:\n\\[\n\\operatorname{Var}(\\mathbf{U}(\\boldsymbol{\\beta})) = \\mathcal{I}(\\boldsymbol{\\beta}) = - \\mathbb{E}[\\mathbf{H}(\\boldsymbol{\\beta})]\n\\]\nSo:\n\\[\n\\operatorname{Var}(\\hat{\\boldsymbol{\\beta}}) \\approx \\mathbf{H}^{-1} \\mathcal{I} \\left( \\mathbf{H}^{-1} \\right)^\\top = \\mathcal{I}^{-1}\n\\]\n\n\nApplication\nIn logistic regression, the Fisher Information (or negative Hessian) at \\(\\hat{\\boldsymbol{\\beta}}\\) is:\n\\[\n\\mathcal{I}(\\hat{\\boldsymbol{\\beta}}) = \\mathbf{X}^\\top \\mathbf{W} \\mathbf{X}\n\\]\nWhere \\(\\mathbf{W}\\) is a diagonal matrix with entries \\(\\pi_i (1 - \\pi_i)\\).\nSo, the variance–covariance matrix of \\(\\hat{\\boldsymbol{\\beta}}\\) is:\n\\[\n\\operatorname{Var}(\\hat{\\boldsymbol{\\beta}}) = \\left( \\mathbf{X}^\\top \\mathbf{W} \\mathbf{X} \\right)^{-1}\n\\]\nThis is the matrix used to compute the standard errors and hence the Wald confidence intervals.\n\n\nTwo Parameters\nWhen we have only two parameters, \\(\\beta_0\\) and \\(\\beta_1\\), the Fisher Information is:\n\\[\n\\mathcal{I} = \\sum_{i=1}^n \\pi_i (1 - \\pi_i)\n\\begin{bmatrix}\n1 & x_i \\\\\nx_i & x_i^2\n\\end{bmatrix}\n\\]\nThen:\n\\[\n\\operatorname{Var}(\\hat{\\boldsymbol{\\beta}}) = \\left[ \\sum_{i=1}^n \\pi_i (1 - \\pi_i)\n\\begin{bmatrix}\n1 & x_i \\\\\nx_i & x_i^2\n\\end{bmatrix} \\right]^{-1}\n\\]\nThe diagonal elements give \\(\\operatorname{Var}(\\hat{\\beta}_0)\\) and \\(\\operatorname{Var}(\\hat{\\beta}_1)\\), respectively. Taking square roots gives their standard errors.\n\nvcov_mat &lt;- solve(-H)\nse &lt;- sqrt(diag(vcov_mat))\nz &lt;- qnorm(0.975)\nci &lt;- cbind(\n  Estimate = beta_hat,\n  SE = se,\n  Lower = beta_hat - z * se,\n  Upper = beta_hat + z * se\n)\nprint(round(vcov_mat, 4))\n\n        [,1]    [,2]\n[1,]  0.0081 -0.0038\n[2,] -0.0038  0.0213\n\n\n\nrownames(ci) &lt;- c(\"beta_0\", \"beta_1\")\nprint(round(ci, 4))\n\n       Estimate    SE   Lower   Upper\nbeta_0  -0.6373 0.090 -0.8137 -0.4608\nbeta_1   2.3031 0.146  2.0168  2.5893"
  },
  {
    "objectID": "uncertainty.html#estimate-variances-by-the-bootstrap-method",
    "href": "uncertainty.html#estimate-variances-by-the-bootstrap-method",
    "title": "Some Topics in Logistic Regression",
    "section": "8. Estimate Variances by the Bootstrap Method",
    "text": "8. Estimate Variances by the Bootstrap Method\n\n# Bootstrap function\nbootstrap &lt;- function(data, B) {\n  n &lt;- nrow(data)\n  beta_boot &lt;- matrix(NA, nrow = B, ncol = 2)  # Store (beta_0, beta_1)\n  \n  for (b in 1:B) {\n    # Sample with replacement\n    idx &lt;- sample(1:n, size = n, replace = TRUE)\n    boot_data &lt;- data[idx, ]\n    out &lt;- NR(boot_data)\n    beta_boot[b, ] &lt;- out$beta\n  }\n  \n  # Remove rows with NAs from failed attempts\n  beta_boot &lt;- beta_boot[complete.cases(beta_boot), ]\n  colnames(beta_boot) &lt;- c(\"beta_0\", \"beta_1\")\n  \n  return(beta_boot)\n}\n\n\nboot_estimates &lt;- bootstrap(sim_data, 1000)\n\n# Compute bootstrap variances\nbeta_boot &lt;- colMeans(boot_estimates)\nboot_var &lt;- apply(boot_estimates, 2, var)\nboot_se &lt;- sqrt(boot_var)\n\n# 95% percentile intervals (optional)\nboot_ci &lt;- apply(boot_estimates, 2, quantile, probs = c(0.025, 0.975))\n\n# Print results\nround(cbind(Estimate = beta_boot, Variance = boot_var, SE = boot_se, t(boot_ci)), 4)\n\n       Estimate Variance     SE   2.5%   97.5%\nbeta_0  -0.7889   0.1149 0.3390 -1.545 -0.1848\nbeta_1   3.0256   0.6575 0.8109  1.862  4.9777\n\n\nThe results are technically consistent with the above, but the CIs only just include the true values. Unclear whether this is due to a programming error, unsufficient sample size, or something else. ————————————————————————"
  },
  {
    "objectID": "uncertainty.html#bootstrap-validation",
    "href": "uncertainty.html#bootstrap-validation",
    "title": "Some topics in LR",
    "section": "9. Bootstrap Validation",
    "text": "9. Bootstrap Validation\nRepeated sampling with replacement to estimate standard errors.\nBootstrap in R:\n\nlibrary(pROC)\nbootstrap_validate_auc &lt;- function(data, B = 200) {\n  n &lt;- nrow(data)\n  x &lt;- data$x\n  y &lt;- data$y\n  optimism &lt;- numeric(B)\n  \n  for (b in 1:B) {\n    idx &lt;- sample(1:n, size = n, replace = TRUE)\n    data_boot &lt;- data[idx, ]\n    \n    fit_boot &lt;- NR(data_boot)\n    beta_boot &lt;- fit_boot$beta\n    \n    x_boot &lt;- data_boot$x\n    y_boot &lt;- data_boot$y\n    eta_boot &lt;- beta_boot[1] + beta_boot[2] * x_boot\n    p_boot &lt;- 1 / (1 + exp(-eta_boot))\n    auc_boot &lt;- auc(y_boot, p_boot, quiet = TRUE) # suppressMessages removed\n    \n    # AUC on original data\n    eta_test &lt;- beta_boot[1] + beta_boot[2] * x\n    p_test &lt;- 1 / (1 + exp(-eta_test))\n    auc_test &lt;- auc(y, p_test, quiet = TRUE)\n    \n    optimism[b] &lt;- auc_boot - auc_test\n  }\n  \n  avg_optimism &lt;- mean(optimism)\n  \n  # Fit model on full data\n  final_fit &lt;- NR(data)\n  beta_final &lt;- final_fit$beta\n  eta_final &lt;- beta_final[1] + beta_final[2] * x\n  p_final &lt;- 1 / (1 + exp(-eta_final))\n  auc_app_final &lt;- auc(y, p_final, quiet = TRUE)\n  \n  validated_auc &lt;- auc_app_final - avg_optimism\n  \n  return(list(\n    apparent = as.numeric(auc_app_final),\n    optimism = avg_optimism,\n    validated = validated_auc\n    )\n  )\n}\n\n\nvalidation_auc &lt;- bootstrap_validate_auc(sim_data)\nprint(validation_auc)\n\n$apparent\n[1] 0.8847723\n\n$optimism\n[1] -0.0006985766\n\n$validated\n[1] 0.8854709\n\n\nCompare the results of this with Harrell’s lrm() and validate() methods from RMS.\n\nlibrary(rms)\ndd = datadist(sim_data)\noptions(datadist = \"dd\")\nfit &lt;- lrm(y ~ x, data = sim_data, x=TRUE, y=TRUE)\nprint(fit)\n\nLogistic Regression Model\n\nlrm(formula = y ~ x, data = sim_data, x = TRUE, y = TRUE)\n\n                       Model Likelihood      Discrimination    Rank Discrim.    \n                             Ratio Test             Indexes          Indexes    \nObs          1000    LR chi2     514.96      R2       0.543    C       0.885    \n 0            595    d.f.             1     R2(1,1000)0.402    Dxy     0.770    \n 1            405    Pr(&gt; chi2) &lt;0.0001    R2(1,722.9)0.509    gamma   0.770    \nmax |deriv| 9e-06                            Brier    0.134    tau-a   0.371    \n\n          Coef    S.E.   Wald Z Pr(&gt;|Z|)\nIntercept -0.6373 0.0900 -7.08  &lt;0.0001 \nx          2.3031 0.1460 15.77  &lt;0.0001 \n\n\n\nval &lt;- validate(fit, method=\"boot\", B=200)\nval\n\n          index.orig training   test optimism index.corrected   n\nDxy           0.7695   0.7675 0.7695  -0.0021          0.7716 200\nR2            0.5433   0.5405 0.5433  -0.0028          0.5461 200\nIntercept     0.0000   0.0000 0.0103  -0.0103          0.0103 200\nSlope         1.0000   1.0000 1.0053  -0.0053          1.0053 200\nEmax          0.0000   0.0000 0.0031   0.0031          0.0031 200\nD             0.5140   0.5103 0.5140  -0.0036          0.5176 200\nU            -0.0020  -0.0020 0.0002  -0.0022          0.0002 200\nQ             0.5160   0.5123 0.5137  -0.0014          0.5174 200\nB             0.1339   0.1340 0.1342  -0.0002          0.1341 200\ng             2.5537   2.5431 2.5537  -0.0106          2.5643 200\ngp            0.3705   0.3683 0.3705  -0.0022          0.3728 200\n\n\nStrangely, Harrell’s validate() function does not report AUC directly; one has to calculate it manually from the reported Dxy (Somers’ D) result:\n\nauc &lt;- (val[\"Dxy\", \"index.corrected\"] + 1) / 2\nprint(auc)\n\n[1] 0.8858172\n\n\nThis is a tiny bit higher than our manually calculated C-statistic, but nevertheless very close.\nWe can visualise calibration also:\n\ncal &lt;- calibrate(fit, method = \"boot\", B = 200)\nplot(cal)\n\n\n\n\n\n\n\n\n\nn=1000   Mean absolute error=0.004   Mean squared error=2e-05\n0.9 Quantile of absolute error=0.007\n\n\nWhile the fit()-reported C-statistic seems to comport well with the manually bootstrapped result, the validate() method doesn’t report optimism in this directly – only on Dxy. This is because Harrell believes rank-correlation theory is a better grounding for discrimination. The two are related, however, by:\n\\[\nD_{xy} = 2 \\cdot (C - \\frac{1}{2})\n\\]\nso that\n\\[\n\\text{C-statistic (AUC)} = \\frac{D_{xy} + 1}{2}\n\\]\nArguably, this accounts well for the problem with very low AUC values: an AUC of 0.1 is as surprising as an AUC of 0.9 in some way; even though the model does not predict correctly, it fails to do so in a systematic way. If what we seek to avoid is the absence of systematicity, where AUC=0.5, then Somers’ D is probably preferable."
  },
  {
    "objectID": "uncertainty.html#cross-validation",
    "href": "uncertainty.html#cross-validation",
    "title": "Some topics in LR",
    "section": "10. Cross-validation",
    "text": "10. Cross-validation\nLastly, we can validate by partitioning our data into \\(K\\) folds, train/test each fold and measure average performance.\n\n5-fold CV in R:\n\ncross_validate &lt;- function(data, K = 5, metric = \"auc\") {\n  n &lt;- nrow(data)\n  partition &lt;- sample(rep(1:K, length.out = n))\n  perf &lt;- numeric(K)\n  \n  for (k in 1:K) {\n    train &lt;- data[partition != k, ]\n    test  &lt;- data[partition == k, ]\n    \n    model &lt;- glm(y ~ x, data = train, family = binomial)\n    p &lt;- predict(model, newdata = test)\n    y &lt;- test$y\n    perf[k] &lt;- suppressMessages(auc(y, p))\n  }\n  \n  list(per_fold = perf, mean = mean(perf))\n}\n\n\ncv_auc &lt;- cross_validate(sim_data, K = 5, metric = \"auc\")\nround(cv_auc$mean, 4)\n\n[1] 0.8843"
  },
  {
    "objectID": "uncertainty.html#estimate-variances-by-bootstrapping",
    "href": "uncertainty.html#estimate-variances-by-bootstrapping",
    "title": "Some topics in LR",
    "section": "8. Estimate Variances by Bootstrapping",
    "text": "8. Estimate Variances by Bootstrapping\n\nbootstrap &lt;- function(data, B) {\n  n &lt;- nrow(data)\n  beta_boot &lt;- matrix(NA, nrow = B, ncol = 2)  # Store for (beta_0, beta_1)\n  \n  for (b in 1:B) {\n    idx &lt;- sample(1:n, size = n, replace = TRUE)\n    boot_data &lt;- data[idx, ]\n    # Use GLM() instead of NR()\n    out &lt;- glm(y ~ x, family = \"binomial\", data = boot_data)\n    beta_boot[b, ] &lt;- out$coefficients\n  }\n  \n  colnames(beta_boot) &lt;- c(\"beta_0\", \"beta_1\")\n  return(beta_boot)\n}\n\n\nboot_estimates &lt;- bootstrap(sim_data, 1000)\n\nbeta_boot &lt;- colMeans(boot_estimates)\nboot_var &lt;- apply(boot_estimates, 2, var)\nboot_se &lt;- sqrt(boot_var)\nboot_ci &lt;- apply(boot_estimates, 2, quantile, probs = c(0.025, 0.975))\nround(cbind(Estimate = beta_boot, Variance = boot_var, SE = boot_se, t(boot_ci)), 4)\n\n       Estimate Variance     SE    2.5%   97.5%\nbeta_0  -0.6377   0.0086 0.0925 -0.8251 -0.4671\nbeta_1   2.3134   0.0226 0.1503  2.0365  2.6142\n\n\nThe results are consistent with the above, but the CIs only just include the true values. Unclear whether this is due to a programming error, insufficient sample size, or something else."
  },
  {
    "objectID": "bootstrap.html",
    "href": "bootstrap.html",
    "title": "Bootstrapping & Sample Sizes",
    "section": "",
    "text": "Load required libraries:\nlibrary(rms)\nlibrary(tidyverse)\nlibrary(pROC)\nlibrary(ggplot2)\nOn the previous page, we ran through some manual bootstrapping for the purposes of showing their in-principle operation. There we used just a single predictor and a relatively small sample size.\nNow let’s try to develop a bigger bootstrap validation procedure with a bigger sample size and more covariates. Again, start by generating some sample data, this time with \\(\\beta_0=-1.5\\) and predictor effects \\(\\beta_k = 0.7\\) for \\(1 \\leq k \\leq 10\\).\nset.seed(11235)\nk &lt;- 10\nn &lt;- 100000\nbeta_0 &lt;- -1.5\nbeta &lt;- rep(0.7, k)\n\nsimulate_data &lt;- function(k, n) {\n  X &lt;- matrix(rnorm(n*k), nrow = n, ncol = k)\n  colnames(X) &lt;- paste0(\"x\", 1:k)\n  eta &lt;- beta_0 + X%*%beta\n  p &lt;- 1 / (1 + exp(-eta))\n  y &lt;- rbinom(n, size = 1, prob = p)\n  return(\n    list(\n      df = data.frame(X, y = y),\n      eta = eta,\n      p = p\n    )\n  )\n}\n\ndgp_data &lt;- simulate_data(k, n)\ndf &lt;- dgp_data$df\nprint(paste0(\"Hence a prevalence of \", round(100 * mean(df$y), 2), \"%\"))\n\n[1] \"Hence a prevalence of 29.39%\"\n\nglm(y ~ ., data = df, family = \"binomial\")\n\n\nCall:  glm(formula = y ~ ., family = \"binomial\", data = df)\n\nCoefficients:\n(Intercept)           x1           x2           x3           x4           x5  \n    -1.5203       0.6942       0.6887       0.6969       0.7059       0.7101  \n         x6           x7           x8           x9          x10  \n     0.7114       0.7021       0.6996       0.7037       0.7062  \n\nDegrees of Freedom: 99999 Total (i.e. Null);  99989 Residual\nNull Deviance:      121100 \nResidual Deviance: 76390    AIC: 76420\nVisualise:\nhist(dgp_data$eta, breaks = 100, col = \"lightblue\", main = \"Distribution of Linear Predictor\",\n     xlab = expression(eta))#, freq = FALSE)\nhist(dgp_data$p, breaks = 100, main = \"Distribution of Probabilities\",\n     xlab = \"Theoretical Probability\")#, freq = FALSE)\nplot_data &lt;- cbind(dgp_data$df, eta = dgp_data$eta, p = dgp_data$p)\nggplot(plot_data, aes(x = eta, y = p)) +\n  geom_line(stat = \"function\", fun = plogis, linewidth = 1) +\n  labs(x = \"Linear Predictor (η)\", y = \"Probability (p)\", title = \"Logistic Function: p = 1 / (1 + exp(-η))\") +\n  theme_minimal()"
  },
  {
    "objectID": "bootstrap.html#create-dataframe-and-model",
    "href": "bootstrap.html#create-dataframe-and-model",
    "title": "Bootstrapping & Sample Sizes",
    "section": "Create dataframe and model",
    "text": "Create dataframe and model\n\ndd &lt;- datadist(df)\noptions(datadist = \"dd\")\nfit &lt;- lrm(y ~ ., data = df, x=TRUE,y=TRUE)\nprint(fit)\n\nLogistic Regression Model\n\nlrm(formula = y ~ ., data = df, x = TRUE, y = TRUE)\n\n                        Model Likelihood         Discrimination    Rank Discrim.    \n                              Ratio Test                Indexes          Indexes    \nObs         1e+05    LR chi2    44717.76         R2       0.514    C       0.882    \n 0          70615    d.f.             10      R2(10,1e+05)0.361    Dxy     0.764    \n 1          29385    Pr(&gt; chi2)  &lt;0.0001    R2(10,62250.7)0.512    gamma   0.764    \nmax |deriv| 0.001                                Brier    0.122    tau-a   0.317    \n\n          Coef    S.E.   Wald Z  Pr(&gt;|Z|)\nIntercept -1.5203 0.0112 -136.06 &lt;0.0001 \nx1         0.6942 0.0098   70.90 &lt;0.0001 \nx2         0.6887 0.0097   70.80 &lt;0.0001 \nx3         0.6969 0.0098   71.19 &lt;0.0001 \nx4         0.7059 0.0099   71.56 &lt;0.0001 \nx5         0.7101 0.0098   72.47 &lt;0.0001 \nx6         0.7114 0.0098   72.46 &lt;0.0001 \nx7         0.7021 0.0099   71.25 &lt;0.0001 \nx8         0.6996 0.0097   71.78 &lt;0.0001 \nx9         0.7037 0.0098   71.69 &lt;0.0001 \nx10        0.7062 0.0098   72.11 &lt;0.0001 \n\n\nOdd ratios:\n\nexp(fit$coefficients)\n\nIntercept        x1        x2        x3        x4        x5        x6        x7 \n0.2186371 2.0021575 1.9911487 2.0075936 2.0255935 2.0342269 2.0367993 2.0179830 \n       x8        x9       x10 \n2.0129725 2.0213141 2.0263181"
  },
  {
    "objectID": "bootstrap.html#initial-model-validation",
    "href": "bootstrap.html#initial-model-validation",
    "title": "Bootstrapping & Sample Sizes",
    "section": "Initial Model Validation",
    "text": "Initial Model Validation\n\nval &lt;- validate(fit, method = \"boot\", B = 200)\nval\n\n          index.orig training    test optimism index.corrected   n\nDxy           0.7641   0.7640  0.7640    0e+00          0.7641 200\nR2            0.5135   0.5135  0.5134    0e+00          0.5135 200\nIntercept     0.0000   0.0000 -0.0002    2e-04         -0.0002 200\nSlope         1.0000   1.0000  0.9999    1e-04          0.9999 200\nEmax          0.0000   0.0000  0.0001    1e-04          0.0001 200\nD             0.4472   0.4471  0.4471    0e+00          0.4471 200\nU             0.0000   0.0000  0.0000    0e+00          0.0000 200\nQ             0.4472   0.4471  0.4471    1e-04          0.4471 200\nB             0.1222   0.1222  0.1222    0e+00          0.1222 200\ng             2.5022   2.5022  2.5017    5e-04          2.5018 200\ngp            0.3172   0.3172  0.3172    0e+00          0.3173 200\n\n\n\nauc &lt;- (val[\"Dxy\", \"index.corrected\"] + 1) / 2\nprint(auc)\n\n[1] 0.8820359"
  },
  {
    "objectID": "bootstrap.html#sample-size",
    "href": "bootstrap.html#sample-size",
    "title": "Bootstrapping & Sample Sizes",
    "section": "Sample Size",
    "text": "Sample Size\nI would like to understand the effect of sample size on model performance. Here I focus on two methods of model validation – Cross Validation (10-fold) and Bootstrapping – and applying them to the model formulation given above on variable sample sizes.\nLet’s start by setting up the basics:\n\ndf &lt;- dgp_data$df\n\nsample_sizes &lt;- c(100, 200, 300, 500, 1000, 5000, 10000, 50000)\nB &lt;- 200\nperformance_metrics &lt;- c('AUC', 'calibration_slope')\n\nSo we have a vector of sample sizes, a number of bootstrap repetitions, and we would like to use 2 performance metrics (AUC, Calibration slope) for understand the impact of sample size on model performance.\nHence we need to calculate 5 * 200 * 2 = 2000 data points, stored in a 4D data structure.\nNext let’s set up a nested-list data structure to hold the results.\n\nrm(results) # if needed\n\nWarning in rm(results): object 'results' not found\n\n\n\nresults &lt;- list()\nfor (s in sample_sizes) {\n  results[[as.character(s)]] &lt;- list(auc = numeric(B), slope = numeric(B))\n}\nfail_counts &lt;- setNames(rep(0, length(sample_sizes)), sample_sizes)\n\nEach of these datapoints will be populated with either an AUC value or a calibration slope. Let’s start with a simple function that will allow us to calculate a single result-pair. Later we will wrap this inside a double-loop to populate the results list fully.\n\nsample300 &lt;- sample_n(df, 300, replace = FALSE) #for testing\n\nperformance &lt;- function(dev_data) {\n  bootstrap_data &lt;- sample_n(dev_data, nrow(dev_data), replace = TRUE)\n  mod &lt;- suppressWarnings(glm(y ~ ., family = \"binomial\", data = bootstrap_data))\n  if (inherits(mod, \"try-error\") || !mod$converged) return(NULL)\n  \n  pred_dev_reponse &lt;- predict(mod, newdata = dev_data, type = \"response\")\n  auc &lt;- as.numeric(roc(dev_data$y, pred_dev_reponse, quiet = TRUE)$auc)\n  \n  # calibration slope\n  pred_dev_link &lt;- predict(mod, newdata = dev_data, type = \"link\")\n  calibration_model &lt;- suppressWarnings(glm(dev_data$y ~ pred_dev_link, family = \"binomial\"))\n  slope &lt;- as.numeric(coef(calibration_model))[2]\n  \n  return(\n    list(\n      auc = auc,\n      slope = slope\n      )\n  )\n}\n\nres &lt;- performance(sample300)\nres\n\n$auc\n[1] 0.8817003\n\n$slope\n[1] 0.9121992\n\n\nNext, let’s check that we can successfully populate our results list with just these two values in the 300 sample-size block.\n\nresults[[\"300\"]][[\"auc\"]][[1]] &lt;- res$auc\nresults[[\"300\"]][[\"slope\"]][[1]] &lt;- res$slope\nresults\n\n$`100`\n$`100`$auc\n  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[112] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[149] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[186] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n\n$`100`$slope\n  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[112] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[149] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[186] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n\n\n$`200`\n$`200`$auc\n  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[112] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[149] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[186] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n\n$`200`$slope\n  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[112] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[149] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[186] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n\n\n$`300`\n$`300`$auc\n  [1] 0.8817003 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n  [8] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n [15] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n [22] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n [29] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n [36] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n [43] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n [50] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n [57] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n [64] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n [71] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n [78] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n [85] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n [92] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n [99] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n[106] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n[113] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n[120] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n[127] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n[134] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n[141] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n[148] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n[155] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n[162] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n[169] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n[176] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n[183] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n[190] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n[197] 0.0000000 0.0000000 0.0000000 0.0000000\n\n$`300`$slope\n  [1] 0.9121992 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n  [8] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n [15] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n [22] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n [29] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n [36] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n [43] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n [50] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n [57] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n [64] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n [71] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n [78] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n [85] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n [92] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n [99] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n[106] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n[113] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n[120] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n[127] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n[134] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n[141] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n[148] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n[155] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n[162] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n[169] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n[176] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n[183] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n[190] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n[197] 0.0000000 0.0000000 0.0000000 0.0000000\n\n\n$`500`\n$`500`$auc\n  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[112] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[149] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[186] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n\n$`500`$slope\n  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[112] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[149] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[186] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n\n\n$`1000`\n$`1000`$auc\n  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[112] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[149] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[186] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n\n$`1000`$slope\n  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[112] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[149] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[186] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n\n\n$`5000`\n$`5000`$auc\n  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[112] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[149] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[186] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n\n$`5000`$slope\n  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[112] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[149] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[186] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n\n\n$`10000`\n$`10000`$auc\n  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[112] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[149] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[186] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n\n$`10000`$slope\n  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[112] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[149] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[186] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n\n\n$`50000`\n$`50000`$auc\n  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[112] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[149] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[186] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n\n$`50000`$slope\n  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[112] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[149] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n[186] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n\n\nThen take the inner loop and draw B bootstrap samples and run the model on each, reporting AUC and calibration-slope for a single sample size; say 300. Again, wrap it in a function, and this time we will just fill up the 300 block of the result nested list.\n\nsample300 &lt;- sample_n(df, 300, replace = FALSE)  # for testing purposes\n\nfor (b in 1:200) {\n  res &lt;- performance(sample300)\n  results[[\"300\"]] [[\"auc\"]] [b] &lt;- res$auc\n  results[[\"300\"]] [[\"slope\"]] [b] &lt;- res$slope\n}\n\n\nresults[[\"300\"]]\n\n$auc\n  [1] 0.8841259 0.8787475 0.8826881 0.8786943 0.8765110 0.8783748 0.8827414\n  [8] 0.8780553 0.8825284 0.8677246 0.8842856 0.8821556 0.8856702 0.8731029\n [15] 0.8770435 0.8779488 0.8850312 0.8787475 0.8706534 0.8840194 0.8856169\n [22] 0.8700676 0.8752862 0.8674583 0.8832739 0.8802386 0.8808776 0.8685766\n [29] 0.8693754 0.8844454 0.8718782 0.8818893 0.8827946 0.8706001 0.8851909\n [36] 0.8832206 0.8756057 0.8846051 0.8765110 0.8816763 0.8801853 0.8838064\n [43] 0.8844986 0.8803451 0.8833804 0.8789605 0.8791203 0.8773630 0.8826881\n [50] 0.8819958 0.8724107 0.8831141 0.8821023 0.8863092 0.8860429 0.8817828\n [57] 0.8835401 0.8681506 0.8782683 0.8743277 0.8829011 0.8774163 0.8732094\n [64] 0.8721977 0.8786410 0.8752862 0.8670323 0.8815166 0.8713989 0.8794398\n [71] 0.8852442 0.8845519 0.8765642 0.8715587 0.8752330 0.8854039 0.8806646\n [78] 0.8843921 0.8811971 0.8704404 0.8828479 0.8770968 0.8793865 0.8719314\n [85] 0.8760317 0.8785878 0.8762980 0.8728899 0.8865222 0.8759785 0.8794398\n [92] 0.8695351 0.8758720 0.8836999 0.8727302 0.8655413 0.8841791 0.8862559\n [99] 0.8832206 0.8771500 0.8816231 0.8823154 0.8702274 0.8830609 0.8629852\n[106] 0.8815698 0.8740082 0.8724639 0.8798658 0.8818361 0.8813036 0.8663401\n[113] 0.8767773 0.8715054 0.8592577 0.8721977 0.8822621 0.8842856 0.8818893\n[120] 0.8799191 0.8667128 0.8812503 0.8783748 0.8842856 0.8774163 0.8786943\n[127] 0.8877470 0.8865754 0.8790138 0.8787475 0.8799191 0.8754992 0.8658076\n[134] 0.8736354 0.8810373 0.8844986 0.8773630 0.8799723 0.8836999 0.8835401\n[141] 0.8848714 0.8731562 0.8795463 0.8777358 0.8833804 0.8831674 0.8747005\n[148] 0.8801853 0.8760850 0.8783748 0.8818893 0.8860429 0.8816231 0.8854572\n[155] 0.8775228 0.8826881 0.8686299 0.8724639 0.8637840 0.8735822 0.8751797\n[162] 0.8805048 0.8794931 0.8798126 0.8817828 0.8781618 0.8690559 0.8745407\n[169] 0.8826881 0.8707066 0.8819426 0.8774163 0.8708664 0.8787475 0.8793333\n[176] 0.8830076 0.8778423 0.8815698 0.8784280 0.8808243 0.8737419 0.8811438\n[183] 0.8818361 0.8801321 0.8736887 0.8711326 0.8668193 0.8724107 0.8696949\n[190] 0.8846051 0.8788540 0.8831141 0.8610150 0.8840194 0.8782683 0.8658608\n[197] 0.8794931 0.8732094 0.8711326 0.8822089\n\n$slope\n  [1] 0.8889082 0.7835603 1.0998928 0.9075851 1.0469064 0.9724926 0.8028218\n  [8] 0.8564232 0.6067719 0.8984688 1.2096357 0.9235358 0.8543193 0.8134986\n [15] 0.7727660 1.0607298 0.7948216 0.8616171 0.7779063 0.6508093 1.0873116\n [22] 0.8832961 0.7331516 0.8875085 0.9145840 0.8954953 1.0001636 0.6855281\n [29] 0.7013706 0.8388970 0.8074663 0.8582302 0.8350815 0.8383861 0.8533940\n [36] 0.8525511 0.9774162 1.0172420 0.8218355 0.7828232 0.8400168 0.8215543\n [43] 0.7015930 0.7294820 0.8005938 0.8484824 0.8359021 0.9888289 0.9727314\n [50] 0.7711334 0.8364241 0.8908883 0.9076600 0.7212976 0.9022637 0.8701368\n [57] 0.7449556 0.8688495 0.8180298 0.9131204 0.8789392 0.9112303 0.9195366\n [64] 0.7396357 0.8179017 0.9359296 0.6220846 0.8183743 0.9936095 0.9430373\n [71] 1.2115619 0.9572224 0.9403656 0.8508424 0.8257627 0.9264804 0.8765501\n [78] 1.1651581 0.8514596 0.8718135 0.7710216 0.8262512 1.0335467 0.8255302\n [85] 0.7184376 1.0885003 0.7556400 0.8356761 0.8118482 0.8595956 0.9516714\n [92] 0.6973051 0.9370568 1.0198609 0.7106397 0.8749268 0.8942730 0.8175278\n [99] 0.8777217 0.9155271 0.8245478 0.9799529 1.0947849 0.8291485 0.8611885\n[106] 0.9121642 0.7645228 0.8007897 0.9314307 0.9440057 0.8357351 0.6211848\n[113] 0.9030527 0.9807442 0.8463329 0.8567849 0.9378717 0.9136026 0.7758592\n[120] 0.8757769 0.9769324 0.8247464 0.8277044 0.8571125 0.7493612 0.8490830\n[127] 0.7195749 0.9477767 0.9771676 0.9388602 0.9604387 0.8193784 0.7232914\n[134] 0.9080426 0.8124742 0.8645012 0.7358860 1.0851271 1.0715879 1.0551447\n[141] 0.8992292 1.2046749 0.9209899 0.9984606 0.9067849 0.8866407 0.7250194\n[148] 1.1175676 0.7013702 0.7763262 0.7991884 1.0592525 0.9152192 1.0308829\n[155] 0.7497446 0.8548355 0.7981949 0.9105384 0.8012398 0.7027564 0.8581011\n[162] 0.6846810 0.9725659 0.7625480 0.8870886 0.6752368 0.7440799 1.0564467\n[169] 0.8431525 0.7697717 0.7970958 0.8602210 0.8333069 0.7279489 1.0695581\n[176] 0.9022436 0.7283357 0.8474979 0.8652495 0.9664762 0.8888945 0.8529874\n[183] 1.2279259 1.0301220 1.0134353 0.7500525 0.7696149 0.8167861 0.9838986\n[190] 0.6907492 0.7995417 0.9215697 0.9145182 0.7600710 0.8578466 0.8832045\n[197] 0.6094729 0.9161611 0.8691550 1.0409001\n\n\nThat all seems correct. It is interesting to note that AUC looks fairly stable here whereas the calibration slope seems more erratic, meaning we expect it to have higher variance than bigger sample sizes. Let’s find out by now generalising and embedding this loop into another for-loop to cover the varying sample sizes.\n\nfor (s in sample_sizes) {\n  test_sample &lt;- sample_n(df, s, replace = FALSE)\n  \n  for (b in 1:B) {\n    res &lt;- performance(test_sample)\n    if (is.null(res)) {\n      fail_counts[as.character(s)] &lt;- fail_counts[as.character(s)] + 1\n      next\n    }\n    results[[as.character(s)]] [[\"auc\"]] [b] &lt;- res$auc\n    results[[as.character(s)]] [[\"slope\"]] [b] &lt;- res$slope\n  }\n}\n\nIf successful, we should now have a full complement of AUC and Slope values across all of our sample sizes.\n\nresults\n\n$`100`\n$`100`$auc\n  [1] 0.9114667 0.8960000 0.9109333 0.9077333 0.8938667 0.8949333 0.9098667\n  [8] 0.8970667 0.9381333 0.9114667 0.0000000 0.9258667 0.9141333 0.9130667\n [15] 0.9253333 0.9005333 0.9168000 0.9162667 0.8741333 0.9200000 0.9269333\n [22] 0.9189333 0.9194667 0.9104000 0.8965333 0.9146667 0.9205333 0.8688000\n [29] 0.9146667 0.9290667 0.0000000 0.9322667 0.9040000 0.9125333 0.9317333\n [36] 0.0000000 0.9317333 0.0000000 0.9226667 0.9285333 0.9194667 0.9040000\n [43] 0.8709333 0.8608000 0.9338667 0.8992000 0.8698667 0.9237333 0.9002667\n [50] 0.9184000 0.9098667 0.9194667 0.8912000 0.0000000 0.9152000 0.9008000\n [57] 0.9194667 0.8976000 0.9194667 0.0000000 0.9301333 0.8858667 0.0000000\n [64] 0.9109333 0.9248000 0.9184000 0.9018667 0.8762667 0.9141333 0.9141333\n [71] 0.9125333 0.9008000 0.9205333 0.0000000 0.9002667 0.9168000 0.8949333\n [78] 0.8826667 0.9173333 0.9056000 0.0000000 0.9184000 0.9200000 0.8869333\n [85] 0.9162667 0.9200000 0.9162667 0.9237333 0.8805333 0.8912000 0.9050667\n [92] 0.8874667 0.9152000 0.9173333 0.9136000 0.9130667 0.9050667 0.8981333\n [99] 0.9136000 0.9216000 0.9328000 0.9194667 0.9253333 0.0000000 0.9157333\n[106] 0.9296000 0.8842667 0.9125333 0.9216000 0.9200000 0.8944000 0.9029333\n[113] 0.8949333 0.8976000 0.0000000 0.0000000 0.8762667 0.9189333 0.0000000\n[120] 0.0000000 0.9312000 0.0000000 0.9184000 0.9146667 0.0000000 0.0000000\n[127] 0.9082667 0.9322667 0.9248000 0.8874667 0.9194667 0.8816000 0.9093333\n[134] 0.9125333 0.9072000 0.9226667 0.9264000 0.9093333 0.9136000 0.0000000\n[141] 0.9168000 0.9333333 0.8970667 0.0000000 0.0000000 0.9205333 0.9306667\n[148] 0.8634667 0.8896000 0.9050667 0.9157333 0.9034667 0.9200000 0.9093333\n[155] 0.9216000 0.9162667 0.9354667 0.8944000 0.9216000 0.9253333 0.9285333\n[162] 0.0000000 0.9018667 0.8997333 0.9152000 0.9077333 0.9365333 0.0000000\n[169] 0.9216000 0.8512000 0.8949333 0.8992000 0.9162667 0.9221333 0.9040000\n[176] 0.8970667 0.9280000 0.0000000 0.9152000 0.8880000 0.9114667 0.9040000\n[183] 0.9093333 0.9168000 0.9194667 0.8853333 0.8928000 0.9184000 0.9328000\n[190] 0.9077333 0.9146667 0.9258667 0.9034667 0.9237333 0.8981333 0.9210667\n[197] 0.9296000 0.8997333 0.8928000 0.9077333\n\n$`100`$slope\n  [1] 0.51780374 0.59075859 0.81184472 0.82164488 0.36559414 0.43597802\n  [7] 0.44854672 0.30945120 0.45400737 0.53633513 0.00000000 0.62811139\n [13] 0.49032984 0.51933972 0.47899963 0.10428352 0.72783271 0.50069912\n [19] 0.14550811 0.36597555 0.77689227 0.42609638 0.26948908 0.45931690\n [25] 0.21077566 0.64819252 0.17839001 0.11573450 0.72012849 0.45565800\n [31] 0.00000000 0.31762235 0.65086235 0.76100884 0.52997157 0.00000000\n [37] 0.37133161 0.00000000 0.74651070 0.24813137 0.62454569 0.90389026\n [43] 0.47702529 0.57846133 0.73551478 0.26743963 0.29902713 0.84557971\n [49] 0.44239616 0.67482701 0.33431278 0.26984567 0.46994393 0.00000000\n [55] 0.40263294 0.78312918 0.70943704 0.71245409 0.90974387 0.00000000\n [61] 0.54505279 0.53851028 0.00000000 0.32761182 0.52451851 0.25122245\n [67] 0.62876623 0.41617184 0.45003718 0.38613282 0.47361886 0.48340349\n [73] 0.31850354 0.00000000 0.41290486 0.43410459 0.48675179 0.46905144\n [79] 0.70664956 0.15545959 0.00000000 0.79502703 0.83337422 0.57474551\n [85] 0.46753597 0.60730455 0.63419708 0.29502353 0.30529132 0.37218316\n [91] 0.60988662 0.29459226 0.33861572 0.64584408 0.67897804 1.06239949\n [97] 0.72151824 0.34185688 0.76768384 0.78923481 0.47076435 0.55185699\n[103] 0.82778185 0.00000000 0.20826536 0.60571352 0.46668099 0.50465505\n[109] 0.60136000 0.71746314 0.65476832 0.76714720 0.20490607 0.82294253\n[115] 0.00000000 0.00000000 0.34666627 0.36502312 0.00000000 0.00000000\n[121] 0.84105904 0.00000000 0.75666258 0.96760326 0.00000000 0.00000000\n[127] 0.54840787 0.60976157 0.49609254 0.19574251 0.67524869 0.10219184\n[133] 0.56074139 0.73893689 0.49150348 1.01408044 0.81331352 0.86517994\n[139] 0.58138839 0.00000000 0.21222007 0.39917949 0.24546066 0.00000000\n[145] 0.00000000 0.99280882 0.27906693 0.34312755 0.35033602 0.61464469\n[151] 0.73557644 0.29668069 0.48980992 0.73515404 0.59782847 0.50486810\n[157] 0.39605195 0.01683664 0.93111334 1.21498187 1.04886906 0.00000000\n[163] 0.51398152 0.19804731 0.64534352 0.54426121 0.36519814 0.00000000\n[169] 0.68112700 0.32106068 0.22348872 0.58329508 0.74581354 0.65129558\n[175] 0.63960980 0.12946651 0.39255257 0.00000000 0.78202106 0.72005984\n[181] 0.54041192 0.51336116 0.51097700 0.62885021 0.61770214 0.35374397\n[187] 0.52654383 0.64241179 0.47829025 0.56093797 0.75015561 0.57893386\n[193] 0.73424320 0.76036473 0.52388319 0.50461609 0.29534931 0.76739469\n[199] 0.61305565 0.35825695\n\n\n$`200`\n$`200`$auc\n  [1] 0.9209040 0.9379733 0.9287174 0.9247506 0.9353288 0.9343671 0.9276355\n  [8] 0.9277557 0.9231879 0.9331650 0.9384541 0.9221060 0.9368915 0.9239091\n [15] 0.9291982 0.9248708 0.9253516 0.9328044 0.9314821 0.9264335 0.9218656\n [22] 0.9346075 0.9311215 0.9323236 0.9317226 0.9305205 0.9285972 0.9318428\n [29] 0.9356894 0.9308811 0.9243900 0.9359298 0.9374925 0.9299195 0.9178988\n [36] 0.9322034 0.9246304 0.9338863 0.9289578 0.9178988 0.9344873 0.9319630\n [43] 0.9297993 0.9255920 0.9316024 0.9235485 0.9300397 0.9384541 0.9348479\n [50] 0.9308811 0.9376127 0.9229475 0.9328044 0.9342469 0.9294386 0.9061185\n [57] 0.9239091 0.9349681 0.9156149 0.9316024 0.9354490 0.9332853 0.9360500\n [64] 0.9241495 0.9162159 0.9310013 0.9372521 0.9317226 0.9314821 0.9289578\n [71] 0.9122491 0.9337661 0.9289578 0.9325640 0.9277557 0.9362904 0.9322034\n [78] 0.9300397 0.9325640 0.9209040 0.9168169 0.9354490 0.9189806 0.9335257\n [85] 0.9359298 0.9209040 0.9313619 0.9261931 0.9267941 0.9189806 0.9326842\n [92] 0.9352086 0.9361702 0.9343671 0.9370117 0.9186200 0.9231879 0.9145330\n [99] 0.9141724 0.9299195 0.9329246 0.9255920 0.9344873 0.9304003 0.9352086\n[106] 0.9302801 0.9281164 0.9311215 0.9370117 0.9301599 0.9313619 0.9341267\n[113] 0.9360500 0.9284770 0.9305205 0.9336459 0.9336459 0.9382137 0.9186200\n[120] 0.9336459 0.9325640 0.9237889 0.9269143 0.9377329 0.9347277 0.9265537\n[127] 0.9245102 0.9361702 0.9397764 0.9233081 0.9195817 0.9330448 0.9267941\n[134] 0.9271547 0.9291982 0.9305205 0.9239091 0.9263133 0.9260728 0.9353288\n[141] 0.9225868 0.9267941 0.9380935 0.9320832 0.9356894 0.9273951 0.9341267\n[148] 0.9269143 0.9311215 0.9340065 0.9368915 0.9354490 0.9275153 0.9330448\n[155] 0.9307609 0.9266739 0.9316024 0.9235485 0.9288376 0.9354490 0.9323236\n[162] 0.9257122 0.9261931 0.9247506 0.9283568 0.9330448 0.9360500 0.9370117\n[169] 0.9326842 0.9329246 0.9272749 0.9318428 0.9070802 0.9326842 0.9374925\n[176] 0.9326842 0.9366510 0.9324438 0.9270345 0.9348479 0.9288376 0.9360500\n[183] 0.9235485 0.9295588 0.9322034 0.9265537 0.9372521 0.9365308 0.9322034\n[190] 0.9352086 0.9221060 0.9252314 0.9310013 0.9222262 0.9320832 0.9341267\n[197] 0.9361702 0.9322034 0.9255920 0.9235485\n\n$`200`$slope\n  [1] 0.6666597 1.0951647 1.0914485 0.6569375 0.9267780 0.7141791 0.6069748\n  [8] 0.7950556 0.8529516 0.8798092 0.6275252 0.5113409 1.0669052 0.6161151\n [15] 0.8132816 0.7320546 0.7044404 1.0130383 0.8680450 0.9519376 0.8251280\n [22] 0.8261902 1.0051458 0.6192939 0.6649297 0.9491739 0.7305079 0.4608708\n [29] 0.8827693 0.9191652 0.7313798 0.5926412 0.8336742 0.7082094 0.7159005\n [36] 1.1135002 0.5124470 0.8111867 0.7478548 0.7760470 1.1507715 0.7329492\n [43] 0.9651104 0.7150520 0.8086764 0.7651786 0.8769793 1.1648201 0.8855346\n [50] 0.9174263 1.0023671 0.7586263 0.9513364 0.6649718 0.6273402 0.5946398\n [57] 0.6865493 1.1033205 0.5809785 0.7982760 0.9615884 0.6145485 0.9184568\n [64] 0.8886345 0.9693277 0.9035337 0.7845367 0.8692808 0.8855606 0.8365523\n [71] 0.5979871 0.8730144 0.7087032 0.9684433 0.8581888 0.5978283 0.9031857\n [78] 0.8809431 0.6094257 0.6466834 0.7170721 0.7977836 0.4940595 0.8614295\n [85] 1.0517167 0.9184036 0.7019549 0.7810703 0.7730809 0.4998303 0.9598838\n [92] 0.9276833 0.7885184 0.9017137 0.8686354 0.6111778 0.6869010 0.8104047\n [99] 0.7862187 0.9056669 0.6946097 0.7252410 0.6845726 0.8824704 0.8595750\n[106] 0.6990807 0.8799592 0.5923279 1.0426501 0.6739113 0.7595659 0.3145946\n[113] 0.8620172 0.8752384 0.9077846 0.8068628 0.8786387 0.6015103 0.8646593\n[120] 0.8691990 0.9496862 0.9377015 0.9657834 0.7069776 0.5652771 0.5214272\n[127] 0.6913011 0.8881903 0.9594550 0.9623314 0.7773086 0.9438617 0.4751414\n[134] 0.6899151 0.7257525 0.8511727 0.8904723 0.7772425 0.7341348 0.9244311\n[141] 0.7653091 0.9539111 0.8156470 0.8662103 0.8240296 0.8711443 0.7957743\n[148] 0.4059646 0.9657788 0.8668570 0.7799388 1.0298945 0.6001362 0.6498779\n[155] 0.9505821 0.6372088 0.7287073 0.6473815 0.7194242 0.6582811 0.8610627\n[162] 0.8707869 0.7349532 0.6399545 0.9666858 0.7991096 0.8155537 1.0163766\n[169] 0.8305174 1.0427405 0.9383505 0.9153426 0.8295019 0.6808584 0.7746943\n[176] 0.8590634 0.8600243 0.8270058 1.0270282 1.0020930 0.6214468 0.8491245\n[183] 0.6465885 0.8121071 0.6414512 0.7537134 0.8028729 0.8686726 0.8560952\n[190] 0.8342062 0.4766341 0.9162527 0.9090078 0.7557419 0.7007264 0.7867541\n[197] 0.8333618 0.9950624 0.8021168 0.7342589\n\n\n$`300`\n$`300`$auc\n  [1] 0.8617518 0.8691392 0.8672295 0.8594904 0.8661742 0.8665260 0.8616011\n  [8] 0.8606965 0.8655209 0.8520026 0.8628072 0.8609478 0.8655711 0.8647671\n [15] 0.8686366 0.8676818 0.8636615 0.8675813 0.8650183 0.8664254 0.8632595\n [22] 0.8610483 0.8640133 0.8637620 0.8596914 0.8650686 0.8594904 0.8647168\n [29] 0.8624554 0.8651691 0.8642645 0.8584854 0.8565757 0.8557214 0.8692397\n [36] 0.8651691 0.8657721 0.8575305 0.8554199 0.8619529 0.8625559 0.8659229\n [43] 0.8717523 0.8713001 0.8640635 0.8659732 0.8636615 0.8629077 0.8562742\n [50] 0.8677320 0.8647168 0.8564249 0.8693904 0.8651189 0.8694407 0.8672295\n [57] 0.8652194 0.8602442 0.8628574 0.8647671 0.8708478 0.8560732 0.8594904\n [64] 0.8630584 0.8561234 0.8678828 0.8649681 0.8670285 0.8665762 0.8604452\n [71] 0.8655209 0.8731092 0.8619026 0.8683351 0.8581838 0.8689381 0.8703955\n [78] 0.8620534 0.8666767 0.8678828 0.8692397 0.8709985 0.8687874 0.8595909\n [85] 0.8634605 0.8701442 0.8665260 0.8640635 0.8639630 0.8709483 0.8658727\n [92] 0.8652194 0.8660234 0.8632595 0.8646666 0.8586361 0.8710488 0.8579326\n [99] 0.8639630 0.8632595 0.8652696 0.8658727 0.8681341 0.8686869 0.8721544\n[106] 0.8690386 0.8530579 0.8600432 0.8605458 0.8674305 0.8643148 0.8577818\n[113] 0.8689381 0.8682346 0.8670285 0.8619529 0.8707975 0.8694407 0.8609478\n[120] 0.8544148 0.8675813 0.8696417 0.8631087 0.8662244 0.8700940 0.8655209\n[127] 0.8690889 0.8614001 0.8721544 0.8591889 0.8638625 0.8591889 0.8700437\n[134] 0.8692899 0.8540630 0.8663752 0.8691392 0.8661742 0.8612493 0.8536610\n[141] 0.8672295 0.8634605 0.8624554 0.8671793 0.8602442 0.8606965 0.8679331\n[148] 0.8554701 0.8566762 0.8639128 0.8647168 0.8588874 0.8660234 0.8629077\n[155] 0.8606463 0.8650183 0.8663249 0.8587869 0.8521534 0.8628574 0.8672295\n[162] 0.8664254 0.8664254 0.8586864 0.8672798 0.8650686 0.8685864 0.8594904\n[169] 0.8627569 0.8646666 0.8592392 0.8688879 0.8636112 0.8641138 0.8661239\n[176] 0.8710488 0.8590381 0.8628072 0.8666265 0.8621036 0.8644656 0.8667270\n[183] 0.8684859 0.8620534 0.8612493 0.8600935 0.8604452 0.8573295 0.8620534\n[190] 0.8668275 0.8659229 0.8684356 0.8654204 0.8615508 0.8629077 0.8665260\n[197] 0.8630584 0.8650183 0.8657721 0.8612493\n\n$`300`$slope\n  [1] 0.7879927 0.7993611 0.7753211 0.9420793 0.8313845 0.8667303 0.9500504\n  [8] 0.8608959 0.8579081 0.9144783 0.8776798 0.8944344 0.7408516 0.8579055\n [15] 1.0230056 1.0776820 0.8482262 0.9682507 0.8307402 0.6849582 0.8791337\n [22] 0.9280547 0.7912982 0.9485147 0.9760599 0.8021058 0.7641364 1.0336303\n [29] 0.9846863 0.9731529 0.9720118 1.0141253 0.8457773 0.9011487 0.9837462\n [36] 0.6024806 0.7860371 0.7343010 0.8867111 0.8320180 0.9560558 0.8956926\n [43] 0.8773986 1.1472819 0.9581412 0.8896131 0.8881308 0.8186153 0.8938528\n [50] 0.9153853 1.0031410 0.8021277 1.0357364 0.6564045 1.0740064 1.0476332\n [57] 0.9680561 0.9533676 0.7279936 0.8022559 0.7369384 0.6412380 0.8781905\n [64] 0.8259511 0.7083809 1.1297796 1.0568260 0.8218258 0.8704938 0.7770058\n [71] 0.7269156 1.0039513 0.9496584 0.8310120 0.7912221 1.0027474 0.9017853\n [78] 0.6499883 0.8835744 0.9815890 0.7906378 0.7066345 0.8117492 0.8284811\n [85] 0.8211991 0.9309553 0.8950801 0.7812445 0.7522209 0.8699986 0.7993100\n [92] 0.8014601 0.6462095 0.9134819 0.9341478 0.9629540 0.8290981 0.8412126\n [99] 0.9001743 1.0478112 0.7574794 0.7294939 0.8084868 0.8381572 0.7613316\n[106] 0.9592954 0.6476804 0.8333533 0.7629040 0.8895526 0.7377474 0.9939105\n[113] 0.9115408 0.8041235 0.8371510 0.8653852 1.0056965 0.7955830 0.7103949\n[120] 0.7746443 0.7388703 0.9692410 1.0138899 0.9046320 0.7966855 0.8195444\n[127] 0.7926048 0.9013068 0.8482746 0.8232046 0.7889784 0.8094994 0.9008792\n[134] 0.7495884 0.8105121 0.8362706 0.7783657 0.8021253 0.8389025 0.7364181\n[141] 0.9894512 0.6587951 0.7685054 0.7393352 1.0050083 0.9121632 0.7615728\n[148] 0.8671511 0.7434121 0.9018379 0.8673344 0.6159232 1.0967284 0.7142825\n[155] 0.8411695 1.0120964 0.9865398 0.8460613 0.8686009 0.8294763 0.9358098\n[162] 0.9577097 0.7755044 0.8598264 0.8097210 0.7367635 0.8382390 0.7962356\n[169] 0.8342498 0.9649305 0.6287618 0.9255225 0.7139509 1.0360695 0.9261150\n[176] 0.9173411 0.7373222 0.8298878 0.8505177 0.9064355 0.8666617 0.8841345\n[183] 0.6542070 0.9672586 1.0033966 0.8584533 1.0485070 0.9885382 0.9257271\n[190] 0.7948972 0.9516159 0.8877714 0.6886272 0.9729293 0.8106340 0.9229275\n[197] 0.8367712 0.8874473 0.7222047 0.6083408\n\n\n$`500`\n$`500`$auc\n  [1] 0.8943429 0.8925905 0.8960952 0.8973143 0.8900952 0.8890095 0.8954286\n  [8] 0.8963429 0.8953714 0.8952190 0.8937333 0.8960952 0.8906857 0.8936000\n [15] 0.8887048 0.8816190 0.8895619 0.8904381 0.8911429 0.8936190 0.8936381\n [22] 0.8950286 0.8864952 0.8943619 0.8951810 0.8897524 0.8934667 0.8928571\n [29] 0.8956000 0.8947048 0.8918667 0.8942476 0.8961905 0.8945714 0.8951429\n [36] 0.8951048 0.8944381 0.8964952 0.8953714 0.8945143 0.8948381 0.8832381\n [43] 0.8973524 0.8960000 0.8907619 0.8932952 0.8923429 0.8953905 0.8969905\n [50] 0.8973905 0.8911810 0.8984381 0.8929905 0.8950857 0.8962095 0.8941143\n [57] 0.8945143 0.8959048 0.8937333 0.8917524 0.8878095 0.8955048 0.8933143\n [64] 0.8908381 0.8912571 0.8967048 0.8941524 0.8977905 0.8950857 0.8936571\n [71] 0.8967429 0.8960762 0.8922286 0.8861524 0.8967810 0.8925333 0.8933333\n [78] 0.8908952 0.8943238 0.8943810 0.8980952 0.8914286 0.8917714 0.8928190\n [85] 0.8923048 0.8941333 0.8934095 0.8889143 0.8921714 0.8949143 0.8896952\n [92] 0.8915238 0.8924762 0.8953143 0.8942286 0.8930857 0.8878286 0.8942095\n [99] 0.8977333 0.8919048 0.8958095 0.8959619 0.8928952 0.8772381 0.8866095\n[106] 0.8988000 0.8943048 0.8917524 0.8890095 0.8964571 0.8984190 0.8954476\n[113] 0.8879048 0.8887810 0.8948571 0.8918286 0.8911238 0.8977143 0.8936000\n[120] 0.8928571 0.8970667 0.8891429 0.8936952 0.8916000 0.8875429 0.8946667\n[127] 0.8891619 0.8915048 0.8975429 0.8876762 0.8936571 0.8950857 0.8909333\n[134] 0.8956571 0.8859238 0.8957143 0.8920762 0.8963429 0.8954286 0.8929143\n[141] 0.8895810 0.8924952 0.8962095 0.8958286 0.8914095 0.8955238 0.8952381\n[148] 0.8832952 0.8920381 0.8932952 0.8919238 0.8938095 0.8949143 0.8935619\n[155] 0.8948190 0.8886095 0.8952762 0.8941143 0.8904762 0.8908952 0.8854476\n[162] 0.8961905 0.8970857 0.8933905 0.8939619 0.8956381 0.8963429 0.8884762\n[169] 0.8962667 0.8948571 0.8950667 0.8952000 0.8942667 0.8982476 0.8975429\n[176] 0.8954667 0.8978095 0.8928381 0.8965905 0.8922476 0.8970476 0.8826286\n[183] 0.8975048 0.8904952 0.8968000 0.8904190 0.8912952 0.8946095 0.8842857\n[190] 0.8944381 0.8906286 0.8919429 0.8939429 0.8963238 0.8952952 0.8944190\n[197] 0.8928000 0.8968952 0.8954286 0.8876381\n\n$`500`$slope\n  [1] 0.8244534 0.9620053 0.9175703 1.0219985 0.9402583 0.7660061 0.9704723\n  [8] 0.9171736 0.8863110 0.9351361 0.9837407 1.0572332 0.8670441 0.6925628\n [15] 1.2068582 0.7915396 0.9447131 0.8216349 1.1407151 1.0318921 0.9644048\n [22] 0.8457886 1.0192688 0.7893072 0.9850486 0.9227640 0.9630457 1.2799796\n [29] 0.8544376 1.1743397 0.8645721 0.9079218 0.8372642 0.9108189 1.0292946\n [36] 0.9050211 0.9006847 0.8026275 0.9030327 0.9135142 1.0768781 0.9332468\n [43] 1.0293188 0.8932148 0.9531569 0.9163341 0.9038668 0.9372096 0.8671921\n [50] 0.9757643 0.9700028 0.7851843 0.7854772 0.9008470 0.8329858 0.7846692\n [57] 0.8782201 1.0929135 1.0818875 0.7465228 1.0586849 1.0752693 1.0389667\n [64] 0.9471586 0.8671236 0.8757006 1.1147423 0.9992332 0.8452273 0.9115584\n [71] 0.7414906 0.9346725 0.7713019 0.9481378 0.9707939 0.9354713 0.8944461\n [78] 0.9666393 0.8250555 0.8487186 0.8012491 0.9163591 0.8555554 0.8976674\n [85] 0.8975972 0.9601964 0.8166601 0.9505691 0.8722888 0.8291167 0.8811782\n [92] 0.9638437 0.9024418 0.9060511 0.9218447 0.9094539 0.9197078 1.0688042\n [99] 0.7502053 0.9289224 0.9531227 1.0505710 0.9522980 0.9343862 0.9150369\n[106] 0.9388166 0.9077634 1.0196730 0.6941965 1.0097366 0.9651267 0.7612630\n[113] 0.8731494 1.0380428 0.9362314 0.8277593 0.8705520 0.8804334 1.0133345\n[120] 0.8813871 1.0179728 1.0283381 0.8654002 0.8756825 0.7980731 0.9675254\n[127] 0.9206177 0.9624827 1.0658094 0.8613158 0.8167309 0.7774870 0.8837559\n[134] 0.9890638 0.9676875 0.9555277 0.7967610 0.9211309 0.9241304 0.9053615\n[141] 0.7743525 1.1451670 0.9000121 0.8663570 0.9107525 0.9445211 0.9445324\n[148] 0.9077301 0.8914197 0.8632127 0.9826240 1.0424607 0.8778629 0.9168231\n[155] 0.8580495 0.9766109 0.9884793 0.9187878 0.7789541 0.9880921 1.0490128\n[162] 0.8942231 0.9043690 0.9386119 0.9673944 0.8827347 1.0195321 0.9226044\n[169] 0.8735568 0.8097479 0.9176926 0.9067266 0.9626221 0.8481729 0.8287407\n[176] 0.9510742 0.9281058 0.9071700 1.1655033 0.9367034 0.9692973 0.9271328\n[183] 0.9035424 0.9863372 0.7685132 1.0099174 0.7713810 0.8733984 0.7680307\n[190] 1.0244312 0.8056952 0.7545905 0.9622415 0.9443199 1.0424115 1.0813396\n[197] 0.9021676 1.0555373 0.6705937 0.8792196\n\n\n$`1000`\n$`1000`$auc\n  [1] 0.8846325 0.8852066 0.8840585 0.8857663 0.8829579 0.8857995 0.8841629\n  [8] 0.8838783 0.8761646 0.8855718 0.8853394 0.8827065 0.8860225 0.8849883\n [15] 0.8858754 0.8829911 0.8860462 0.8810034 0.8817387 0.8862597 0.8848081\n [22] 0.8838166 0.8823365 0.8833185 0.8826401 0.8847796 0.8827966 0.8863546\n [29] 0.8855718 0.8866250 0.8874741 0.8864352 0.8853157 0.8860794 0.8857284\n [36] 0.8850832 0.8844950 0.8838545 0.8861553 0.8863119 0.8838783 0.8852160\n [43] 0.8851686 0.8861174 0.8850453 0.8862644 0.8853963 0.8871278 0.8859513\n [50] 0.8852018 0.8849978 0.8853157 0.8854105 0.8804769 0.8863119 0.8829627\n [57] 0.8865491 0.8803915 0.8842151 0.8857711 0.8856098 0.8874504 0.8832615\n [64] 0.8847701 0.8868717 0.8844380 0.8824551 0.8857189 0.8854105 0.8865491\n [71] 0.8837786 0.8826116 0.8840917 0.8842862 0.8856762 0.8814778 0.8852113\n [78] 0.8829390 0.8842720 0.8860747 0.8864874 0.8836173 0.8857568 0.8844048\n [85] 0.8851259 0.8836173 0.8837407 0.8863498 0.8845566 0.8848175 0.8863878\n [92] 0.8841724 0.8810414 0.8802586 0.8833564 0.8835699 0.8828963 0.8847416\n [99] 0.8865681 0.8845139 0.8853868 0.8847132 0.8818099 0.8817672 0.8854580\n[106] 0.8854770 0.8844665 0.8823839 0.8855813 0.8847986 0.8850974 0.8861411\n[113] 0.8848318 0.8851876 0.8850168 0.8867768 0.8836078 0.8829200 0.8851686\n[120] 0.8853916 0.8867151 0.8840965 0.8859798 0.8858280 0.8864020 0.8836078\n[127] 0.8835177 0.8828393 0.8848128 0.8857521 0.8828298 0.8854105 0.8851117\n[134] 0.8848175 0.8856382 0.8846278 0.8834750 0.8859513 0.8844380 0.8850547\n[141] 0.8825926 0.8821467 0.8861126 0.8838688 0.8838308 0.8850547 0.8846942\n[148] 0.8864969 0.8840728 0.8850785 0.8842293 0.8854864 0.8849646 0.8856525\n[155] 0.8861506 0.8861696 0.8824693 0.8851069 0.8857379 0.8851069 0.8862597\n[162] 0.8851354 0.8845282 0.8820945 0.8850026 0.8864495 0.8838640 0.8855339\n[169] 0.8842625 0.8793051 0.8854295 0.8854153 0.8845993 0.8859656 0.8852160\n[176] 0.8849741 0.8848745 0.8861743 0.8854390 0.8848033 0.8836078 0.8842578\n[183] 0.8854105 0.8844380 0.8853489 0.8871421 0.8849551 0.8859988 0.8854343\n[190] 0.8839304 0.8831429 0.8849029 0.8861126 0.8863166 0.8855718 0.8827777\n[197] 0.8847843 0.8855813 0.8848887 0.8844950\n\n$`1000`$slope\n  [1] 0.9852381 0.7898581 0.8812900 0.9786486 0.9097667 0.9947630 1.0684678\n  [8] 1.0496328 0.8964255 1.0641163 1.0246567 0.8692932 0.9416823 0.9835684\n [15] 1.0678668 0.8182538 0.9996595 0.9173864 0.9164270 0.9010734 0.9198166\n [22] 1.0739346 1.0237897 0.9606352 0.8834124 0.9320456 0.9007392 0.9075971\n [29] 0.8842166 0.9932480 0.9303757 0.9157048 1.0498048 0.9289318 0.9557996\n [36] 1.0080922 1.0453777 1.1324027 0.8850802 0.9963002 0.9513899 0.9939835\n [43] 0.8640341 0.9567189 0.9663666 0.9792783 0.9622800 0.8690554 0.9256156\n [50] 1.0355969 1.0381388 0.8899893 0.9415012 0.9054642 0.9860825 0.9573320\n [57] 0.8976083 0.9340546 1.0802978 0.9388002 1.0267930 0.9054178 0.8453311\n [64] 0.8143484 0.7662951 0.9477910 1.0020081 1.1614899 0.9751820 0.9543141\n [71] 1.0103703 0.9927176 0.9870779 0.8945258 1.0592164 0.9802521 0.9449747\n [78] 0.9852600 0.9698190 0.8814869 0.9961142 1.0588849 0.8899665 1.0595174\n [85] 0.9918076 0.9604684 0.9321236 0.9588438 0.8588146 1.0389137 1.0990709\n [92] 1.0975232 0.8456669 0.8495586 0.9652924 0.8849180 0.9585211 0.9167412\n [99] 0.9807617 0.9569287 0.8937268 1.0197892 0.8916115 1.0713543 0.9600638\n[106] 0.9986271 0.9473869 0.9819097 1.0352753 1.0520562 1.0125950 0.9661816\n[113] 1.1058315 0.9217728 0.9789093 1.0089878 0.8743264 0.9817567 1.0064379\n[120] 0.9810415 1.0901619 0.9290745 0.9942657 0.9296945 1.0056320 1.0061146\n[127] 0.8747390 1.0275949 0.8329266 0.8819037 1.0040348 0.9273354 1.0125173\n[134] 0.8669738 0.9379315 0.9507643 0.8879255 0.9821571 1.0317899 0.9509164\n[141] 0.9850218 0.9847530 0.8358493 0.9276015 1.1021661 0.9647318 0.8973355\n[148] 0.9021280 1.0151838 0.8697623 0.8305347 0.9824436 0.8757694 1.0161260\n[155] 0.9540422 1.1269491 0.9098900 0.9269110 0.9584609 1.0278809 0.8685256\n[162] 1.0486597 0.9431285 0.9351415 0.9396171 0.9585241 0.9637169 1.0332917\n[169] 0.8846559 0.8870210 1.0401249 0.8126616 0.9787590 1.0256073 1.0149118\n[176] 1.0392576 0.9397645 0.9957260 0.8048890 0.9418402 0.8578680 0.9624894\n[183] 0.9161299 0.9131407 0.9109385 0.9299624 0.9386772 1.0705691 0.9882677\n[190] 0.9773611 0.8498530 0.9955588 1.0840412 1.0051635 0.8291410 1.0562396\n[197] 0.9059320 0.9201050 0.9494543 1.0356501\n\n\n$`5000`\n$`5000`$auc\n  [1] 0.8835379 0.8842208 0.8842667 0.8843166 0.8840394 0.8843501 0.8837065\n  [8] 0.8841414 0.8842321 0.8846038 0.8847359 0.8840247 0.8841512 0.8841421\n [15] 0.8842479 0.8839390 0.8843435 0.8840615 0.8841001 0.8836650 0.8846036\n [22] 0.8841268 0.8839766 0.8840222 0.8843210 0.8837491 0.8840412 0.8841902\n [29] 0.8839838 0.8842231 0.8843231 0.8841551 0.8842286 0.8841863 0.8844142\n [36] 0.8841357 0.8839561 0.8842146 0.8836177 0.8838571 0.8838832 0.8837204\n [43] 0.8840338 0.8842243 0.8842995 0.8845255 0.8842001 0.8845853 0.8840319\n [50] 0.8842454 0.8836071 0.8842121 0.8839830 0.8838799 0.8842010 0.8840131\n [57] 0.8842160 0.8843819 0.8842974 0.8844497 0.8843400 0.8839811 0.8840216\n [64] 0.8839160 0.8839873 0.8838530 0.8840702 0.8841728 0.8844871 0.8841470\n [71] 0.8841336 0.8842863 0.8845881 0.8837960 0.8837935 0.8841846 0.8844065\n [78] 0.8841303 0.8843499 0.8838241 0.8844344 0.8845660 0.8842621 0.8838105\n [85] 0.8842264 0.8844445 0.8845078 0.8842541 0.8841516 0.8842425 0.8844162\n [92] 0.8839166 0.8841846 0.8837578 0.8840782 0.8843041 0.8843036 0.8840702\n [99] 0.8842311 0.8844780 0.8840495 0.8837652 0.8842733 0.8842976 0.8842968\n[106] 0.8841092 0.8837786 0.8842369 0.8838154 0.8845704 0.8837718 0.8842561\n[113] 0.8843476 0.8844336 0.8835950 0.8844642 0.8826832 0.8842609 0.8841073\n[120] 0.8835297 0.8840782 0.8841220 0.8841462 0.8840999 0.8842276 0.8842805\n[127] 0.8842030 0.8841648 0.8845540 0.8842148 0.8841576 0.8840447 0.8844532\n[134] 0.8837685 0.8841563 0.8844365 0.8844900 0.8836857 0.8837646 0.8842371\n[141] 0.8844929 0.8842536 0.8845776 0.8842557 0.8839933 0.8837830 0.8839183\n[148] 0.8839200 0.8843390 0.8841123 0.8845685 0.8841131 0.8842290 0.8842276\n[155] 0.8840067 0.8842733 0.8843700 0.8841065 0.8845881 0.8838162 0.8843782\n[162] 0.8843883 0.8842069 0.8845574 0.8837766 0.8839873 0.8837941 0.8844873\n[169] 0.8841293 0.8844636 0.8846497 0.8840580 0.8844069 0.8844478 0.8843737\n[176] 0.8844962 0.8843369 0.8839133 0.8842102 0.8839388 0.8846117 0.8839454\n[183] 0.8839859 0.8840181 0.8835978 0.8841260 0.8842648 0.8843063 0.8845627\n[190] 0.8843408 0.8844915 0.8841807 0.8841801 0.8842594 0.8844317 0.8843706\n[197] 0.8844745 0.8838003 0.8845136 0.8843233\n\n$`5000`$slope\n  [1] 1.0020884 0.9869733 1.0543947 1.0265568 1.0163076 1.0235099 0.9547532\n  [8] 1.0490466 0.9527026 0.9740088 0.9963167 0.9814370 0.9199352 0.9249279\n [15] 0.9880835 0.9919776 1.0080039 0.9388240 0.9877319 1.0168739 1.0046323\n [22] 0.9987513 0.9858084 0.9930379 0.9851755 1.0177145 0.9736193 0.9897526\n [29] 0.9991262 1.0207252 1.0115280 1.0298818 0.9843607 0.9933381 0.9513718\n [36] 0.9897210 0.9806545 1.0467718 1.0030928 1.0343622 0.9706614 0.9845833\n [43] 0.9347191 1.0028174 0.9885147 0.9897480 1.0220732 0.9728529 0.9414544\n [50] 0.9494817 0.9949753 1.0133353 1.0451255 0.9549601 0.9258497 1.0800102\n [57] 0.9652977 0.9914661 0.9889701 0.9829457 0.9785646 0.9561449 0.9785316\n [64] 0.9781321 0.9488670 0.9727670 0.9767407 0.9589600 1.0040288 0.9938442\n [71] 0.9991667 1.0085399 0.9743935 1.0029399 0.9980030 1.0231036 0.9709843\n [78] 0.9819480 1.0135145 0.9774044 1.0270305 0.9680403 1.0011287 1.0425385\n [85] 0.9878241 0.9792718 1.0033679 0.9569338 1.0199260 1.0371483 0.9667956\n [92] 0.9409969 0.9847913 1.0254929 1.0007109 0.9818024 0.9976748 0.9414328\n [99] 0.9267406 0.9721077 0.9970464 0.9484630 0.9827552 1.0005559 0.9703505\n[106] 1.0260915 0.9478498 0.9721332 0.9562474 0.9703999 0.9794460 1.0162062\n[113] 0.9973685 0.9590050 1.0067888 0.9846172 1.0013590 1.0262559 1.0343485\n[120] 0.9726751 1.0041691 1.0462502 0.9571640 1.0281130 0.9843435 1.0352205\n[127] 0.9266957 1.0111390 1.0027814 0.9582237 0.9680874 1.0531405 0.9888778\n[134] 1.0008048 0.9663217 0.9708898 0.9956714 0.9745025 1.0169374 1.0166036\n[141] 0.9957236 0.9945313 1.0301918 0.9801190 1.0013831 0.9816076 0.9910369\n[148] 0.9953709 1.0140930 0.9988147 1.0115176 0.9768929 0.9835005 1.0046859\n[155] 0.9873789 0.9872784 0.9521305 0.9814084 1.0258655 0.9462405 1.0286026\n[162] 0.9914965 0.9999586 1.0426169 1.0140692 1.0113299 0.9904001 1.0851429\n[169] 0.9987557 1.0227655 1.0114226 0.9734808 0.9966421 1.0074962 0.9898179\n[176] 0.9307540 1.0032550 0.9752774 1.0377307 1.0259352 1.0054978 0.9849706\n[183] 0.9469630 1.0294533 0.9476580 0.9882558 1.0364438 0.9831283 0.9549583\n[190] 0.9814888 0.9741411 0.9965687 0.9523056 1.0189921 1.0291677 0.9965344\n[197] 0.9501182 1.0457724 0.9873110 1.0254627\n\n\n$`10000`\n$`10000`$auc\n  [1] 0.8769145 0.8767915 0.8769011 0.8768863 0.8768833 0.8768326 0.8767436\n  [8] 0.8767851 0.8769554 0.8768941 0.8767596 0.8769554 0.8770908 0.8767280\n [15] 0.8769504 0.8768489 0.8769308 0.8768678 0.8767347 0.8766999 0.8770004\n [22] 0.8768336 0.8768740 0.8764621 0.8769056 0.8768174 0.8764806 0.8769534\n [29] 0.8770432 0.8768694 0.8770516 0.8769643 0.8766448 0.8767682 0.8767181\n [36] 0.8768859 0.8768879 0.8767829 0.8768023 0.8770002 0.8767262 0.8768592\n [43] 0.8770155 0.8766772 0.8768247 0.8769501 0.8766894 0.8767320 0.8768281\n [50] 0.8768225 0.8766739 0.8769648 0.8769203 0.8770212 0.8769065 0.8770056\n [57] 0.8767638 0.8770543 0.8767390 0.8768440 0.8762676 0.8767238 0.8769124\n [64] 0.8770189 0.8767817 0.8769839 0.8764904 0.8769359 0.8767269 0.8769645\n [71] 0.8767908 0.8769481 0.8767027 0.8767860 0.8769046 0.8764455 0.8768934\n [78] 0.8769660 0.8762298 0.8769233 0.8768980 0.8767449 0.8766309 0.8768288\n [85] 0.8769924 0.8766938 0.8766971 0.8767413 0.8767793 0.8770266 0.8768681\n [92] 0.8770993 0.8768178 0.8769848 0.8770469 0.8767681 0.8767995 0.8766397\n [99] 0.8767614 0.8768621 0.8770918 0.8768786 0.8768830 0.8769555 0.8769049\n[106] 0.8769989 0.8770334 0.8768694 0.8766519 0.8767934 0.8770897 0.8768409\n[113] 0.8768060 0.8769588 0.8767328 0.8769091 0.8768393 0.8769370 0.8768692\n[120] 0.8770572 0.8765175 0.8769704 0.8767294 0.8770573 0.8768845 0.8770027\n[127] 0.8769704 0.8769020 0.8769100 0.8769057 0.8769208 0.8770668 0.8768982\n[134] 0.8770071 0.8765872 0.8767033 0.8768896 0.8769268 0.8768109 0.8770122\n[141] 0.8766695 0.8768686 0.8763975 0.8767851 0.8765890 0.8767492 0.8767845\n[148] 0.8767708 0.8767748 0.8767733 0.8769273 0.8769158 0.8768628 0.8768680\n[155] 0.8766927 0.8768074 0.8768730 0.8768331 0.8769210 0.8767577 0.8761954\n[162] 0.8768293 0.8768532 0.8770302 0.8768965 0.8769747 0.8768007 0.8769490\n[169] 0.8766222 0.8768374 0.8768459 0.8771179 0.8767909 0.8769183 0.8764960\n[176] 0.8764942 0.8766308 0.8767626 0.8769071 0.8767748 0.8769112 0.8766914\n[183] 0.8768981 0.8768465 0.8769768 0.8768232 0.8767878 0.8768235 0.8766940\n[190] 0.8766533 0.8769497 0.8769146 0.8766728 0.8767866 0.8766432 0.8768989\n[197] 0.8769760 0.8770011 0.8769862 0.8768189\n\n$`10000`$slope\n  [1] 0.9817303 0.9951440 0.9998664 1.0014408 1.0139639 1.0096906 0.9871761\n  [8] 0.9762096 0.9870555 1.0019042 1.0442131 1.0187593 1.0139113 0.9718841\n [15] 0.9606565 0.9785742 0.9840210 1.0032835 0.9729817 1.0135341 0.9817563\n [22] 0.9788217 0.9922760 0.9881096 1.0116629 1.0026813 0.9949518 1.0035553\n [29] 1.0004519 1.0044123 0.9946399 1.0048425 0.9874267 1.0104476 0.9737562\n [36] 0.9970364 1.0011269 0.9940634 1.0113779 0.9952561 1.0250888 0.9488589\n [43] 1.0189621 0.9434872 0.9702663 1.0002202 1.0112510 0.9822492 1.0154041\n [50] 0.9751703 0.9885562 0.9994089 1.0049673 1.0085068 0.9812264 0.9793094\n [57] 1.0066141 1.0104210 0.9427491 1.0320405 0.9832864 1.0186848 0.9592021\n [64] 0.9845199 0.9498705 0.9977111 0.9951747 1.0064590 0.9720877 0.9842752\n [71] 0.9782999 0.9978103 0.9879649 1.0083576 0.9905841 1.0118415 1.0324556\n [78] 1.0067240 0.9547293 1.0126970 1.0029589 1.0020586 1.0034074 0.9809624\n [85] 1.0100268 1.0196274 1.0196248 0.9602873 1.0102408 1.0334075 0.9850315\n [92] 0.9784252 0.9702421 1.0146568 0.9771626 0.9976260 1.0136356 0.9755303\n [99] 1.0039985 0.9943236 1.0040964 0.9704100 0.9550298 0.9731474 0.9899580\n[106] 1.0296163 0.9746139 0.9947373 0.9733317 0.9718005 0.9765496 0.9679628\n[113] 0.9990314 0.9747054 0.9915564 0.9859372 0.9979313 0.9854778 0.9708868\n[120] 0.9755120 0.9852090 0.9992797 0.9995976 0.9880738 0.9678247 0.9731745\n[127] 1.0337709 0.9663980 0.9817194 1.0083139 1.0034665 1.0202912 0.9725542\n[134] 1.0407467 1.0289946 1.0001271 1.0353787 1.0130701 0.9709634 1.0029567\n[141] 1.0170572 0.9908324 1.0161924 1.0160166 1.0274727 0.9614347 0.9714319\n[148] 0.9577191 1.0077119 0.9915454 0.9955643 1.0077535 1.0015609 0.9504134\n[155] 0.9968966 0.9912295 1.0071380 0.9915851 0.9844511 0.9872112 1.0355267\n[162] 0.9879130 1.0263142 0.9927091 0.9942829 0.9870626 0.9976046 0.9896430\n[169] 0.9822587 0.9654274 0.9918125 1.0150475 0.9754423 0.9976175 1.0157156\n[176] 0.9663291 1.0202870 0.9879914 1.0008415 0.9732872 0.9910241 1.0011829\n[183] 1.0038028 0.9929135 0.9883010 1.0082478 0.9555647 0.9924776 0.9820774\n[190] 0.9731466 0.9882706 0.9814835 1.0047609 0.9809707 1.0637701 1.0178161\n[197] 0.9907090 0.9837907 1.0011008 1.0172801\n\n\n$`50000`\n$`50000`$auc\n  [1] 0.8823436 0.8823260 0.8823902 0.8824228 0.8823599 0.8824131 0.8823825\n  [8] 0.8823980 0.8823903 0.8823661 0.8823702 0.8823824 0.8822969 0.8823841\n [15] 0.8823976 0.8823934 0.8824079 0.8824147 0.8823849 0.8823987 0.8823675\n [22] 0.8824254 0.8823388 0.8823704 0.8824004 0.8823662 0.8823206 0.8823582\n [29] 0.8824029 0.8823979 0.8823974 0.8823439 0.8824044 0.8823926 0.8823911\n [36] 0.8823335 0.8823870 0.8823890 0.8823964 0.8824089 0.8823567 0.8823955\n [43] 0.8823554 0.8824059 0.8823566 0.8823133 0.8823995 0.8823453 0.8824138\n [50] 0.8824067 0.8823960 0.8824056 0.8823686 0.8824096 0.8823658 0.8823188\n [57] 0.8823606 0.8823512 0.8824112 0.8823942 0.8823374 0.8823635 0.8823659\n [64] 0.8823948 0.8823797 0.8823797 0.8824293 0.8823320 0.8823394 0.8823848\n [71] 0.8823944 0.8823707 0.8824137 0.8823841 0.8823366 0.8823827 0.8823820\n [78] 0.8823889 0.8824067 0.8824215 0.8822759 0.8823825 0.8823649 0.8823479\n [85] 0.8824197 0.8824177 0.8823648 0.8823528 0.8823901 0.8823734 0.8823771\n [92] 0.8824298 0.8823903 0.8823713 0.8824090 0.8823916 0.8823942 0.8823732\n [99] 0.8823551 0.8823437 0.8823316 0.8824006 0.8823824 0.8823843 0.8823730\n[106] 0.8823223 0.8823657 0.8823899 0.8823864 0.8824021 0.8823786 0.8823992\n[113] 0.8823223 0.8823646 0.8824030 0.8823785 0.8822852 0.8824124 0.8823617\n[120] 0.8824132 0.8823573 0.8824207 0.8823783 0.8824100 0.8823454 0.8823691\n[127] 0.8823644 0.8823833 0.8823844 0.8823972 0.8823979 0.8823415 0.8824122\n[134] 0.8823233 0.8824055 0.8823706 0.8824030 0.8823756 0.8823689 0.8823844\n[141] 0.8824248 0.8823425 0.8824125 0.8824166 0.8823732 0.8823824 0.8823932\n[148] 0.8824164 0.8824053 0.8823714 0.8823665 0.8823131 0.8824016 0.8823734\n[155] 0.8824073 0.8823303 0.8823867 0.8823945 0.8823410 0.8823956 0.8823915\n[162] 0.8824005 0.8823734 0.8823954 0.8822965 0.8823793 0.8823280 0.8823763\n[169] 0.8823776 0.8823565 0.8823965 0.8824015 0.8823177 0.8823776 0.8823954\n[176] 0.8824214 0.8823966 0.8824097 0.8823593 0.8823099 0.8823794 0.8824093\n[183] 0.8823870 0.8823776 0.8823630 0.8823656 0.8823796 0.8824075 0.8823750\n[190] 0.8823950 0.8823158 0.8824130 0.8823989 0.8823425 0.8823665 0.8823869\n[197] 0.8823965 0.8823831 0.8823880 0.8823936\n\n$`50000`$slope\n  [1] 1.0063084 0.9916017 0.9925961 1.0019882 1.0147770 1.0054990 0.9921707\n  [8] 0.9880877 0.9983682 1.0042703 0.9881082 1.0011098 0.9869611 0.9937892\n [15] 0.9912269 0.9998952 1.0234154 1.0060018 1.0082012 0.9982618 0.9969829\n [22] 0.9826383 0.9939616 1.0015775 1.0073802 1.0124648 1.0043224 1.0008967\n [29] 1.0077950 1.0015486 1.0100212 0.9965761 1.0046688 0.9975582 0.9966118\n [36] 0.9875235 0.9793948 1.0040766 1.0064378 0.9947021 0.9861129 0.9786661\n [43] 1.0197972 0.9847580 1.0115986 1.0044170 1.0022323 1.0195921 1.0189130\n [50] 0.9910044 0.9960488 0.9933686 0.9894659 0.9939394 1.0093668 1.0046352\n [57] 1.0019044 0.9988059 1.0020417 0.9947529 0.9989248 0.9881295 1.0123523\n [64] 0.9984256 0.9976232 0.9953217 1.0078921 1.0115131 1.0000485 0.9993028\n [71] 0.9882184 0.9924399 0.9897208 0.9921104 0.9967538 0.9935756 1.0002198\n [78] 1.0096991 1.0021160 1.0053622 1.0044759 0.9900337 1.0113312 1.0100334\n [85] 0.9908711 0.9849146 1.0061466 0.9941169 0.9946836 0.9970439 0.9823840\n [92] 1.0124338 1.0002622 1.0080290 0.9996986 1.0163522 0.9778210 1.0051435\n [99] 1.0142630 0.9878473 1.0022260 0.9911504 1.0088557 1.0011570 1.0024815\n[106] 0.9966855 1.0026054 0.9905250 1.0004838 0.9967494 1.0039931 0.9974408\n[113] 0.9978650 0.9980613 1.0088936 1.0074555 1.0021915 1.0011275 0.9994384\n[120] 1.0119697 0.9891635 1.0015444 0.9974174 0.9923848 0.9968526 0.9730493\n[127] 0.9839221 0.9929151 0.9912389 0.9985886 1.0104694 1.0062956 0.9980179\n[134] 1.0107602 0.9823987 1.0020404 1.0069449 1.0005524 1.0137922 0.9928850\n[141] 1.0180505 0.9955086 0.9916078 0.9971575 0.9698767 1.0024745 0.9917832\n[148] 0.9828822 0.9912918 0.9890056 0.9954151 1.0166417 1.0004716 0.9898205\n[155] 1.0045176 0.9916420 0.9957197 0.9997991 0.9957185 0.9949985 0.9867868\n[162] 1.0012780 1.0090009 0.9963115 1.0067183 0.9889615 0.9949286 0.9856602\n[169] 1.0073318 0.9826510 0.9874762 1.0023312 0.9910046 1.0034492 1.0093212\n[176] 0.9961491 0.9949629 1.0229789 1.0059380 1.0081719 1.0240622 0.9852548\n[183] 0.9993545 1.0082371 0.9914737 0.9911733 1.0093337 1.0210324 0.9938527\n[190] 0.9997679 1.0044354 0.9855792 1.0043282 0.9805987 0.9872146 1.0151380\n[197] 1.0114449 1.0030099 0.9881540 1.0207834\n\n\n\nfail_counts\n\n  100   200   300   500  1000  5000 10000 50000 \n   23     0     0     0     0     0     0     0"
  },
  {
    "objectID": "bootstrap.html#plot-results",
    "href": "bootstrap.html#plot-results",
    "title": "Bootstrapping & Sample Sizes",
    "section": "Plot results",
    "text": "Plot results\n\n# Flatten into a data frame\nresults_df &lt;- map_dfr(names(results), function(s) {\n  tibble(\n    sample_size = as.integer(s),\n    auc = results[[s]]$auc,\n    slope = results[[s]]$slope\n  )\n})\nhead(results_df)\n\n# A tibble: 6 × 3\n  sample_size   auc slope\n        &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1         100 0.911 0.518\n2         100 0.896 0.591\n3         100 0.911 0.812\n4         100 0.908 0.822\n5         100 0.894 0.366\n6         100 0.895 0.436\n\n\n\nresults_long &lt;- results_df %&gt;%\n  pivot_longer(cols = c(auc, slope), names_to = \"metric\", values_to = \"value\")\nhead(results_long, 10)\n\n# A tibble: 10 × 3\n   sample_size metric value\n         &lt;int&gt; &lt;chr&gt;  &lt;dbl&gt;\n 1         100 auc    0.911\n 2         100 slope  0.518\n 3         100 auc    0.896\n 4         100 slope  0.591\n 5         100 auc    0.911\n 6         100 slope  0.812\n 7         100 auc    0.908\n 8         100 slope  0.822\n 9         100 auc    0.894\n10         100 slope  0.366\n\n\n\nfail_df &lt;- tibble(\n  sample_size = as.integer(names(fail_counts)),\n  failures = as.numeric(fail_counts)\n)\nfail_df\n\n# A tibble: 8 × 2\n  sample_size failures\n        &lt;int&gt;    &lt;dbl&gt;\n1         100       23\n2         200        0\n3         300        0\n4         500        0\n5        1000        0\n6        5000        0\n7       10000        0\n8       50000        0\n\n\n\nggplot(results_long, \n       aes(x = factor(sample_size), y = value, fill = metric)\n       ) +\n  geom_boxplot(outlier.shape = NA, alpha = 0.5, \n               position = position_dodge(width = 0.75)) +\n  scale_y_continuous(\n    name = \"Model Performance\"\n  ) +\n  labs(x = \"Sample Size\", fill = \"Metric\", color = \"Metric\", title = \"Bootstrap Model Performance vs Sample Size\") +\n  theme_minimal()"
  }
]